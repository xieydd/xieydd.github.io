<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>inference - Tag - Xieydd&#39;s Blog</title>
        <link>https://blog.xieydd.top/en/tags/inference/</link>
        <description>inference - Tag - Xieydd&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>xieydd@gmail.com (xieydd)</managingEditor>
            <webMaster>xieydd@gmail.com (xieydd)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 08 Jan 2025 16:37:10 &#43;0800</lastBuildDate><atom:link href="https://blog.xieydd.top/en/tags/inference/" rel="self" type="application/rss+xml" /><item>
    <title>Optimizing Model Inference Cold Start</title>
    <link>https://blog.xieydd.top/en/improve-model-serving-cold-start/</link>
    <pubDate>Wed, 08 Jan 2025 16:37:10 &#43;0800</pubDate>
    <author>xieydd</author>
    <guid>https://blog.xieydd.top/en/improve-model-serving-cold-start/</guid>
    <description><![CDATA[Previously, while working on Serverless model inference Modelz, although we have pivoted now, I still want to share how to optimize the cold start problem of model inference. Since our service is based on container orchestration, it also involves the cold start problem of containers.
Optimizing Model Inference Cold Start Problem First, let's look at the process of Serverless model inference, from user request to model inference:
 Click me 1 2 3 4 5 6 7 8 9 10 11 12 13  sequenceDiagram participant User participant Cloudflare participant Ingress participant AutoScaler participant Node participant containerd User-&gt;&gt;Cloudflare: Model Call Cloudflare-&gt;&gt;Ingress: Request Ingress-&gt;&gt;AutoScaler: Request AutoScaler-&gt;&gt;Node: Scale Up Node-&gt;&gt;containerd: Container Note right of containerd: 1.]]></description>
</item>
</channel>
</rss>
