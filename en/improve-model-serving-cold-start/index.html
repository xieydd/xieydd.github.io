<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Optimizing Model Inference Cold Start | Xieydd&#39;s Blog</title>
  <meta name="description" content="AI Infra Engineer&#39;s Blog">
  
  
  <meta property="og:title" content="Optimizing Model Inference Cold Start">
  <meta property="og:description" content="AI Infra Engineer&#39;s Blog">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://blog.xieydd.top/en/improve-model-serving-cold-start/">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Optimizing Model Inference Cold Start">
  <meta name="twitter:description" content="AI Infra Engineer&#39;s Blog">
  
  
  <link rel="icon" href="/favicon-32x32.png">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
  <link rel="stylesheet" href="/css/style.css">
  
  
  <style>
    @font-face {
      font-family: 'Monaco';
      src: local('Monaco'), local('Menlo'), local('Consolas');
      font-display: swap;
    }
  </style>
  
  
</head>
<body class="antialiased">
  <div class="container">
    <header class="site-header">
      <div class="language-switcher">
  
  
    
      
      
        
          <a href="https://blog.xieydd.top/improve-model-serving-cold-start/" class="lang-link">中文</a>
          
        
      
      
    
  
    
      <span class="lang-current">EN</span>
    
  
</div>

    </header>
    <main class="content">
      
<article class="post">
  <header class="post-header">
    <h1 class="post-title">Optimizing Model Inference Cold Start</h1>
    <div class="post-meta">
      <time datetime="2025-01-08">2025-01-08</time>
      
      <span class="post-author">by xieydd</span>
      
      
      <span class="reading-time">5 min read</span>
      
    </div>
    
    <div class="post-tags">
      
      <a href="https://blog.xieydd.top/en/tags/serving" class="tag">#serving</a>
      
      <a href="https://blog.xieydd.top/en/tags/cold-start" class="tag">#cold start</a>
      
      <a href="https://blog.xieydd.top/en/tags/inference" class="tag">#inference</a>
      
    </div>
    
  </header>

  
  <nav class="toc">
    <h2>Table of Contents</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#problem">Problem</a></li>
    <li><a href="#deep-dive">Deep Dive</a>
      <ul>
        <li><a href="#why-is-the-image-so-large">Why is the image so large?</a></li>
      </ul>
    </li>
    <li><a href="#solutions-we-have-tried">Solutions we have tried</a>
      <ul>
        <li><a href="#1-preheating">1. Preheating</a></li>
        <li><a href="#2-cache-model">2. Cache Model</a></li>
        <li><a href="#3-gcp-image-streaming">3. GCP Image Streaming</a></li>
        <li><a href="#4-change-image-format">4. Change Image Format</a></li>
        <li><a href="#5-use-juicefs-to-build-model-cache-cluster">5. Use JuiceFS to Build Model Cache Cluster</a></li>
      </ul>
    </li>
    <li><a href="#possible-future-optimizations">Possible Future Optimizations</a></li>
    <li><a href="#references">References</a></li>
  </ul>
</nav>
  </nav>
  

  <div class="post-content">
    <p>Previously, while working on Serverless model inference <a href="https://modelz.ai/">Modelz</a>, although we have pivoted now, I still want to share how to optimize the cold start problem of model inference. Since our service is based on container orchestration, it also involves the cold start problem of containers.</p>
<h1 id="optimizing-model-inference-cold-start">Optimizing Model Inference Cold Start</h1>
<h2 id="problem">Problem</h2>
<p>First, let's look at the process of Serverless model inference, from user request to model inference:</p>
<details>
  <summary>Click me</summary>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">sequenceDiagram
    participant User
    participant Cloudflare 
    participant Ingress 
    participant AutoScaler 
    participant Node
    participant containerd
    User-&gt;&gt;Cloudflare: Model Call
    Cloudflare-&gt;&gt;Ingress: Request
    Ingress-&gt;&gt;AutoScaler: Request
    AutoScaler-&gt;&gt;Node: Scale Up
    Node-&gt;&gt;containerd: Container
    Note right of containerd: 1. Pull Image &lt;br&gt;2. Start Container&lt;br&gt;3. Download model
</code></pre></td></tr></table>
</div>
</div></details>
<div align="center">
  <img src="image-1.png" alt="image 1" />
</div>
<p>The entire process chain is very long, but the real time-consuming part is the process of pulling the image and starting the container by Containerd at the end. We further break down this part, and the time for each stage here is roughly from reference 1:</p>
<details>
  <summary>Click me</summary>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">flowchart TD
    subgraph Pod Create
    3A[Pull Image 3.5GB 140s] --&gt; 3B[Download Model]
    end
    subgraph GPU Node Provision
    2A[VM Create 40s] --&gt; 2B[Node Initialize 45s]
    2B --&gt; 2C[GPU Driver Install 25s]
    end
    subgraph AutoScaler
    1A[HPA reaction 10s] --&gt; 1B[Auto Provisioning reaction 30s] --&gt; 1C[Node auto-scaling 35s]
    end
</code></pre></td></tr></table>
</div>
</div></details>
<div align="center">
  <img src="image-2.png" alt="image 2" />
</div>
<p>If it is a 30G image (not uncommon in AI inference scenarios), the pull time will exceed 15 minutes, which is unacceptable for users.</p>
<p>The model download depends on the size of the model and whether the model already exists in the Pod. This time is also uncontrollable, but we will propose targeted optimization solutions later.</p>
<h2 id="deep-dive">Deep Dive</h2>
<h3 id="why-is-the-image-so-large">Why is the image so large?</h3>
<div align="center">
  <img src="ai-image.png" alt="ai docker image" />
</div>
<div align="center">
  <img src="nvidia-dl-stack.png" alt="nvidia docker image stack" />
</div>
<p>From the above two images, we can see that</p>
<ol>
<li>Except for the NVIDIA Kernel Driver and CUDA Lib placed on the Host, the libraries that AI applications and frameworks depend on are all placed in the image.</li>
<li>NVIDIA's strategy prevents you from significantly reducing your image size. You don't know which libraries will be used, so you have to put all the libraries in the image.</li>
</ol>
<h2 id="solutions-we-have-tried">Solutions we have tried</h2>
<h3 id="1-preheating">1. Preheating</h3>
<p>First, we use <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a> to scale the GPU resources to 2 nodes when the total number of nodes is 8, even if there are no requests, there is a reserved bubble. Then, according to the frequency of image usage, we use <a href="https://github.com/senthilrch/kube-fledged">kube-fledged</a> to create an ImageCache on these nodes, so that when the actual request comes, the image is already on the node.</p>
<h3 id="2-cache-model">2. Cache Model</h3>
<p>We developed a HuggingFace model caching service. This service compares the hash value of the model when the model is called. If the model already exists in the caching service, it directly returns the cached model; otherwise, it downloads the model to the caching service.</p>
<h3 id="3-gcp-image-streaming">3. GCP Image Streaming</h3>
<p>Use <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-container-image-streaming-in-gke">GCP Image Streaming</a> to convert self-managed images or user-defined images to GCP's Artifact Registry. When the node pulls the image, it mounts the container layers to the node through the network, making containerd think the image is already on the node. However, this solution has several drawbacks:</p>
<ol>
<li>Requires GCP support, vendor lock-in</li>
<li>User images need to be proxy converted to GCP, which will have some delay</li>
<li>Although the pod is running, it is not fully operational, which may cause slow runtime</li>
</ol>
<h3 id="4-change-image-format">4. Change Image Format</h3>
<p>Convert the OCI image format to <a href="https://github.com/dragonflyoss/nydus">nydus</a> format and combine it with lazy pulling technology <a href="https://github.com/dragonflyoss/nydus/blob/master/docs/nydus-zran.md">zran</a>. Testing shows several times improvement. However, it requires modifying containerd configuration to support nydus. Combined with <a href="https://github.com/dragonflyoss/dragonfly">Dragonfly</a> P2P technology, the image pull speed can be further improved.</p>
<h3 id="5-use-juicefs-to-build-model-cache-cluster">5. Use JuiceFS to Build Model Cache Cluster</h3>
<p>By building an independent cache pool, cache the model to JuiceFS. Mount the cache directory to the container through JuiceFS CSI. If the model already exists in JuiceFS, use it directly; if not, download and cache it directly to JuiceFS. This architecture mainly utilizes JuiceFS Posix and the advantages of using object storage, without worrying about cache size. Here, JuiceFS parameters need to be tuned, such as prefetch block, buffer size, etc.</p>
<h2 id="possible-future-optimizations">Possible Future Optimizations</h2>
<ul>
<li>
<p>Use GCP's <a href="data-container-image-preloading">image preloading</a> feature to preload images to the node through secondary boot disks.</p>
</li>
<li>
<p>In-class registry cache <a href="https://github.com/spegel-org/spegel">spegel</a>.</p>
</li>
<li>
<p>Parallel Downloading in Kubelet <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/-kubelet-parallel-image-pull-limit/README.md">KEP 3673</a>.</p>
</li>
<li>
<p><a href="https://github.com/containerd/containerd/issues/8881">Parallel Container Layer Unpacking</a>, mentioned in reference 1, containerd needs to implement high IO queue depth to fully utilize EBS throughput.</p>
</li>
<li>
<p><a href="https://x.com/yetone/status/1858034646585368891">yetone's solution</a>:</p>
<blockquote>
<p>parsed the Dockerfile, then obtained the base image and a series of args, env, and commands, and merged them in order to hash as the s3 object key. Then, in the image builder job's pod container, started dind, then started a new container with the base image inside, executed the parsed commands, and after completion, tarred the container's rootfs, compressed it with zstd, and uploaded it to s3.</p>
</blockquote>
<blockquote>
<p>OCI image builder and containerd remote snapshotter, on the builder side, build the image and split all layers into two layers: environment (very large) and code (very small). Then use pzstd and s5cmd for streaming compression and streaming upload to s3. On the snapshotter side, use s5cmd and pzstd for streaming download and streaming decompression, fully utilizing GKE's disk IO, improving image pull speed by about 4 times.</p>
</blockquote>
</li>
<li>
<p>Modal lazy container loading</p>
<blockquote>
<p><img src="image.png" alt="alt text"></p>
</blockquote>
</li>
<li>
<p>Do some research on <a href="https://www.usenix.org/system/files/osdi24-fu.pdf">ServerlessLLM OSDI24</a></p>
</li>
</ul>
<h2 id="references">References</h2>
<ol>
<li><a href="https://www.youtube.com/watch?v=e6Oo2aoZPnA">https://www.youtube.com/watch?v=e6Oo2aoZPnA</a></li>
<li><a href="https://www.youtube.com/watch?v=SlkEW4C2kd4">https://www.youtube.com/watch?v=SlkEW4C2kd4</a></li>
</ol>

  </div>

  <footer class="post-footer">
    <div class="post-nav">
      
      <a href="https://blog.xieydd.top/en/postgres-ha/" class="prev-post">&larr; PostgreSQL High Availability</a>
      
      
      <a href="https://blog.xieydd.top/en/llm-infra/" class="next-post">Everything You Need to Know About LLM Infra &rarr;</a>
      
    </div>
    
    <a href="https://blog.xieydd.top/en/" class="back-home">&larr; Back to Home</a>
  </footer>
</article>

    </main>
  </div>
</body>
</html>
