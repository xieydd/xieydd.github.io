[{"categories":["tech","2025","training","serving"],"contents":"Everything You Need to Know About LLM Infra GPT 3 The GPT model is the starting point of LLM models. If you say Transformer is the starting point, that's also correct. Since we're going to discuss LLM Infra, we need to know who our system architecture and design are serving. Without understanding the model architecture, you'll feel confused about the Infra designs introduced later, such as Prefill-Decode Disaggregation during inference, Context Cache Migration, Traffic-Adaptive Request Routing, etc. Know not only the how but also the why. We start with GPT3 because the parameter sizes of GPT1 and GPT2 are too small. GPT2 has only 1.5B parameters, while GPT3 has 175B parameters. If you're not familiar with Transformer architecture, I recommend this article How Transformers Work. For Chinese users, I recommend the Zhihu article Transformer模型详解（图解最完整版） (in Chinese).\n First, let's determine the input and output of GPT3:\n Input: A text segment with a length of 2048 tokens Output: The probability distribution of the next token given 2048 input tokens  Here, tokens are tokenized using Byte Pair Encoding (BPE) tokenizer. For example, when I use OpenAI Tokenizer to tokenize I am xieydd, I love GPT3., the result is as follows, with token ids [40, 716, 2124, 494, 88, 1860, 11, 314, 1842, 402, 11571, 18, 13]:\n Encoding The token ids after Tokenizer are just a sequence of numbers, not vectors. We need to vectorize them, converting each token into a 50257-dimensional vector, with the position of the vector token id being 1 and other positions being 0. This way, we obtain the input vector, with a length of 2048 * 50257 = 102760896.\n Embedding This vector is too sparse, so we need to reduce its dimensionality, reducing it to 2048 * 12288.\n Positional Encoding Since Transformer doesn't have a recurrent mechanism like RNN, positional encoding is used to provide information about the position of each token in the sequence. This enables them to understand the position of each word in the sentence. Transformer uses a combination of sin and cos, allowing position encoding to encode sentences of arbitrary length.\n Attention After adding the Vector and Positional Encoding, we get the input vector. Next, we perform Attention. Here, we simplify the 2048 * 12288 input to 3*512.\n     Image from https://dugas.ch/artificial_curiosity/GPT_architecture.html\n  The input vector passes through weights Wq, Wk, Wv to get Query, Key, Value. Query is dotted with Key, then divided by the square root of d Softmax is applied to get the attention score The attention score is dotted with Value  Sparse Attention GPT3 doesn't use standard Attention, but Sparse Attention. In Sparse Attention, the attention of all tokens is set to 0, except for tokens with a relative distance not exceeding k and tokens with relative distances of k, 2k, 3k, \u0026hellip;, as shown in the following figure:\n There are two benefits:\n Reduce the computational complexity of the attention layer, saving memory and time, thus enabling processing of longer input sequences; Has the characteristic of \u0026ldquo;local tight correlation and remote sparse correlation,\u0026rdquo; paying more attention to closer context and less attention to distant context;  Multi-Head Attention  GPT3 repeats the above process 96 times, with 96 heads, and finally concatenates the output in rows and columns.\nFeed Forward The Feed Forward layer is a fully connected layer. The input is the output of Multi-Head Attention, and the output is 2048 * 12288.\n Add \u0026amp; Norm The input before Multi-Head Attention is added to the output of Feed Forward, then Layer Normalization is performed.\n Decoding We need to convert the Add \u0026amp; Norm output of 2048 * 12288 back to 2048 * 50257 to know the probability distribution of the next token among the 2048 tokens. We reuse the weights of the Embedding layer, transpose them, and multiply them with the Add \u0026amp; Norm output to get 2048 * 50257 output. Then, through Softmax, we get the probability distribution, and GPT3 takes the topk tokens as output.\n  Understanding the architecture of GPT3 allows us to better understand the design of LLM Infra. Next, let's enter the topic of how to train large models. Here, we'll use the current SOTA open-source model DeepSeek-V3 for explanation.\nInfra GPU The comprehensive solution GB200 NVL72:\n Blackwell Architecture NVIDIA Grace CPU Fifth-Generation NVIDIA NVLink NVIDIA Networking   NVIDIA Quantum-X800 InfiniBand NVIDIA Spectrum™-X800 Ethernet NVIDIA BlueField®-3 DPU  CUDA Network NVLink Intra-machine communication can be done through:\n PCIe Memory NVLink  Simply put, the current PCIe 5.0 bandwidth cannot meet the time requirements for parameter synchronization in AI distributed training, requiring intra-machine device communication with higher bandwidth. NVLink is a technology that provides high-speed interconnection for multiple GPUs on a single machine, replacing PCIe.\n With the development of NVLink generations, speeds are continuously improving. At the current fifth generation, a single NVIDIA Blackwell Tensor Core GPU supports up to 18 NVLink 100 GB/s connections, with a total bandwidth of up to 1.8 TB/s, twice that of the previous generation and 14 times that of PCIe 5.0 bandwidth.\n Note that only SXM sockets can truly use NVLink P2P communication, not NVLink Bridge connections. NVLink's latest technology, NVLink C2C interconnect, connects two high-performance NVIDIA Blackwell Tensor Core GPUs and an NVIDIA Grace CPU to two Blackwell GPUs. The ultimate goal is to enable NVIDIA GPUs, DPUs, and CPUs to achieve consistent interconnection with custom chips, reaching speeds of 900 GB/s. The following figure shows the architecture diagram of NVIDIA GH200 Grace Hopper Superchip:\n NVSwitch  When GPUs are connected via NVLink for P2P, the maximum total bandwidth can reach 1.8TB/s, but each GPU must split the same 1.8TB/s connection into seven dedicated 256 GB/s point-to-point connections. That is, the communication between GPU and GPU can only reach up to 1.8TB/s. To break this limitation, NVLink Switch is needed. NVSwitch is actually an ASIC chip. NVLink Switch can ensure that communication bandwidth between GPUs reaches the full bandwidth of 1.8TB/s, significantly compressing communication time. During the inference process, real-time response tok/s/user increases by 1.4 times compared to the situation without NVSwitch, and the larger the batch, the more obvious the improvement.\nInfiniBand Refer to 知乎-AI 集群基础设施 InfiniBand 详解 (in Chinese).\nInter-machine communication can be done through:\n TCP/IP protocol RDMA(Remote Direct Memory Access) protocol   InfiniBand RoCE (RDMA over Converged Ethernet) iWARP (Internet Wide Area RDMA Protocol)  RDMA protocol doesn't require CPU participation like the TCP/IP protocol. Data doesn't need to pass through the data link layer, network layer, transport layer, and application layer, with the CPU participating in operations such as receiving and unpacking data packets. RDMA's kernel bypass mechanism allows direct data reading and writing between applications and network cards. At the same time, RDMA's zero-copy mechanism allows the receiving end to directly read data from the sender's memory, greatly reducing the burden on the CPU and improving CPU efficiency.\n RDMA networks are divided into three types: Infiniband, RoCE, and iWARP. Among them, Infiniband is a network designed specifically for RDMA, ensuring reliable transmission at the hardware level. RoCE and iWARP are both Ethernet-based RDMA technologies that support corresponding verbs interfaces.\n IB (InfiniBand): Building an RDMA network based on IB technology requires dedicated IB network cards and IB switches.\niWARP (Internet Wide Area RDMA Protocal): An RDMA technology based on TCP/IP protocol, defined by the IETF standard. iWARP supports using RDMA technology on standard Ethernet infrastructure, but servers need to use network cards that support iWARP.\nRoCE (RDMA over Converged Ethernet): An Ethernet-based RDMA technology, also proposed by IBTA. RoCE supports using RDMA technology on standard Ethernet infrastructure, but requires switches to support lossless Ethernet transmission and servers to use RoCE network cards.\nIB link speeds are getting faster and faster with network bandwidth upgrades, as shown in the following figure:\n Here, x4 and x12 refer to the number of Links in the cable. x4 represents 4 Links, and x12 represents 12 Links. One Link consists of two wires, one for sending and one for receiving.\nInfiniBand cables are different from Ethernet cables and fiber optic cables. InfiniBand network interconnection products include: DAC high-speed copper cables, AOC active cables, and optical modules.\nMellanox's market share in the global InfiniBand market is basically unbeatable. After NVIDIA acquired Mellanox, it also launched its own seventh-generation NVIDIA InfiniBand architecture in 2021: NVIDIA Quantum-2.\nNVIDIA Quantum-2 platform includes: NVIDIA Quantum-2 series switches, NVIDIA ConnectX-7 InfiniBand adapters, BlueField-3 InfiniBand DPUs, and cables. Currently, the latest are:\n NVIDIA Quantum-X800 InfiniBand  NVIDIA® ConnectX® InfiniBand HCAs (Host Channel Adapters), ConnectX-8 InfiniBand SuperNIC can provide data throughput of up to 800 Gb/s InfiniBand Switches have up to 144 ports, with each port at 800 Gb/s for a total of 115Tb/s   NVIDIA Spectrum™-X800 Ethernet  Provides 800 Gb/s throughput through SuperNICs and NVIDIA Spectrum-X800 SN5600 Ethernet switch Switch has up to 64 ports, with each port at 800 Gb/s for a total of 51.2Tb/s   NVIDIA BlueField®-3 DPU  Supports Ethernet and InfiniBand connections, at 400Gb/s speed NVIDIA DOCA software framework for SDN Networking, security, storage, management, AI acceleration, and other functions   BlueField SuperNICs: Provides best-in-class remote direct-memory access over converged Ethernet (RoCE) network connectivity between GPU servers at up to 400Gb/s.  GPUDirect is a technology developed by NVIDIA that enables direct communication and data transfer between GPUs and other devices (such as network interface cards (NICs) and storage devices) without involving the CPU. GPUDirect technology includes GPUDirect Storage, GPUDirect RDMA, GPUDirect P2P, and GPUDirect Video.\n GPUDirect P2P is mainly for multi-GPU scenarios on a single machine, allowing GPUs to directly access the video memory of the target GPU through PCI Express GPUDirect RDMA is mainly for multi-machine, multi-GPU scenarios. GPU 1 on computer 1 can directly access the GPU memory of computer 2 without needing CPU -\u0026gt; CPU and then from computer 2's CPU -\u0026gt; GPU with multiple copies  Communication MPI (Message Passing Interface) is a communication protocol commonly used in parallel computing. MPI has multiple implementations, such as OpenMPI, Gloo, NCCL, etc.\nNCCL (Nvidia Collective multi-GPU Communication Library) implements multi-GPU and multi-node collective communication primitives optimized for NVIDIA GPU performance. NCCL provides implementations such as All Gather, All Reduce, Broadcast, Reduce, Reduce-Scatter, etc. These implementations are optimized to communicate through PCIe, NVLink, InfiniBand, and other high-speed interconnections, thereby achieving high bandwidth and low latency.\nLet's introduce several communication primitives:\n Reduce: Receive data from multiple senders, and finally combine them onto one node.   AllReduce: A variant of Reduce that reduces the data from all nodes onto all nodes.   Broadcast: One node sends data to all nodes.  Different network topologies have a significant impact on NCCL communication speed, as shown in the following figure:\n  The first type is where two GPUs are connected via the CPU and then via QPI to the two cards on another CPU, so the speed is the slowest, but it can still reach \u0026gt;5GB/s. The second type is where two GPUs are connected via a PCIe switch and then through the CPU, so the speed is slightly lower. The third type is where all four cards are on the same PCIe switch, so the bandwidth is higher and can reach \u0026gt;10GB/s, which is the size of PCIe bandwidth. The fourth type is the DGX-1 architecture, connected directly via NVLink, with the fastest speed, reaching 60GB/s.    The left figure shows 2 machines with 8 cards, with PCIe within the machine and InfiniBand between machines, achieving \u0026gt;10GB/s speed. InfiniBand basically achieves the communication speed within the machine; The right figure shows 4 machines with 32 cards, with NVLink within the machine and InfiniBand between machines, achieving bandwidth \u0026gt;40GB/s.  NCCL P2P has several levels:\n LOC: Never use P2P (always disabled) NVL: Use P2P when GPUs are connected via NVLink PIX: Use P2P when GPUs are on the same PCI switch. PXB: Use P2P when GPUs are connected via PCI switches (possibly multiple hops). PHB: Use P2P when GPUs are on the same NUMA node. Traffic will pass through the CPU. SYS: Use P2P between NUMA nodes, possibly across SMP interconnects (e.g., QPI/UPI).  Storage Operation and Maintenance Failure Hardware Exceptions:\n GPU ECC Error: Usually requires restarting the machine or Reset GPU. With 128 nodes, there is one every 1-2 days. IB (Infiniband)/NCCL issues: Usually hardware issues, such as a broken network card or network jitter, etc.   Due to network issues, training speed is reduced, such as by about 20%, requiring binary search to identify abnormal nodes and replace them accordingly. It can also cause tasks to fail directly, such as: p2p_plugin.c:141 NCCL WARN NET/IB : Got async event : port error.  Task Hang: Usually related to IB/NCCL issues and also requires manual detection. Task hangs occurred multiple times during OPT-175B training. GPU Card Loss: Usually triggers CUDA Error or the program exits abnormally, such as: \u0026ldquo;RuntimeError: Caught RuntimeError in pin memory thread for device 4.\u0026rdquo; Machine Abnormalities: Hardware abnormalities other than GPU, such as hard disk, CPU, etc., or even the machine crashing directly. Machine Configuration Abnormalities: For example, discovering that a machine has MIG enabled. Cluster Maintenance: Usually, a cluster doesn't support just one task, and occasionally maintenance and upgrades are needed, which may require stopping tasks. Storage Failure: For example, Lustre storage failing  Monitoring \u0026amp; Fault Tolerance Automatic failure detection and rapid recovery mechanisms are needed:\n When a user submits a training task, in addition to creating a training Executor on each GPU, a training daemon process is also created that periodically sends heartbeats to the Driver. Heartbeat signals contain various information to enable real-time anomaly detection and send alert information. When the Driver doesn't receive heartbeat signals within a specific time, it also triggers the failure recovery process, including:   Pause all training Executors and perform a series of self-check diagnostics. Once an abnormal machine is identified, it is evicted, and an equal number of healthy machines that passed tests are added to the cluster. Additionally, user interfaces are provided to allow users to manually identify and evict abnormal machines. After machine recovery is complete, training resumes from the most recent Checkpoint. The process of saving and restoring Checkpoints also needs optimization to minimize the impact on training progress. A two-stage approach is introduced. In the first stage, each GPU training process writes state information from video memory to host memory. Due to high-speed PCIe bandwidth, this process usually takes only a few seconds, so the training process can immediately continue training. In the second stage, a background process asynchronously synchronizes state information to a distributed file system. MegaScale uses HDFS. Its heartbeat signals include IP address, Pod name, hardware information, etc., and also include current training progress information. Additionally, the stdout/stderr logs of training processes are also collected for real-time aggregation, filtering, and analysis. If warning or error keywords are identified, the Driver will report these diagnostic information in real-time. Finally, RAMA's traffic metrics are also included to better identify network utilization and communication efficiency  Routine inspections (need a monitoring system accurate to the millisecond level to avoid Spikes):\n Start large matrix multiplication tasks to capture these failures Also start some simple communication tests to ensure the network is normal In-machine testing: Loopback test: Measure the loopback bandwidth from all RAMA network cards in the machine to various in-machine endpoints (including memory, GPU). Full-mesh testing is performed, covering all possible link combinations, to infer potential link issues in PCIe configuration. RNIC-to-RNIC testing: Mainly tests connectivity and bandwidth performance between different RNICs in the machine. This is to discover whether RNICs meet speed specifications or have routing issues. NCCL testing: All-to-All testing is performed within the machine, and All Reduce testing is also performed between machines on the same TOR switch to discover potential hardware failures and performance issues  Training For large model training, here we mainly reference Andrej Karpathy's sharing at MSBuild 2023, which is divided into the following four stages:\n Pre-training Supervised Fine-Tuning Reward Modeling Reforcement Learning  Before introducing these four stages, let's first普及 some Hardware knowledge, because large model training requires a large amount of computing resources, such as GPUs. Additionally, data storage and network communication need to be considered.\nHardware GPU When we mention GPU (Graphics Processing Unit), we naturally associate it with NVIDIA. There's some truth to this because NVIDIA's GPUs occupy a large market share in the deep learning field. However, this market is huge, with Application-specific Integrated Circuits (ASICs) chips from AMD and Brodcom also continuously devouring market share.\nAlthough GPU has the word \u0026ldquo;Graph\u0026rdquo; in it, in AI, we mostly use GPGPU (General-purpose computing on graphics processing units). If you're not familiar with GPU terminology, I recommend checking out the GPU Glossary maintained by Modal. GPU relative to CPU\nNetwork Storage System Pre-training Pre-training the model is the stage that consumes the most computing power and data in large model training. The goal of this stage is to let the model learn the basic laws of language, such as grammar, semantics, etc. The data volume in this stage is very large, usually at the TB level, and the training usually takes from a few weeks to several months. Training in this stage is usually unsupervised, meaning we don't need labeled data. Training in this stage is usually done on large-scale GPU clusters, such as OpenAI's GPT3, which was trained on thousands of V100s for a month.\nAs the pre-training that consumes the most computing resources, in this chapter, we'll share some techniques for LLM training, such as distributed training parallelism techniques and training optimization techniques.\nDistributed Training Refer to OpenAI's article Techniques for training large neural networks, which is mainly divided into the following four parts:\n Data Parallelism Pipeline Parallelism Tensor Parallelism Expert Parallelism    Here, blocks of different colors represent different layers of the model Dashed lines represent splitting onto different GPUs Arrows represent the forward propagation and backpropagation of the model  Data Parallelism Data Parallelism divides data into different subsets and distributes them to different GPUs. For each GPU, the model is complete, so each GPU needs to store the complete parameters of the model. Therefore, when GPU video memory cannot store model parameters, Data Parallelism is not applicable, although there are currently some Offload Memory technologies or KV Cache that can alleviate this problem.\nBecause each GPU needs complete parameters, when updating, it's necessary to ensure that the parameters of working threads (usually consistent with GPUs) remain consistent. This requires introducing blocking communication on working threads:\n Each worker calculates gradients Average the gradients of each worker Each worker updates parameters  The larger the number of parameters and the more threads, the greater the overhead of this blocking communication. Of course, asynchronous communication can be used, but this may damage the learning rate, which is not worth the loss, especially for LLM models, which involve significant investment and cannot tolerate mistakes. Communication between GPUs is implemented through the NCCL (NVIDIA Collective Communication Library). The above communication process is usually called AllReduce, as shown in the following figure:\n We notice that if AllReduce is processed in a separate process, this process needs to Gather All and Scatter All, so the communication overhead of a single process is proportional to the number of nodes and the number of parameters. To eliminate the blocking wait waste caused by the number of nodes and parameters, we can use Ring AllReduce, as shown in the following figure:\n First, construct a ring with N workers, and each process divides data into N chunks.\n The Nth worker sends the Nth chunk to worker[N+1] and simultaneously receives chunks from worker[N-1].\n The Nth worker reduces the received N-1 chunk with its own N-1 chunk and then sends it to worker[N+1].\n Repeat the receiving, reducing, and sending process N-1 times, so each worker obtains a part of the result. Finally, perform another round of Send operations without reduce operations, so each worker obtains the final result.\nPipeline Parallelism (Model Parallelism) Pipeline Parallelism divides the model into different stages and distributes them to different GPUs. For large models, such as LLaMA 65B, if the parameters are FP32, the total video memory needed is 260GB. However, most training uses half precision, which is FP16, so 130GB of video memory is needed. Currently, the most advanced H200 has 80GB of video memory. Currently, the DeepSeek V3 trained by the DeepSeek team has first verified the feasibility and effectiveness of FP8 training on ultra-large-scale models. However, DeepSeek V3 is a MOE (Mixtures of Experts) model with parameters reaching 671B, which we'll discuss later. The communication volume of PP is relatively small, so it's often placed on different machines.\n  The input and output of each layer of the model have an order. F represents Forward, B represents Backward, and U represents Update Each worker (GPU) is responsible for processing only one stage at the same time On the time series, resource utilization appears with a large number of bubbles  To optimize this problem, we can apply the idea of ring allreduce mentioned earlier, which is to split batch data and overlap computation time with waiting time, as shown in the following figure:\n The above figure shows the schematic of GPipe. GPipe continuously passes the activation values of the layer chunks of the model to the next worker and also continuously performs backward passing. Then, it synchronously aggregates gradients and updates parameters.  PipeDream, on the other hand, lets workers alternately handle forward and backward passing. Compared to GPiPe, PipeDream has better resource utilization but requires more communication overhead.\nTensor Parallelism Tensor Parallelism divides the model's operators into different subsets and distributes them to different GPUs, such as matrix multiplication. Pipeline parallelism divides the model's layers onto different GPUs, while Tensor Parallelism divides the operators within model layers onto different GPUs. For modern models such as Transformer, dot product computation of activation values and large weights is the bottleneck of computation. For example, MegatronLM parallelizes matrix multiplication in the Self-Attention and MLP layers of Transformer. PTD-P uses tensor, pipeline, and data parallelism, where the pipeline scheduler assigns multiple non-consecutive layers to each device, reducing bubble overhead at the cost of network communication. Its communication volume is larger, so TP is often placed within a single machine to fully utilize the high bandwidth of NVLink within the machine.\nThe following figure uses the 8DP 12PP 4TP scheme, so the GPUs needed are 8x12x4=384:\n Expert Parallelism Mixture-of-Experts (MoE) is a method where certain parts of each layer of the model are placed on the same GPU for execution. This means you can control which parts of the model will be applied to the current input and output through a gate controller. Each group of parameters controlled by a gate is an Expert, and different Experts are on different GPUs.\n Obviously, the MoE architecture can significantly increase the size of the model. This comes from an experience: with limited computing resource budgets, training a larger model with fewer training steps often yields better results than training a smaller model with more steps. Especially in the pre-training stage, compared to dense models, mixture-of-experts models can usually reach the same quality level faster. MoE is based on the Transformer architecture and consists of the following two parts:\n Sparse MoE layers: MoE layers have several experts. As shown in the following figure, there are 8 experts. These experts can be FFN layers or MoE layers, like Russian nesting dolls. Gating network or routing: This routing decides which expert a token will be sent to. The router's parameters are trained during training.   Above, we mentioned the advantages of MoE, but MoE also has some disadvantages:\n Insufficient generalization capability during fine-tuning, prone to overfitting During inference, all parameters need to be loaded into memory, even if only one expert is activated. For example, for Mixtral 8x7B, VRAM needs to hold 47B parameters. This is not 56B because only the FFN in MoE is independent, and other layers share parameters. Tokens are routed to different experts, resulting in uneven batch size distribution and waste of computing resources.  For gating networks, in addition to the most typical network with Softmax, there are other networks, such as Noisy Top-K Gating, which introduces adjustable random noise and retains the top k values.\n Random noise is to make the load between experts more balanced, avoiding only popular experts being trained during training. In the transformers library, this can be controlled by the aux_loss parameter. topk can selectively retain the top k values, accelerating training and inference  Let's list two typical MoE models or methods:\n Gshard   To ensure load balancing and training efficiency, Gshard uses the following optimizations in addition to the auxiliary loss mentioned above:  Random routing: In the Top-2 setting, we always choose the highest-ranked expert, but the second expert is randomly chosen based on its weight proportion. Define how many tokens an expert can handle. If both experts\u0026rsquo; capacities reach the limit, tokens will overflow and be passed to the next layer through residual connections, or in some cases, completely discarded.  Switch Transformer  Switch Transformer accepts two inputs, each with 4 experts, rather than a single input with at least two experts. The advantages of doing this are:\n Reduce the computational burden of the gating network (routing) The input batch for each expert can be at least halved Lower communication costs while maintaining model quality  At the same time, Switch Transformer also researched expert capacity, providing a buffer for uneven token allocation through a capacity factor greater than 1. Increasing the capacity factor (CF) can enhance model performance, but this also means higher communication costs and demand for video memory to save activation values. During training, the auxiliary loss for each Switch layer is added to the total model loss. This loss encourages uniform routing and can be weighted using hyperparameters.\nSwitch Transformer also explored mixed precision, meaning experts use original precision while other parts use Bfloat16 precision, increasing training speed while ensuring training stability.\nResearchers of ST-MoE found that different experts in the encoder tend to focus on specific types of tokens or shallow concepts. For example, some experts may specialize in punctuation, while others focus on proper nouns, etc. In contrast, experts in the decoder usually have a lower degree of specialization.\nWhat are some tricks for training MoE models:\n Sparse models are more prone to overfitting, so it's beneficial to try stronger internal regularization measures when handling these models, such as using a higher dropout rate. For example, we can set a lower dropout rate for dense layers and a higher dropout rate for sparse layers to optimize model performance; MoE models are prone to overfitting on small tasks such as SuperGLUE and don't perform as well as on large tasks such as TriviaQA; Freezing MoE parameters for fine-tuning can significantly speed up training and has little impact on model effectiveness; MoE is suitable for smaller batch sizes and higher learning rates; Fine-tuning on sparse models works better than on dense models.  How to choose between sparse models and dense models?\n Sparse Mixture-of-Experts (MoE) models are suitable for scenarios with multiple machines and high throughput requirements. Under fixed pre-training computing resources, sparse models can often achieve better results. Conversely, in scenarios with less video memory and lower throughput requirements, dense models are a more suitable choice. Directly comparing the number of parameters of sparse models and dense models is inappropriate because these two types of models are based on completely different concepts and parameter calculation methods.   We just talked about optimization methods for distributed training and mentioned Expert Parallelism, which led us to MoE models. So why do MoE models need Expert Parallelism?\n In Expert Parallelism, experts are placed on different devices, and each device processes different batches of training samples; When the parameters of a single expert can be on one device, use data parallelism and expert parallelism for training If a single expert model cannot be on one device, model parallelism needs to be added to distribute the single expert model to different devices, while using data parallelism for training acceleration  Deployment optimization methods for expert models:\n Distill expert models The router is modified to route entire sentences or tasks directly to one expert. This can extract a subnetwork for serving, helping to simplify the model structure. Merge the weights of various experts, reducing the number of parameters needed during inference  Zero-DP Sequence Parallel Context Parallelism Prepare Data The training data for LLaMA during Pre-training is as follows:\n After collecting raw data, data processing is needed, such as Tokenization.\n Model Let's look at a comparison of some parameters between GPT3 in 2020 and LLaMA in 2023:\n Explanation of the parameters in the above figure:\n Vocabulary Size: 50257. This refers to the number of vocabulary in the Tokenizer, which is the dimension of the input vector. Context Length: 2048. This refers to the size of the window that the generative model sees before generating the next token. Although the parameter size of GPT3 training is 175B, while LLaMA only has 65B, the tokens trained by LLaMA are 1.5T \u0026gt; GPT3's 300B. Some training hyperparameters, such as batch size, learning rate, number of heads, and number of model layers, etc.  If the training input is the following text, it will be marked by special tokens \u0026lt;s\u0026gt;:\n Row 1: Here is an example document 1 showing some tokens. Row 2: Example document 2 Example document 3 Example document Row 3: This is some random text just for example This Row 4: 1,2,3,4,5  The result after Tokenization is as follows:\n Here, Batch is 4, and Context Length is 10. Each cell can only see cells in the same row. Green is the context that the currently highlighted token can see, and red is its target.\nTraining Supervised Fine-Tuning In this stage, small amounts of high-quality data are needed, usually human-labeled data, such as prompts and corresponding ideal responses, usually requiring 10~100k.\nIn this stage, the Pre-training model is loaded, and then training is performed on this dataset to get the SFT (Supervised Fine-Tuning) model. Currently, there are many models undergoing SFT, such as Instruct, Coder, Math, and Reasoning, which are targeted at instructional QA, programming, mathematics, and reasoning task scenarios, respectively.\nReward Modeling The RLHF (Reward Learning from Human Feedback) stage is divided into two parts: one is the Reward Model, and the other is Reinforcement Learning.\nThe Reward Model turns data collection into a comparative form. For example:\n Here, humans need to input the same instruction and Rank among different outputs to get a pair dataset, about 100k~1M.\nDuring training:\n  The blue prompt in each row is the same Yellow is the SFT model output Green is the reward token, which is the SFT evaluation of the quality of the model output, compared with the quality of human evaluation The loss function measures the consistency between the reward token and the human-labeled ground truth  Reinforcement Learning Prepare 10k~100k Prompt data, and the Reward Model trains these models, at the level of 100 GPU-days.\n  The blue prompt in each row is the same Yellow is the SFT model output, serving as the initialization value and as training data for the Reward Model Green is the reward token, which will evaluate the sampled token, which is the yellow part. If it's high, the probability of the yellow part token being sampled in subsequent sampling will increase.  The PPO algorithm is the RLHF model. Why use RLHF? See the following figure. RLHF can significantly reduce the entropy of predict, meaning the prediction is more stable.\n Refer to 聊聊Reasoning Model的精巧实现（ReFT, Kimi K1.5, DeepSeek R1） (in Chinese) summarized by Dr. Jiang Fuchun on Zhihu, which introduces the implementation of three Reasoning Models: ReFT, Kimi K1.5, and DeepSeek R1, where RL is key.\nRLHF can refer to OpenRLHF.\nServing Model Optimization I originally wanted to put this model section under Model, but the model optimization here is mainly to make inference faster, so I put it under Serving.\nMHA vs MQA vs GQA vs MLA First, let's explain these terms:\n MHA: Multi-Head Attention MQA: Multi-Query Attention GQA: Global Query Attention MLA: Multi-Latent Attention  Refer to Soochow University's blog 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA (in Chinese).\nMHA In the Transformer decoder, since the attention of a token depends on previous tokens, it doesn't recalculate the previous context but caches its Key and Value. Why isn't Query cached?\n As shown in the Attention formula above without Causal Mask, from top to bottom, in stage T, only Q participates in the calculation at every place, but K and V are already calculated and don't need to be calculated again. However, after introducing KV Cache, although the computational amount is reduced, the resident memory usage increases. Moreover, as the Context length grows, the video memory occupation also increases, and a single card may not be able to hold it. If extended to multiple cards or multiple machines, communication bandwidth bottleneck limitations will be introduced. Therefore, this chapter is to optimize the KV Cache problem.\nMQA MQA (Multi-Query Attention) has a simple idea: all headers share one K V, so KV Cache is reduced to 1/h of the original. Its disadvantage is that it brings certain impacts to model performance.\nGQA GQA (Grouped-Query Attention) is to alleviate the impact of MQA on model performance. It divides h heads into g groups, so KV Cache is compressed by h/g times. In llama2/3-70B, GQA's g=8, and h=32. Here, g=8 is considering that a single machine has 8 cards, so during calculation, it ensures the diversity of K and V as much as possible while minimizing inter-card communication.\nMLA MLA (Multi-Latent Attention) has many mathematical concepts that are a bit complex for me. You can refer to the Soochow University blog mentioned above. Let me summarize the idea:\n On the basis of GQA, treat K and V together as the result of multiplying the input by a parameter matrix [K,V] = x.W Enhance GQA's capability through different projection matrices, and the KV Cache can be kept the same size during inference Then conversely, if we only need capabilities similar to GQA, we can take a smaller value for dimension W (DeepSeek-V2 took 512), thereby further compressing KV Cache.  Serving Pattern With the booming development of LLM applications such as Chat and Code Assistant, LLM Serving has gradually expanded from single-card to multi-card, and even multiple instances. This has given rise to Orchestration Patterns, such as prefill-decode disaggregation, context cache migration, traffic-adaptive request routing, etc.\nPrefill-Decode Disaggregation Why separate P (Prefill) and D (Decode)? Refer to prefill 和 decode 该分离到不同的卡上么？ - Chayenne Zhao的文章 - 知乎 (in Chinese).\nThe essence of P and D separation is that the computational models of the P and D stages are different. The P stage is Compute bound, and the D stage is Memory bound.\n Compute bound: The calculation of Attention for all input tokens. Increasing batch is limited by this, and there's no more computing power available. Memory bound: Each token generation requires frequent reading of KV Cache from video memory. Increasing batch is limited by this.  Advantages:\n Significantly reduces video memory usage. During the P stage calculation, intermediate activation value parameters are kept in video memory, while the D stage only needs the KV Cache from the P stage. If P and D are not separated, the video memory occupied by activation values during the D stage is waste. In cases of larger models and longer contexts, the waste of video memory is more obvious. Improve throughput: Separating prefill cards and decode cards on a multi-card system can achieve more efficient pineline parallelism, increasing throughput.  Disadvantages:\n Communication overhead: Transferring KV cache across cards undoubtedly brings new communication pressure. Hidden overhead: To strengthen communication, the cost of networking hardware cannot be ignored. Additionally, the fragmentation caused by deploying many different machine models in large-scale data centers also comes with a significant price.  P/D separation has brought architectural innovations, such as:\n  MoonCake   Maximize KV Cache cache hits and MFU (Model FLOPs Utilization) while meeting SLO (TTFT (time to first token), TBT (token between token)). Because reusing KV Cache involves remote communication, such as reading from other media (CPU, DRAM, SSD) into video memory, it increases TTFT. Meanwhile, larger batches increase MFU but also increase TBT time. MoonCake designed a global scheduler (conductor). For each request, Conductor needs to select a set of devices for prefill and decode, then schedule. First, migrate as much KV cache as possible to the prefill device. Then, on the prefill device, continuously stream prefill KV cache to the decode device through chunked and layer-wise prefill methods. Finally, on the decode device, load KV cache, add this request to continuous batching, and complete decoding. MoonCake established resource pools for CPU DRAM, SSD, and RDMA resources in the GPU cluster. Transferring KV cache between GPU and GPU is completed by a separate GPU direct RDMA device called messenger. The hash-based prefix storage in the original article can provide context caching APIs for upper-layer users. If the number of uncached tokens in a request exceeds a specific threshold (prefill_chunk), this request will be split into multiple chunks and executed in a pipeline. Generally, the prefill_chunk size is greater than 1k. Requests that were estimated during the prefill stage and not rejected may be rejected during the decode stage due to SLO reasons. This is also a significant staleness problem.  Chunked prefill can refer to 基于 chunked prefill 理解 prefill 和 decode 的计算特性 - Chayenne Zhao的文章 - 知乎 (in Chinese).\nMLC's MicroServing API orchestrates the inference engine through fine-grained APIs:\n Data Parallel Prefill-Decode Disaggregation Balanced prefill-decode disaggregation: Prefill and decode workloads may be imbalanced. When processing long prompts, the prefill engine may be overutilized, while the decode engine runs at low utilization or even remains idle. Dynamically offload some prefill computation to the decode engine. To achieve this, the router needs to decide \u0026ldquo;decode_start\u0026rdquo; (the position where the decode engine starts prefilling) and pass it to all APIs.  Context Cache Migration When serving QA workloads, developers tend to put different categories of context caches into different engines and schedule incoming traffic based on their matched context categories. Consider multiple engines, some specialized in historical backgrounds and others in scientific backgrounds. If scientific requests outnumber historical requests, we may want to switch some historical engines to scientific engines through context migration, and vice versa. The MicroServing fine-grained control inference engine API mentioned above enables efficient KV transmission between engines without interrupting service.\nTraffic-Adaptive Request Routing Dynamically reconfigure orchestration patterns based on workload characteristics without changing the underlying engine. Programmable routers allow seamless switching between different orchestration strategies:\n When the prefill:decode ratio (time of prefill input tokens / total time of decode all output tokens) increases, more prefill computation can be transferred to the decode engine through balanced prefill-decode disaggregation. If most prompt data is found in the decode engine's Context Cache, the system can completely bypass the prefill engine and directly call start_generate to handle the non-cache part of the prompt.  Serving Optimization  KV Cache Paged Attention Quantization Speculate Decode Constrainted Decoding Chunked prefill Prompt Cache KV Compression  KV Cache Refer to 大模型推理加速：看图学KV Cache - 看图学的文章 - 知乎 (in Chinese).\nA common technique for large model inference performance optimization is KV Cache. This technique can improve inference performance through the idea of space for time without affecting any computational accuracy. For details, refer to the reference above. Let me summarize:\n Transformer Encode stage is autoregressive Under autoregression, the kth calculation of Attention only depends on the kth Q. Other parts are repeated calculations and can be cached through KV Cache  Paged Attention Refer to 图解大模型计算加速系列之：vLLM核心技术PagedAttention原理 - 猛猿的文章 - 知乎 (in Chinese).\nPaged Attention mainly solves the problem of low video memory utilization caused by directly pre-allocating video memory for input requests. It improves the video memory utilization of inference by borrowing from the memory paging technology of modern operating systems.\n As shown in the figure above:\n Request A is a model request, equivalent to a process in an operating system Logical KV Blocks are equivalent to virtual memory pages in an operating system. Each Block has a fixed size, with a default size of 16 in vLLM, which can hold K/V values of 16 tokens Block Table is equivalent to a page table in an operating system, recording the position of each Logical Block in physical memory Physical KV blocks are equivalent to physical memory pages in an operating system, storing real K/V values, which are stored in vRAM  How does Paged Attention work in different scenarios?\n Scenario  Parallel Sampling: I send a request to the model, hoping it will continue writing the prompt and give three different answers. We call this scenario parallel sampling. Beam Search: At each decode stage, I don't produce just 1 token, but produce top k tokens (here k is also called beam width). The top k tokens necessarily correspond to the top k sequences at this moment. I feed these top k sequences into the model. Assuming the vocabulary size is |V|, at the next moment, I'll select top k again among k*|V| candidates, and so on. It's not hard to imagine that every time I feed the top k sequences into the model, there's a lot of KV cache duplication in their preceding tokens.   How does vLLM schedule:  Parallel Sampling: Each request is assigned a Logical Block, which is mapped to a Physical Block in vRAM. For Parallel Sampling, each request's Logical Block is independent, but Physical Blocks are shared. Whether to release is determined by ref count rather than independently allocating Physical Blocks. In the Decode stage, through Copy-on-Write, the KV Block of the newly generated token is copied to a new Physical Block, and the ref count of the Block where the preceding token of the newly generated token is located is -1. Beam Search: Based on the latest beam search decoding results, release the logical blocks and corresponding physical memory space that are no longer needed, achieving the goal of saving video memory. For the specific process, refer to the Zhihu article mentioned above.    Speculate Decoding Refer to Jiarui's article 大模型推理妙招—投机采样（Speculative Decoding） - 方佳瑞的文章 - 知乎 (in Chinese).\nThe professional name \u0026ldquo;speculative sampling\u0026rdquo; is a bit awkward and should be called inference sampling. The core of this technology is that during the decoding process, the decoding of some tokens is relatively easy, while the decoding of other tokens is very difficult. Therefore, simple token generation can be handled by small models, while difficult tokens are handled by large models. The small model here can use the same structure as the original model but with fewer parameters, or simply use n-gram models. Small models not only have less computational volume but, more importantly, small model generation with large model correction reduces the need for memory access.\nConstrainted Decoding Chunked prefill Prompt Cache KV Compression Quantization DeepSeek V3 Finally, let's analyze the domestic LLM model DeepSeek V3. This model was developed by the DeepSeek team. Through analysis, let's correspond it with the technologies mentioned above. Currently, DeepSeek-V3 is a MoE architecture trained on 14.8T tokens, with total parameters of 671B and 38B activated per token. Complete training took 2.788M hours on H800.\nArchitecture  As shown in the figure above, DeepSeek-V3 adopts the MoE architecture, where:\n The FFN layer in the Decode part of the original Transformer is replaced with a MoE layer, using DeepSeekMoE here; The Multi-Head Attention layer in the original Transformer architecture is replaced with a Multi-Latent Attention layer.  DeepSeekMoE Refer to DeepSeekMoE论文阅读 (in Chinese). This architecture mainly makes experts truly experts (having non-overlapping and concentrated knowledge), avoiding knowledge hybridization and knowledge redundancy among experts, achieving better performance with less resource occupation.\n DeepSeekMoE introduces two core strategies:\n Subdivide experts into smaller units and activate more combinations from them, allowing more flexible selection of activated experts.  The intuition of this strategy is that if the fewer experts a token can be assigned to, the harder it is for these experts to learn differentiated knowledge To not increase computational volume when increasing the number of assignable experts for tokens, expand activated experts by N times while reducing the hidden layer dimension of FFN to 1/N   Isolate some experts as shared experts to capture general knowledge and reduce redundancy between routing experts  Having dedicated shared experts to capture and integrate general knowledge from different contexts can reduce parameter redundancy between other routing experts. This reduction in redundancy will help build a model with higher parameter efficiency and more professional experts. Here, the router won't work on shared experts but will directly assign tokens to shared experts. Shared experts are separated from the total number of experts, meaning subdivided experts = total experts - shared experts    Multi-Latent Attention This method comes from DeepSeek-V2. Refer to MHA vs MQA vs GQA vs MLA.\nAuxiliary-loss-free strategy for load balancing Auxiliary loss introduces random noise to prevent experts from having the problem of \u0026ldquo;the rich get richer\u0026rdquo; during training, but it also damages model performance. The Auxiliary-loss-free strategy is to achieve a better trade-off between load balancing and model performance. Specific approach:\n Introduce bias During training, if it's discovered which experts are overloaded, reduce γ (the hyperparameter for bias update speed). If expert load is insufficient, increase γ  But to prevent extreme imbalance within any single sequence, Complementary Sequence-Wise Auxiliary Loss is also adopted, encouraging expert load on each sequence to reach balance. For specific algorithm formulas, refer to the paper.\nNode-Limited Routing Use a limited routing mechanism to limit communication costs during training. Ensure each token will be sent to at most M nodes, which are selected based on the sum of the highest affinity scores of experts distributed on each node. Under this constraint, the MoE training framework can almost achieve full computation-communication overlap, meaning computation and communication can proceed simultaneously without blocking each other.\nNo Token-Dropping DeepSeek-V3 doesn't drop any tokens during training. Additionally, specific deployment strategies are used to ensure load balancing during inference, so DeepSeek-V3 also doesn't lose tokens during inference.\nMulti-Token Prediction  Refer to 知乎 (in Chinese).\nInfrastructure Cluster Status  256 nodes, with 8 cards per node, totaling 2048 NVIDIA H800 GPUs Cards are connected via NVLink and NVSwitch between cards, and different nodes are connected via InfiniBand  Training Framework HAI-LLM:\n 16-way Pipeline Parallelism across 8 nodes   DualPipe overlaps the computation and communication phases of forward and backward processes, thereby solving the challenge of heavy communication overhead brought by cross-node expert parallelism  64-way Expert Parallelism   cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths.  ZeRO-1 Data Parallelism   Carefully optimized memory usage, enabling us to train without using expensive tensor parallelism (TP)   Based on the figure above:\n DualPipe significantly reduces bubbles DualPipe increases peak activation memory by several times, with PP \u0026lt; 1 Although DualPipe needs to retain two copies of model parameters, this doesn't significantly increase memory consumption because a larger EP size is used during training Neither bubbles nor activation memory increase as mini-batch increases  Communication Cross-Node All-to-All Communication:\n cross-node GPUs are fully interconnected with IB (50GB/s) intra-node communications are handled via NVLink (160GB/s) How to effectively utilize the different bandwidths of NVLink and IB   Limit each token to be scheduled to at most 4 nodes, thereby reducing IB traffic When each token makes a routing decision, it will first be transmitted via IB to the GPU with the same intra-node index on its target node. Once it arrives at the target node, we will strive to ensure it's immediately forwarded via NVLink to the specific GPU hosting the target expert without being blocked by subsequently arriving tokens Communication via IB and NVLink is completely overlapped, and each token can effectively select an average of 3.2 experts per node without additional overhead from NVLink. This means that while DeepSeek-V3 currently selects 8 experts, it can be expanded to 3.2x4. Use warp specialization technique to split the Streaming Multiprocessor (SM) into 10 communication channels.  Dispatching phase: (1) IB sending, (2) IB to NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps assigned to each communication task is dynamically adjusted based on the actual workload of all SMs. Combining phase: (1) NVLink sending, (2) NVLink to IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and interference to other SMs.    Memory Optimization Extremely Memory Saving with Minimal Overhead:\n Recomputation of RMSNorm and MLA Up-Projection during backpropagation eliminates the need to continuously store their output activations. Through slight additional overhead, this strategy significantly reduces the memory demand for storing activations. Exponential Moving Average in CPU: During training, we retain the exponential moving average (EMA) of model parameters to estimate model performance early after learning rate decay. EMA parameters are stored in CPU memory and updated asynchronously after each training step. This approach allows us to maintain EMA parameters without additional memory or time overhead. Shared Embedding and Output Head for Multi-Token Prediction: Adopting the DualPipe strategy, the shallowest layer (including embedding layer) and deepest layer (including output head) of the model are deployed on the same PP rank. This arrangement enables physical sharing of parameters and gradients between the MTP module and the main model, sharing embedding and output head. This physical sharing mechanism further improves our memory efficiency.  FP8 Mixed Precision Training:\n  Group elements using 1 × N elements or use N × N elements for block grouping. During accumulation with improved precision, the related dequantization overhead is largely alleviated To further reduce memory and communication overhead in MoE training, cache and schedule activations in FP8 while storing low-precision optimizer states in BF16. Most core computation kernels, namely GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as input and produce output in BF16 or FP32 format. As shown in the figure above, all three GEMMs associated with linear operators, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles computational speed compared to the original BF16 method. Additionally, FP8 Wgrad GEMM allows storing activations in FP8 for use in the backward pass. This significantly reduces memory consumption Maintain the original precision of the following components (e.g., BF16 or FP32): embedding module, output head, MoE gating module, normalization operators, and attention operators.    A fine-grained quantization method that applies scaling at a more fine-grained level. As shown in the figure above, (1) for activations, we group and scale elements based on 1x128 tiles (i.e., per 128 channels per token); (2) for weights, we group and scale elements based on 128x128 blocks (i.e., per 128 input channels per 128 output channels). This method ensures the quantization process can better adapt to outliers by adjusting scales based on smaller element groups. Introduce scaling factors along the internal dimension of the GEMM operation microscaling formats with smaller quantization granularity will be supported in Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) Underflow issues: During MMA (matrix multiplication accumulation) on Tensor Core, a limited bit width is used to accumulate intermediate results. Once the interval N is reached, these partial results are copied to FP32 registers on the CUDA kernel, where full-precision FP32 accumulation is performed. As mentioned earlier, our fine-grained quantization applies a set of scaling factors per group along the internal dimension K. These scaling factors can be effectively multiplied on the CUDA kernel as a dequantization process with minimal additional computational cost. It's worth noting that this modification reduces the WGMMA (Warpgroup-level matrix multiply accumulate) instruction issue rate of a single warpgroup. However, on the H800 architecture, two WGMMA being concurrently active is typical: when one warpgroup executes a promotion operation, another warpgroup can execute MMA operations. This design enables the overlap of two operations, thereby maintaining high Tensor Core utilization. According to experiments, setting N = 128 elements, equivalent to 4 WGMMA, represents the minimum accumulation interval and can significantly improve accuracy without introducing significant overhead.  Low-Precision Storage and Communication:\n Compress cached activations and optimizer states to lower-precision formats, further reducing memory consumption and communication overhead. Low-precision optimizer states. We adopt the BF16 data format rather than FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer without causing observable performance degradation. However, the main weights (stored by the optimizer) and gradients (for batch size accumulation) are retained in FP32 to ensure numerical stability throughout training. Low-precision activations. Wgrad operations are performed in FP8. To reduce memory consumption, it's a natural choice to cache activations in FP8 format for the backward pass of linear operators. But for low-cost, high-precision training, special consideration is given to several operators:   Linear input after attention operator. These activations are also used in the backward pass of the attention operator, making it sensitive to precision. We specifically adopt a customized E5M6 data format for these activations. Additionally, these activations will be converted from 1x128 quantized tiles to 128x1 tiles during the backward pass. To avoid introducing additional quantization errors, all scaling factors are rounded to powers of 2. Input of SwiGLU operator in MoE. To further reduce memory costs, we cache the input of the SwiGLU operator and recompute its output during the backward pass. These activations are also stored in FP8 through our fine-grained quantization method, striking a balance between memory efficiency and computational precision.  Low-precision communication   Quantize activations to FP8 before projecting to MoE, then apply a scheduling component that is compatible with FP8 Fprop in the projection to MoE. Similar to the Linear input after the attention operator, the scaling factor for this activation is an integer power of 2. A similar strategy is applied to activation gradients before projecting below MoE. For forward and backward combining components, we keep them in BF16 to maintain training precision in critical parts of the training pipeline.  Inference and Deployment Deploying DeepSeek-V3 on the H800 cluster, GPUs within each node are interconnected via NVLink, and all GPUs in the cluster are fully interconnected via IB. To ensure both service-level objectives (SLO) for online services and high throughput, the following deployment strategies are adopted to separate the prefilling and decoding stages:\n Prefilling stage:   The minimum deployment unit for the prefilling stage consists of 4 nodes and 32 GPUs. The attention part uses 4-way tensor parallelism (TP4) and sequence parallelism (SP), combined with 8-way data parallelism (DP8). Its smaller TP size of 4 limits TP communication overhead. For the MoE part, we use 32-way expert parallelism (EP32), ensuring each expert handles a sufficiently large batch size, thereby improving computational efficiency. For MoE's all-to-all communication, we use the same method as in training: first transmit tokens across nodes via IB, then forward between GPUs within the node via NVLink. In particular, we use 1-way tensor parallelism for the dense MLP in shallow layers to save TP communication. To achieve load balancing between different experts in the MoE part, it's necessary to ensure each GPU processes approximately the same number of tokens. To this end, a redundant expert deployment strategy is introduced, which means replicating high-load experts and deploying them redundantly. High-load experts are detected based on statistics collected during online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts between GPUs within the node based on observed loads, striving to balance loads between GPUs as much as possible without increasing cross-node all-to-all communication overhead. For DeepSeek-V3 deployment, we set 32 redundant experts for the prefilling stage. For each GPU, in addition to the 8 experts it originally hosts, it will also host an additional redundant expert. Additionally, during the prefilling stage, to improve throughput and hide the overhead of all-to-all and TP communication, two micro-batches with similar computational workloads are processed simultaneously, overlapping the attention and MoE of one mini-batch with the scheduling and combining of another micro-batch. Finally, the dynamic redundant strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 experts are activated in each inference step. Before all-to-all operations at each layer begin, we dynamically calculate the global optimal routing scheme. Considering the large amount of computation involved in the prefilling stage, the overhead of calculating this routing scheme is almost negligible.  Decoding stage:   Treat shared experts as routing experts. From this perspective, each token selects 9 experts during routing, where shared experts are considered heavy-load experts and are always selected. The minimum deployment unit for the decoding stage consists of 40 nodes and 320 GPUs. The attention part uses TP4+SP, combined with DP80, and the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, with 64 GPUs responsible for hosting redundant experts and shared experts. All-to-all communication for the scheduling and combining parts is performed via direct point-to-point transmission over IB to achieve low latency. Additionally, we utilize IBGDA (InfiniBand GPUDirect Accelerator) (NVIDIA, 2022) technology to further reduce latency and improve communication efficiency. Similar to prefilling, based on the statistical expert load of online services, the set of redundant experts is determined periodically at certain time intervals. However, rearranging experts isn't needed because each GPU hosts only one expert. To improve throughput and hide all communication overhead, we're exploring processing two micro-batches with similar computational workloads simultaneously during the decoding stage. Unlike prefilling, attention consumes most of the time during the decoding stage. Therefore, we overlap the attention of one micro-batch with the scheduling+MoE+combining of another micro-batch. During the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is small, so using fewer SMs doesn't significantly affect overall performance. Therefore, to avoid affecting the computational speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine.  Hardware Recommendations Communication Hardware:\n Future vendors develop hardware capable of offloading these communication tasks from the computing unit SM, acting as a GPU coprocessor or network coprocessor. To reduce application programming complexity, unify IB (scale-out) and NVLink (scale-up) networks from the computing unit's perspective. With this unified interface, the computing unit can easily complete read, write, multicast, and reduce operations across the entire IB-NVLink unified domain by submitting communication requests based on simple primitives.  Compute Hardware:\n Recommend that future chip designs improve accumulation precision in Tensor Cores to support full-precision accumulation, or select appropriate accumulation bit widths based on the precision requirements of training and inference algorithms. This approach ensures errors remain within acceptable ranges while maintaining computational efficiency. Current GPUs only support per-tensor quantization and lack native support for fine-grained quantization (such as tiled quantization and block quantization). In current implementations, when the N interval is reached, partial results are copied from Tensor Core to CUDA Core, multiplied by scaling factors, and then added to FP32 registers on CUDA Core. Although combined with our precise FP32 accumulation strategy, this significantly alleviates dequantization overhead, but frequent data movement between Tensor Core and CUDA Core still limits computational efficiency. Therefore, we recommend that future chips support fine-grained quantization by enabling Tensor Core to receive scaling factors and achieve MMA through group scaling. This way, the entire partial sum accumulation and dequantization can be completed directly inside Tensor Core until the final result is produced, avoiding frequent data movement. Support online quantization. Although our research demonstrates the effectiveness of online quantization, current implementations still struggle to effectively support online quantization. In the existing workflow, we need to read 128 BF16 activation values (previously computed outputs) from HBM (High Bandwidth Memory) for quantization, then write the quantized FP8 values back to HBM, and read them again only during MMA. To solve this inefficiency, we recommend that future chips integrate FP8 conversion and TMA (Tensor Memory Accelerator) access into a single fused operation, so quantization can be completed during the process of transferring activations from global memory to shared memory, thereby avoiding frequent memory reads and writes. We also recommend supporting warp-level conversion instructions for acceleration, which further promotes better fusion of layer normalization and FP8 conversion. Alternatively, near-memory computing approaches can be adopted, where computational logic is placed near HBM. In this case, BF16 elements can be directly converted to FP8 when read from HBM into the GPU, reducing off-chip memory access by approximately 50%. Support transposed GEMM operations. Current architectures make it cumbersome to fuse matrix transposition with GEMM operations. In the workflow, activations during the forward pass are quantized into 1x128 FP8 blocks and stored. During the backward pass, it's necessary to read out the matrix, dequantize, transpose, requantize into 128x1 blocks, and store in HBM. To reduce memory operations, we recommend that future chips enable direct transposed reads of matrices in shared memory before MMA operations to meet the precision required in training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly simplify the quantization workflow.  Pre-Training References  GPT3 Architecture Microserving LLM engines State of GPT Andrej Karpathy LLM Action MOE Architecture 万卡 GPU 集群实战：探索 LLM 预训练的挑战 (in Chinese)  ","date":"2025-01-09","permalink":"https://blog.xieydd.top/en/llm-infra/","tags":["llm","infra","model training","vllm"],"title":"Everything You Need to Know About LLM Infra"},{"categories":["tech","2025","serverless","serving"],"contents":"Previously, while working on Serverless model inference Modelz, although we have pivoted now, I still want to share how to optimize the cold start problem of model inference. Since our service is based on container orchestration, it also involves the cold start problem of containers.\nOptimizing Model Inference Cold Start Problem First, let's look at the process of Serverless model inference, from user request to model inference:\n Click me 1 2 3 4 5 6 7 8 9 10 11 12 13  sequenceDiagram participant User participant Cloudflare participant Ingress participant AutoScaler participant Node participant containerd User-\u0026gt;\u0026gt;Cloudflare: Model Call Cloudflare-\u0026gt;\u0026gt;Ingress: Request Ingress-\u0026gt;\u0026gt;AutoScaler: Request AutoScaler-\u0026gt;\u0026gt;Node: Scale Up Node-\u0026gt;\u0026gt;containerd: Container Note right of containerd: 1. Pull Image \u0026lt;br\u0026gt;2. Start Container\u0026lt;br\u0026gt;3. Download model     The entire process chain is very long, but the real time-consuming part is the process of pulling the image and starting the container by Containerd at the end. We further break down this part, and the time for each stage here is roughly from reference 1:\n Click me 1 2 3 4 5 6 7 8 9 10 11  flowchart TD subgraph Pod Create 3A[Pull Image 3.5GB 140s] --\u0026gt; 3B[Download Model] end subgraph GPU Node Provision 2A[VM Create 40s] --\u0026gt; 2B[Node Initialize 45s] 2B --\u0026gt; 2C[GPU Driver Install 25s] end subgraph AutoScaler 1A[HPA reaction 10s] --\u0026gt; 1B[Auto Provisioning reaction 30s] --\u0026gt; 1C[Node auto-scaling 35s] end     If it is a 30G image (not uncommon in AI inference scenarios), the pull time will exceed 15 minutes, which is unacceptable for users.\nThe model download depends on the size of the model and whether the model already exists in the Pod. This time is also uncontrollable, but we will propose targeted optimization solutions later.\nDeep Dive Why is the image so large?   From the above two images, we can see that\n Except for the NVIDIA Kernel Driver and CUDA Lib placed on the Host, the libraries that AI applications and frameworks depend on are all placed in the image. NVIDIA's strategy prevents you from significantly reducing your image size. You don't know which libraries will be used, so you have to put all the libraries in the image.  Solutions we have tried 1. Preheating First, we use cluster-proportional-autoscaler to scale the GPU resources to 2 nodes when the total number of nodes is 8, even if there are no requests, there is a reserved bubble. Then, according to the frequency of image usage, we use kube-fledged to create an ImageCache on these nodes, so that when the actual request comes, the image is already on the node.\n2. Cache Model We developed a HuggingFace model caching service. This service compares the hash value of the model when the model is called. If the model already exists in the caching service, it directly returns the cached model; otherwise, it downloads the model to the caching service.\n3. GCP Image Streaming Use GCP Image Streaming to convert self-managed images or user-defined images to GCP's Artifact Registry. When the node pulls the image, it mounts the container layers to the node through the network, making containerd think the image is already on the node. However, this solution has several drawbacks:\n Requires GCP support, vendor lock-in User images need to be proxy converted to GCP, which will have some delay Although the pod is running, it is not fully operational, which may cause slow runtime  4. Change Image Format Convert the OCI image format to nydus format and combine it with lazy pulling technology zran. Testing shows several times improvement. However, it requires modifying containerd configuration to support nydus. Combined with Dragonfly P2P technology, the image pull speed can be further improved.\n5. Use JuiceFS to Build Model Cache Cluster By building an independent cache pool, cache the model to JuiceFS. Mount the cache directory to the container through JuiceFS CSI. If the model already exists in JuiceFS, use it directly; if not, download and cache it directly to JuiceFS. This architecture mainly utilizes JuiceFS Posix and the advantages of using object storage, without worrying about cache size. Here, JuiceFS parameters need to be tuned, such as prefetch block, buffer size, etc.\nPossible Future Optimizations   Use GCP's image preloading feature to preload images to the node through secondary boot disks.\n  In-class registry cache spegel.\n  Parallel Downloading in Kubelet KEP 3673.\n  Parallel Container Layer Unpacking, mentioned in reference 1, containerd needs to implement high IO queue depth to fully utilize EBS throughput.\n  yetone's solution:\n parsed the Dockerfile, then obtained the base image and a series of args, env, and commands, and merged them in order to hash as the s3 object key. Then, in the image builder job's pod container, started dind, then started a new container with the base image inside, executed the parsed commands, and after completion, tarred the container's rootfs, compressed it with zstd, and uploaded it to s3.\n  OCI image builder and containerd remote snapshotter, on the builder side, build the image and split all layers into two layers: environment (very large) and code (very small). Then use pzstd and s5cmd for streaming compression and streaming upload to s3. On the snapshotter side, use s5cmd and pzstd for streaming download and streaming decompression, fully utilizing GKE's disk IO, improving image pull speed by about 4 times.\n   Modal lazy container loading\n    Do some research on ServerlessLLM OSDI24\n  References  https://www.youtube.com/watch?v=e6Oo2aoZPnA https://www.youtube.com/watch?v=SlkEW4C2kd4  ","date":"2025-01-08","permalink":"https://blog.xieydd.top/en/improve-model-serving-cold-start/","tags":["serving","cold start","inference"],"title":"Optimizing Model Inference Cold Start"},{"categories":["tech","2024","Postgres"],"contents":"I've been researching PostgreSQL high availability solutions recently, and here's what I've learned.\nPostgreSQL High Availability High Availability Goals PostgreSQL high availability typically has two main objectives:\n RPO (Recovery Point Objective): The maximum acceptable amount of data loss measured in time. This represents how much data loss a business can tolerate. RTO (Recovery Time Objective): The maximum acceptable downtime, measured from when a disaster occurs until the system is operational again. This represents how quickly the system needs to be restored. In simple terms, this means determining how quickly to restore the database and to what state - for example, recovering within 5 minutes to a state no more than 30 minutes old. The ideal scenario would be RTO \u0026lt; 30s and RPO ≈ 0.  Scenarios To achieve the ideal scenario mentioned above, the following situations need to be addressed:\n When the Primary node fails, automatic failover to the Standby node occurs within RTO requirements while meeting RPO goals. When accidental data deletion occurs, upgrade errors happen, or hardware failures occur, the ability to recover to a specific point in time.  Concepts To handle these scenarios, the following technologies and concepts are essential:\n Continuous Archiving: Generally involves archiving WAL (Write Ahead Log) files; in case of database system crashes, recovery can be performed by replaying WAL. Point-in-Time Recovery (PITR): For hardware failures, high availability failover based on physical replication might be the best choice. For data corruption (whether machine or human error), Point-in-Time Recovery (PITR) is more appropriate as it provides a safety net for worst-case scenarios. Physical Replication: Complete replication of data files and transaction log files (PGData, pg_wals) Logical Replication: Replication between publisher and subscriber based on replication identifiers (e.g., primary keys), typically used for Foreign Data Wrapper (FDW) scenarios rather than disaster recovery. Streaming Replication: WAL log-based streaming replication, primarily used for disaster recovery. WAL XLOG records are continuously transmitted from primary to standby, available in both synchronous and asynchronous modes.  Tools Backup and Restore Here are the common backup and recovery methods along with their pros and cons:\n1. Pg_dump (Logical Backup) Logical backup exports database data to a file using the pg_dump SQL command, which can then be imported back using SQL commands. Advantages:\n Logical backups can be performed at table to database level as needed Backups don't block read/write activities on the database Can be restored to different major PostgreSQL versions and even different operating system architectures Disadvantages: Logical backups require replay during recovery, which can take considerable time for large datasets and may impact overall performance Doesn't support dumping global variables, requires pg_dumpall instead  2. Physical Backup Physical backup is an offline backup of the PostgreSQL cluster performed when the cluster is stopped, containing the entire cluster data. Advantages:\n Fast backup and recovery Suitable for large databases Ideal for high availability scenarios Disadvantages: Cannot restore across different versions Cannot restore across different operating systems  3. Continuous Archiving and Point-in-Time Recovery (PITR) Online Backup or Hot Backup starts with a full backup that can be performed online without stopping the PostgreSQL cluster. Incremental backups generate WAL logs, which can then be used for recovery through WAL archive replay. Advantages:\n Can recover to any point in time No application downtime required Disadvantages: May require significant time to recover data from archives, primarily used for massive databases that cannot be backed up frequently  4. Snapshots and Cloud Backups Snapshots require operating system or cloud provider support, with tools like rsync available for taking snapshots. Disadvantages:\n Not suitable when database tablespaces are stored across multiple drive volumes  Several considerations go into backup planning, including frequency, storage location, recovery time, and retention policies. Here are some popular open-source tools to assist with backups:\n pgbackrest EDB barman WAL-G From this discussion, barman lacks some features compared to pgbackrest:   Zstd compression Delta restore Encryption at rest Native postgres page checksum validation Multi repo  High Availability  Patroni  Patroni uses asynchronous Streaming Replication by default, meaning transactions committed on the primary node may take some time to replicate to standby nodes. During this time, if the primary node fails, data from this period could be lost. Synchronous replication can be used to reduce data loss, but this affects primary node performance as it must wait for all standby nodes to receive and write WAL logs before committing transactions. A balance between availability and performance must be struck.\nPatroni's maximum_lag_on_failover and PostgreSQL's wal_segsize need to be balanced between availability and durability:\n maximum_lag_on_failover defaults to 1MB (1048576 bytes), meaning if a node lags beyond this value, it won't be chosen as the new primary. This typically works in conjunction with loop_wait and ttl parameters. For example, with a ttl of 30, if a Patroni node fails to renew with Etcd or Consul within 30 seconds, it loses leadership. With loop_wait set to 10 seconds, Patroni performs its main operation loop every 10 seconds, including status checks and necessary operations. Worst-case data loss: maximum_lag_on_failover bytes + logs written in the last TTL seconds. Reducing this value lowers the upper limit of data loss during failover but increases the chance of automatic failover being rejected due to unhealthy replicas (too far behind). wal_segsize parameter defines the size of each WAL log segment file, defaulting to 16MB  Architecture There are many PostgreSQL high availability architectures available. Here are two common architectures, corresponding to self-hosted PostgreSQL and cloud-hosted PostgreSQL typical architectures:\n Pigsty Cloudnative-PG HA  Pigsty HA Architecture The following diagram is from pigsty: From top to bottom:\n Application layer resolves DNS to vip-manager's VIP, vip-manager gets the current primary's IP address through etcd, then binds the L2 VIP to the primary node; HAProxy handles L5 layer port forwarding.  Patroni: Synchronizes primary node information to etcd. vip-manager: Virtual IP and state managed synchronously by etcd. HAProxy: Routes based on ports  5433: Connects to PGBouncer pool for primary read/write 5434: Connects to PGBouncer pool for replica read-only 5436: Direct connection to primary for management 5438: Direct connection to replica for management, connects to dedicated replicas not handling online read traffic, used for ETL and analytical queries.     Primary and replica synchronize WAL logs through Streaming Replication, primary sends WAL logs to replica via pg_receivexlog, replica replays WAL logs via pg_replay. Patroni performs backups through pgBackRest, backup data can be stored locally, in remote s3 or minio storage, refer to the documentation.    PostgreSQL uses standard streaming replication to set up physical replicas, with replicas taking over when the primary fails. Patroni manages PostgreSQL server processes and handles high availability matters. Etcd provides distributed configuration storage (DCS) capability and is used for leader election after failures Patroni relies on Etcd to reach cluster leader consensus and provides health check interfaces. HAProxy exposes cluster services and automatically distributes traffic to healthy nodes using Patroni health check interfaces. vip-manager provides an optional layer 2 VIP, gets leader information from Etcd, and binds the VIP to the node hosting the cluster's primary. With primary-replica architecture + automatic failover + synchronous streaming replication + pgBackRest backup, RTO is within 1 minute and RPO is 0, meaning recovery within 1 minute with no data loss.   Cloudnative-PG HA Architecture Based on Kubernetes container orchestration characteristics, Cloudnative-PG HA architecture adopts a more modern approach:\n Multi-region Kubernetes deployment PostgreSQL nodes deployed across multiple availability zones (three or more) Primary-Standby using synchronous or asynchronous Streaming Replication PostgreSQL instances don't share resources, have dedicated node resources, run on different Kubernetes worker nodes, use local volumes Application layer provides rw, ro, r services for connecting to primary node, hot standby replicas for read-only workloads, and any read-only workloads respectively; during failover, it automatically updates services to point to the promoted service, ensuring seamless traffic redirection from applications. Provides Pooler object to create PGBouncer connection pools for connecting to primary and read-only nodes Deploys PostgreSQL across multiple Kubernetes clusters through Replica Cluster  Reduces global Recovery Point Objective (RPO) by storing PostgreSQL backup data across multiple locations, regions, and potentially different providers (disaster recovery) Reduces global Recovery Time Objective (RTO) by leveraging PostgreSQL replication outside the primary Kubernetes cluster (high availability) Designated primary cluster can be promoted at any time, making the replica cluster the primary cluster accepting write connections.   WAL archiving through s3 Backups through barman, can backup to cloud object storage like s3 or use Volume Snapshot  Under this architecture, cross-region disaster recovery provides approximately 5 minutes RPO at most, with synchronous Streaming Replication achieving 0 RPO and extremely low RTO.\nSupabase Backup  Click me ```mermaid graph TD; A(Supabase Backup)---\u0026gt;B(Pro); B(Pro)---\u0026gt;E(Database Size 0-40GB); B(Pro)---\u0026gt;F(Database Size 40GB+); B(Pro)---\u0026gt;G(PITR); B(Pro)---\u0026gt;H(Read Replica); E(Database Size 0-40GB)---\u0026gt;I(Logical Backup); F(Database Size 40GB+)---\u0026gt;J(Physical Backup); G(PITR)---\u0026gt;J(Physical Backup); H(Read Replica)---\u0026gt;J(Physical Backup); A(Supabase Backup)---\u0026gt;C(Team); C(Team)---\u0026gt;K(Database Size 0-40GB); C(Team)---\u0026gt;L(Database Size 40GB+); C(Team)---\u0026gt;M(PITR); C(Team)---\u0026gt;N(Read Replica); K(Database Size 0-40GB)---\u0026gt;I(Logical Backup); L(Database Size 40GB+)---\u0026gt;J(Physical Backup); M(PITR)---\u0026gt;J(Physical Backup); N(Read Replica)---\u0026gt;J(Physical Backup); A(Supabase Backup)---\u0026gt;D(Enterprise); D(Enterprise)---\u0026gt;O(Database Size 0-40GB); D(Enterprise)---\u0026gt;P(Database Size 40GB+); D(Enterprise)---\u0026gt;Q(PITR); D(Enterprise)---\u0026gt;R(Read Replica); O(Database Size 0-40GB)---\u0026gt;J(Physical Backup); P(Database Size 40GB+)---\u0026gt;J(Physical Backup); Q(PITR)---\u0026gt;J(Physical Backup); R(Read Replica)---\u0026gt;J(Physical Backup); ```     Click me ```mermaid graph TD; A(Supabase Backup)--\u0026gt;B(Pro); A(Supabase Backup)--\u0026gt;C(Team); A(Supabase Backup)--\u0026gt;D(Enterprise); B(Pro)--\u0026gt;E(Daily Backup, Retain 7 days); E--\u0026gt;H(pg_dumpall logical backup， when database size \u0026gt; 40GB will use physical backup); C(Team)--\u0026gt;F(Daily Backup, Retain 2 weeks); F--\u0026gt;H(pg_dumpall logical backup， when database size \u0026gt; 40GB will use physical backup); D(Enterprise)--\u0026gt;G(Daily Backup, Retain 1 month); D--\u0026gt;J(physical backup); ```     Users can access the daily generated logical backup SQL files for restore.\n Click me ```mermaid graph LR; A(Supabase PITR)--\u0026gt;B(WAL-G, archiving Write Ahead Log files, default 2 min or certain file size threshold and physical backups); B--\u0026gt;C(2 minutes RPO); C--\u0026gt;D(show database restore available from and latest restore available at); ```      Click me ```mermaid graph LR; A(PGVecto.rs Cloud PITR)--\u0026gt;B(barman-cloud-wal-archive archiving Write Ahead Log files, default 5 min or certain file size threshold and barman-cloud-backup for physical backups); B--\u0026gt;C(5 minutes RPO); C--\u0026gt;D(show database restore available from and latest restore available at); D--\u0026gt;E(delete cluster will delete all wal and physical backups); ```     References  https://pigsty.io/ https://cloudnative-pg.io/ https://www.cnblogs.com/xianghuaqiang/p/14792001.html https://docs.pgbarman.org/release/3.10.1/ https://github.com/cloudnative-pg/cloudnative-pg/discussions/3145 https://supabase.com/blog/postgresql-physical-logical-backups  ","date":"2024-07-26","permalink":"https://blog.xieydd.top/en/postgres-ha/","tags":["High Availability","Postgres"],"title":"PostgreSQL High Availability"},{"categories":["tech","2022","kubernetes","documentation"],"contents":"Recently, I've been working on some NUMA-aware scheduling tasks on an internally developed platform, involving the discovery of Kubernetes node resource topology and scheduling. However, due to my limited knowledge, I often find myself struggling to grasp the full picture. This article is an attempt to summarize and organize my understanding.\nWhy Topology Awareness is Needed According to the official Kubernetes documentation, more and more systems are utilizing CPUs and hardware accelerators like GPUs and DPUs to support low-latency tasks and high-throughput parallel computing tasks.\nHowever, the explanation seems a bit unclear. The fundamental reason lies in the issues brought by the Von Neumann architecture. As the saying goes, there is no silver bullet. The Von Neumann architecture separates memory and processors, with both instructions and data stored in memory, laying the foundation for the universality of modern computing. However, it also poses a hidden risk: as memory capacity increases exponentially, data transfer between the CPU and memory becomes a bottleneck. Currently, most devices in servers are connected via high-speed PCIe buses, and the bus layout may vary depending on the server's purpose. As shown in the figure below (found online, not drawn by me), in the left diagram, GPUs reside in different PCIe domains, making direct P2P copying between GPU memories impossible. To copy memory from GPU 0 to GPU 2, it must first be copied via PCIe to the memory connected to CPU 0, then transferred to CPU 1 via QPI link, and finally transferred to GPU 2 via PCIe again. This process adds significant overhead in terms of latency and bandwidth, whereas the right diagram can achieve ultra-high-speed communication through GPU P2P connections. In summary, topology affects communication between devices, impacting business stability and efficiency, necessitating some technical means to make businesses topology-aware.\n  PCIe Topo (figure 1)   Types of Topology Currently, the types of topology that need to be aware of include:\n GPU Topology Awareness NUMA Topology Awareness  GPU Topology Manager There are several implementation solutions in the industry:\n Volcano GPU Topology Awareness Baidu Intelligent Cloud GPU Topology-Aware Scheduling  Volcano is not fully implemented yet, and Baidu's solution is closed-source, so we can only get a glimpse through shared information.\nWhy Why is GPU topology awareness needed? Let's start with a diagram from NVIDIA, which describes the topology of the mainstream V100 GPU graphics card in servers.\n  GPU Topo (figure 2)   Each V100 GPU has 6 NVLink channels, and 8 GPUs cannot achieve full connectivity, with a maximum of 2 NVLink connections between 2 GPUs. For instance, there are 2 NVLink connections between GPU0 and GPU3, and between GPU0 and GPU4, while there is only one NVLink connection between GPU0 and GPU1, and no NVLink connection between GPU0 and GPU6. Therefore, communication between GPU0 and GPU6 still requires PCIe. The unidirectional communication bandwidth of NVLink is 25 GB/s, and the bidirectional bandwidth is 50 GB/s, while the communication bandwidth of PCIe is 16 GB/s. Thus, if GPUs are allocated incorrectly during GPU training, such as a training task Pod requesting two cards, GPU0 and GPU6, cross-GPU communication may become a bottleneck for the training task.\nTopology information can be viewed by executing the following command on a node:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # nvidia-smi topo -m GPU0\tGPU1 GPU2 GPU3 GPU4\tGPU5 GPU6 GPU7 GPU0\tX PIX\tPHB PHB SYS\tSYS\tSYS SYS GPU1\tPIX\tX PHB PHB SYS\tSYS\tSYS SYS GPU2\tPHB\tPHB\tX PIX SYS\tSYS\tSYS SYS GPU3\tPHB\tPHB\tPIX X SYS\tSYS\tSYS SYS GPU4\tSYS\tSYS\tSYS SYS X PIX\tPHB PHB GPU5\tSYS\tSYS\tSYS SYS PIX\tX PHB PHB GPU6\tSYS\tSYS\tSYS SYS PHB\tPHB\tX PIX GPU7\tSYS\tSYS\tSYS SYS PHB\tPHB\tPIX X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge) PIX = Connection traversing a single PCIe switch NV# = Connection traversing a bonded set of # NVLinks   How I won't parse it all here, as there isn't a complete implementation to look at. I'll outline some general ideas based on my understanding.\nThe first step is awareness, which involves using a daemon component to gather information on NVIDIA GPUs, network topology, NVLink, and PCIe. The second step is the scheduler, which defines strategies. Strategy 1: Preferably schedule GPUs with the most NVLinks under the same NUMA node to a Pod; Strategy 2: Preferably allocate GPUs and network cards under the same PCI switch to the same Pod. The general process is as follows:\n GPU-device-plugin or other daemon processes construct the GPU topology information CRD for the node; The Pod defines a topology strategy, such as Strategy 1 or Strategy 2; The newly defined scheduler filters nodes that do not meet the strategy during the filter and priority phases and scores nodes that meet the strategy highly; The discovery and update of GPU devices on the node are handled by the device-plugin and kubelet, as referenced in this article.  Currently, GPU topology information can be queried through the official nvml (NVIDIA Management Library) interface.\nNUMA Topology Awareness Why When discussing NUMA topology awareness, it's essential to first explain what NUMA is and why it needs to be aware of it.\n  NUMA Topo (figure 3)     CPU Cache Latency (figure 4)   The two figures above provide the answer. Modern CPUs often use the NUMA architecture, which stands for \u0026ldquo;Non-Uniform Memory Access.\u0026rdquo; Why use non-uniformity? Isn't uniformity better? The answer is no, because if UMA (Uniform Memory Access) is used, as the number of physical cores on the northbridge increases and CPU frequency rises, the bus bandwidth cannot keep up, and conflicts over accessing the same memory will become more severe. Returning to the NUMA architecture, each NUMA node has its own physical CPU cores, and the cores within each NUMA node also share the L3 Cache. Additionally, memory is distributed across each NUMA node. Some CPUs with hyper-threading enabled will present two logical cores for each physical CPU core in the operating system.\nFrom a business perspective, if programs run on the same NUMA node, they can better share some L3 Cache, which has a very fast access speed. If the L3 Cache is not hit, data can be read from memory, significantly reducing access speed.\nIn today's container-dominated world, the issue of incorrect CPU allocation is particularly severe. Because nodes are now oversold, with many containers running simultaneously, what happens if the same process is allocated to different NUMA nodes:\n CPU contention leads to frequent context switching time; Frequent process switching causes CPU cache failures; Cross-NUMA memory access results in more severe performance bottlenecks.  In summary, in modern CPU architectures, if NUMA topology relationships are not considered, incorrect CPU allocation can lead to performance issues, affecting business SLAs.\nHow The previous section explained why NUMA-aware scheduling is needed. So how can NUMA topology be sensed, and what existing solutions are available? Here, I briefly list some projects in the Kubernetes ecosystem. If you have any additions, feel free to comment:\n Kubernetes Topology Manager Official Crane NUMA Topology Awareness Koordinator Fine-grained CPU Orchestration  Kubernetes Topology Manager The Topology Manager is a kubelet component designed to coordinate a set of components responsible for these optimizations. The Topology Manager addresses a historical issue where the CPU Manager and Device Manager worked independently and were unaware of each other. Let's first look at the implementation of the Kubernetes Topology Manager. I don't want to reinvent the wheel here, so you can refer to a great article summarized by a colleague from Alibaba. Here's a summary:\n Find the topology hints for different resources, i.e., topology information. The CPU selection criteria prioritize the smallest number of NUMA nodes involved, with the smallest number of sockets involved as a secondary priority. The device manager prioritizes the smallest number of NUMA nodes involved while meeting resource requests. Merge hints from different topology types, i.e., union, to select the optimal strategy.  If a selection is made, that's good. If not, what happens? Kubernetes provides kubelet configuration strategies:\n best-effort: The Kubernetes node will accept the Pod, but the effect may not meet expectations. restricted: The node will refuse to accept the Pod, and if the Pod is rejected, its status will become Terminated. single-NUMA-node: The node will refuse to accept the Pod, and if the Pod is rejected, its status will become Terminated. This is more restrictive than restricted, as the selected NUMA node count must be 1.  Therefore, we see that the Kubernetes Topology Manager is still centered around NUMA, performing complete fair shortest path selection for different resources (NIC, GPU, CPU). Moreover, this process occurs after the Pod is scheduled to a specific node, which brings several issues:\n There is a high probability that the Pod will be Terminated, making it unusable in production. Configuring the topology-manager-policy on nodes is inconvenient, as kubelet needs to be restarted every time parameters are configured. In some special versions, this may restart all node Pods, as detailed in this article.  So, we think of several optimization solutions:\n A daemon process similar to kubelet can discover topology relationships and expose them externally; Topology awareness can be placed in the kube-scheduler to sense and guide scheduling when assigning nodes to Pods; Provide declarative and flexible topology manager policies.  The following topology-aware solutions are based on the above ideas.\nCrane NUMA Topology Awareness Let's first look at the architecture diagram of Crane NUMA-aware scheduling.   Crane NUMA Topology Aware (figure 5)   The general process is as follows:\n Crane-Agent collects resource topology from nodes, including NUMA, Socket, and device information, and aggregates it into the NodeResourceTopology custom resource object. Crane-Scheduler references the NodeResourceTopology object of the node to obtain detailed resource topology structure during scheduling, and allocates topology resources to the Pod while scheduling it to the node, writing the results into the Pod's annotations. Crane-Agent watches the Pod being scheduled on the node, retrieves the topology allocation results from the Pod's annotations, and performs fine-grained CPUSet allocation according to the user-defined CPU binding strategy.  This essentially addresses the shortcomings of the Kubernetes Topology Manager mentioned above. However, how do we configure strategies? Here are two strategy configuration options:\n Business side, specify strategies by labeling Pods:  none: This strategy does not perform special CPUSet allocation, and the Pod will use the node's CPU shared pool. exclusive: This strategy corresponds to kubelet's static strategy, where the Pod will exclusively occupy CPU cores, and no other Pod can use them. NUMA: This strategy specifies a NUMA Node, and the Pod will use the CPU shared pool on that NUMA Node. immovable: This strategy fixes the Pod on certain CPU cores, but these cores belong to the shared pool, and other Pods can still use them.   Node side  The default cpu manager policy is static, allowing Pods with certain resource characteristics on the node to have enhanced CPU affinity and exclusivity; the topology manager policy is SingleNUMANodePodLevel. If the node does not have the topology.crane.io/topology-awareness label, the topology manager policy is none.    There is a particularly notable feature here. By default, kubelet's static CPU manager strategy only applies to Pods with qos as guaranteed and resource requests as integers, and the allocated CPUs cannot be occupied by other processes. However, with the cooperation of crane-agent and Crane NUMA-aware scheduling, Pods and bound-core Pods can share resources, allowing for the benefits of fewer context switches and higher cache affinity with bound cores, while also allowing other workloads to share resources, improving resource utilization. Moreover, the requirements for Pods are relaxed, as any container with a CPU limit greater than or equal to 1 and equal to the CPU request can be set to bind cores. Let's experiment:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $ cat nginx apiVersion: v1 kind: Pod metadata: name: nginx annotations: topology.crane.io/topology-awareness: \u0026#39;true\u0026#39; topology.crane.io/cpu-policy: \u0026#39;immovable\u0026#39; spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: requests: cpu: 2 memory: 1Gi limits: cpu: 2 memory: 1Gi $ k exec -it nginx /bin/bash $ taskset -cp 1 # 查看绑核 pid 1\u0026#39;s current affinity list: 0,1 # 查看 burstable pod 的 cpuset 信息 $ cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2260198d_db73_41f0_8ae3_387e09d3b9ec.slice/cri-containerd-6a5dfa37f9ce9102e1f781160d1fecb11b17dc835e5d72b9d7f573b515af86b3.scope/cpuset.cpus 0-9 # change to exclusive annotations: topology.crane.io/topology-awareness: \u0026#39;true\u0026#39; topology.crane.io/cpu-policy: \u0026#39;exclusive\u0026#39; $ cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2260198d_db73_41f0_8ae3_387e09d3b9ec.slice/cri-containerd-6a5dfa37f9ce9102e1f781160d1fecb11b17dc835e5d72b9d7f573b515af86b3.scope/cpuset.cpus 2-9   As expected, for those unfamiliar with cpuset, you can refer to Linux Cgroup Introduction: cpuset.\nKoordinator Fine-grained CPU Orchestration Koordinator and Crane have similar architectures in NUMA awareness, with koordlet replacing crane-agent and koord-scheduler replacing crane-scheduler. Even the CRD describing node topology is named the same, NRT. Here are a few different points:\n Koordinator supports more CPU manager strategies, in addition to static, it also supports the full-pcpus-only strategy for requesting complete physical cores, and the distribute-cpus-across-NUMA strategy for even distribution when multiple NUMAs are required. Koordinator supports more scheduling strategies based on NUMA topology, such as bin-packing, which prioritizes scheduling to a single node or the most idle node. Additionally, Koordinator has finer granularity in PodQos and CPU manager compared to Crane, which is why it's called Fine-grained CPU Orchestration. I'll write a separate article to explain it in detail later.  References Standing on the shoulders of giants, thanks again.\n https://kubernetes.io/ https://github.com/volcano-sh/volcano/ https://mp.weixin.qq.com/s/uje27_MHBh8fMzWATusVwQ https://www.infoq.cn/article/tdfgiikxh9bcgknywl6s https://github.com/NVIDIA/go-nvml https://gocrane.io/docs/ https://koordinator.sh/docs/user-manuals https://www.likakuli.com/posts/kubernetes-kubelet-restart/ https://zhuanlan.zhihu.com/p/121588317  ","date":"2022-12-29","permalink":"https://blog.xieydd.top/en/kubernetes-topo-aware-all-you-need-know/","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes"},{"categories":["tech","2024","vector search","Postgres"],"contents":"It has been over a year since I joined Tensorchord, and I haven't had the time to sit down and write some articles. Mainly because after having my daughter Tongtong, things have become much busier. During this time, I also experienced the pivot of the business from Serverless model inference Modelz to the vector search field VectorChord. The experience of this pivot might be shared in future articles, and those interested can also directly contact me. Recently, I have been developing VectorChord Cloud, so I am summarizing the ins and outs of vector databases while learning.\n1. What is a Vector The meaning of vectors in physics, mathematics, and computer science is different. Here, vectors mainly refer to vectors in computer science, which are an ordered set of numerical values. In computer science, vectors are usually used to represent data. For example, in machine learning, we usually convert an image into a vector or tokenize a piece of text and then convert it into a vector for training. In vector databases, we usually convert an image, a piece of text, or an audio segment into a vector through an embedding model and then store and retrieve it. Below is a simple example where we convert a piece of text into a vector using the all-MiniLM-L6-v2 model. all-MiniLM-L6-v2 maps sentences and paragraphs to a 384-dimensional dense vector and can be used for tasks such as clustering or semantic search.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  from sentence_transformers import SentenceTransformer model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) sentences = [ \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34;, \u0026#34;I love natural language processing.\u0026#34;, \u0026#34;Transformers are state-of-the-art models for NLP tasks.\u0026#34; ] # generate embeddings embeddings = model.encode(sentences) # print the embeddings for sentence, embedding in zip(sentences, embeddings): print(f\u0026#34;Sentence: {sentence}\u0026#34;) print(f\u0026#34;Embedding: {embedding}\\n\u0026#34;)   In summary, vectors are actually the bridge between real-world entities and the computer world. Computers understand and process real-world data through vectors.\n2. What is a Vector Database The world originally did not have vector databases, but with more vectors, vector databases emerged, just kidding hh. Here I give a simple definition: a database that can index and store vectors to achieve fast retrieval and similarity search functions. Many people on the internet define vector databases as databases that focus on processing vector data, which is not accurate. To be precise, vectors and vector search are a new data type and query processing method, which is not fundamentally different from similar and indexing methods in traditional databases.\n3. What is Vector Search Vector search, also known as vector retrieval, is a technique in Information Retrieval used to find the most similar vectors to a given query vector in a high-dimensional vector space. To measure the similarity between two vectors, we usually use cosine similarity, Euclidean distance, Manhattan distance, etc. To speed up vector search, we usually use index structures such as KD-Tree, IVF (Inverted File Index), HNSW (Hierarchical Navigable Small World), etc. Vector search has applications in many fields, such as in recommendation systems, where we can use vector search to find products most similar to a user's historical behavior and then recommend them to the user; in image retrieval, we can use vector search to find images most similar to a given image; in RAG (Retrieval Augmented Generation), we can use vector search to find text most similar to a given question, enhancing the Context of large models to improve the quality of generated answers.\n3.1 Vector Search Application Scenarios 3.1.1 Recommendation System As in the On-premise case of Qdrant about Video Content-based Recommendation, the multilingual universal sentence encoder is used to embed the script when uploading a video. Here, it is not simply extracting frames from the video, but more information comes from the video title, description, automatically detected tags, and content recognized by Whisper speech recognition. So the current problem is that if the video has no audio, the title and description are forced to be used for recommendation, which is a big challenge for the review team. Here, the call start issues in the recommendation field are mentioned, that is, when users first start using the system, the recommendation quality of the recommendation system is not high, and the user experience is poor at this time. On the basis of non-real-time updated collaborative recommenders and metadata recommenders, adding a content-based recommender can greatly optimize call start issues.\n3.1.2 Image Retrieval immich is a high-performance open-source self-hosted image and video management solution. Imagine when you upload all your videos and images to immich, it is difficult to find the image or video you want in a short time. At this time, an efficient image retrieval system smart search is needed. Through vector search technology, you can quickly and accurately find the image or video you want through text descriptions and additional filters (tags, dates, etc.).\n   Images from immich\n 3.1.3 RAG RAG (Retrieval Augmented Generation) mainly solves several problems in LLM applications:\n The data used to train LLM models is not real-time, in other words, it is static data, and the cost of obtaining the latest data and retraining is too high. LLM lacks domain-specific knowledge because the training corpus of LLM is mostly general datasets on the internet. In fields such as finance, healthcare, and law, private data may be the most important, and the lack of domain data will cause LLM to hallucinate. The black box problem of LLM, we cannot know how LLM generates answers, and where the source of the answers comes from.  Here, I borrow two diagrams from Paul lusztin and Aurimas Griciunas to explain how RAG works:\n Obtain streaming real-time data of financial news and historical data. Chunk the data into inputs for the embedding model, and then store the embeddings in the vector database. User asks a question. Find the most similar news chunks through vector search, and then perform Prompt composition with the user's historical chat information and news chunks. Input into the LLM to generate an answer. Return the answer to the user. Store the new chat information in the user's historical data.   Private data, such as Notion, Jira, local PDF files, etc., are chunked into inputs for the embedding model. Input the chunks into the embedding model, and then store the embeddings in the vector database. The Vector Database builds an Index. User asks a question, input into the embedding model. The embedding model outputs the query's embedding vector. Use the vector from step 5 as the Query vector and input it into the vector database. The vector database finds the most similar chunks through ANNs (Approximate Nearest Neighbors Search). Construct a Prompt with the searched chunks and the query. Input into the LLM to generate an answer.  3.2 Similarity Metrics Cosine similarity is a method used to measure the similarity between two vectors. It is measured by calculating the angle between two vectors. The range of cosine similarity is [-1, 1], where 1 means the angle between two vectors is 0 degrees, indicating that the two vectors are identical; -1 means the angle between two vectors is 180 degrees, indicating that the two vectors are completely opposite; 0 means the angle between two vectors is 90 degrees, indicating that there is no similarity between the two vectors. The calculation formula is as follows:\n This formula calculates the cosine value of the angle between vectors 𝐴 and 𝐵.\nEuclidean distance is a method used to measure the similarity between two vectors. It is measured by calculating the distance between two vectors. The range of Euclidean distance is [0, ∞], where 0 means the two vectors are identical, and the larger the value, the greater the difference between the two vectors. The calculation formula is as follows:\n This formula calculates the Euclidean distance between vectors 𝐴 and 𝐵. Some do not take the square root, which only changes the numerical value but does not fundamentally differ.\nNegative inner product is measured by calculating the inner product between two vectors. The larger the value, the higher the similarity between the two vectors. The calculation formula is as follows:\n Manhattan distance (taxicab distance) is measured by calculating the distance between two vectors. The range of Manhattan distance is [0, ∞], where 0 means the two vectors are identical, and the larger the value, the greater the difference between the two vectors. The calculation formula is as follows:\n 3.3 Vector Search Algorithms Intuitively, we can find the vector most similar to a given query vector by traversing all vectors, but the time complexity of this method is O(n), which is not feasible when the number of vectors is large. To speed up vector search, we usually use index structures such as IVF (Inverted File Index), HNSW (Hierarchical Navigable Small World), etc. Through ANNs (Approximate Nearest Neighbors Search) algorithms, we can find the vector most similar to a given query vector with lower time complexity, such as O(log(n)).\n3.3.1 #### LSH (Locality Sensitive Hashing)\nLocality Sensitive Hashing (LSH) works by processing each vector with a hash function, grouping vectors into buckets, thereby maximizing hash collisions, rather than minimizing collisions as usual hash functions do.\nHere is a diagram from Pinecone: The specific details of LSH are as follows:\n Shingling: Use k-shingling and one-hot encoding to convert text into sparse vectors.  k-shingling means using a sliding window of size k to extract k consecutive characters from the text. one-hot encoding means comparing the result of k-shingling with the vocabulary, and if it exists, it is represented as 1 in the vocabulary, otherwise 0.    Then use MinHash to create a \u0026ldquo;signature\u0026rdquo;.  Create a random permutation of [1\u0026hellip;len(voc)+1]. Use the value from top to bottom in the random permutation as the index. If the index-1 position of the original sparse vector is 1, take the index-1 position number of the random permutation as the signature value. Repeat n times to get an n-dimensional dense vector.    Band and Hash  Divide the n-dimensional signature vector into b groups, each with r elements. Hash each group to get b hash values. If two vectors have the same hash value, put them in the same bucket. If in the same bucket, consider them as candidate pairs.     Here, as b increases, more candidate pairs are returned, which naturally leads to more false positives.\n This means that as the dimension increases, the possibility of false positives increases, and more hash buckets need to be maintained, which also increases storage overhead. Therefore, LSH is more suitable for low-dimensional vector search and is not the mainstream vector search algorithm.\n3.3.2 IVF（Inverted File Index） The inverted index algorithm is a simple, easy-to-understand, and very easy-to-implement algorithm, and it has a good search speed, but the search accuracy is worse than HNSW, but the memory consumption is relatively less than HNSW.\nThe core of building an IVF index is divided into two steps:\n Use a clustering algorithm to divide the vectors into nlist clusters. Assign the vectors to the corresponding clusters.  When searching, set the number of cells to search nprobe.\n The impact of the parameters here is:\n Increasing nlist will slow down the index building speed because the vectors need to be calculated with more centroids during the clustering process; at the same time, it will reduce the search time because there are fewer vectors corresponding to the centroids, making knn faster. Increasing nprobe will improve the recall rate but will reduce the search speed because more cells need to be searched.   3.3.3 HNSW (Hierarchical Navigable Small World) HNSW combines the advantages of NSW and Skip List and is an efficient vector search algorithm. The core idea of HNSW is to build a multi-layer graph, where each layer is a small world. By searching for the nearest nodes in each layer and then searching for the nearest nodes in the next layer, the vector most similar to the given query vector is finally found.\nNSW is based on a theory that the distance from any point to any other point on NSW is finite and can be found with a few jumps.\nThe construction process of NSW:\n Randomly select a point as the insertion point. Find the m nearest points to the insertion point. Connect the insertion point with the m points.  The randomness here will increase the number of long connections in the early graph, speeding up the search, which can be understood as \u0026ldquo;highways\u0026rdquo;. The red lines in the figure below are long connections: The search process of NSW is as follows, here I borrow a diagram from Zhihu user \u0026ldquo;工牌厂程序猿\u0026rdquo;:\n  Initialize three sets, namely visited, candidate, result (fixed length); randomly select the initial point to enter and add it to the visited and candidate sets, candidate saves the distance to the query point. Find the n nearest neighbors of the initial point, add them to the visited set, note that if the friend point is in the visited set, it is discarded, calculate the distance to the query point for the n nearest neighbors in parallel, sort them in ascending order (from near to far) and add them to the candidate set. Find the n nearest neighbors of the candidate set, add them to the visited set, if they are already in the visited set, discard them; here query point C, only point D is not visited, because the distance from point D to the query point is less than the distance from point C to the query point, so replace point C with point D in the result set, and replace point C with point D in the candidate set. Repeat step 3, find the n nearest neighbors of D, add them to the visited set, if they are already in the visited set, discard them; here query points E and G, because the distance from point E to the query point is less than the maximum distance in the result set, replace point H with point E in the result set, and remove point E from the candidate set. Repeat step 3, the distance from the point with the smallest distance to the query in the candidate set H is greater than the distance from the point with the largest distance to the query in the result set E, then stop the query.  Skip List is an efficient data structure that can find the vector most similar to a given query vector in O(log(n)) time complexity. The core idea of Skip List is to build a multi-layer linked list, where each layer is an ordered linked list. By searching for the nearest nodes in each layer and then searching for the nearest nodes in the next layer, the vector most similar to the given query vector is finally found.\nHere are a few points to note about HNSW:\n Note that the maximum number of connections per layer Max needs to be controlled. When randomly inserting nodes (the lower the layer, the greater the probability), if the number of connections of neighbor node N is greater than Max, perform a KNN search on N and re-establish connections with new neighbors. Heuristic edge selection strategy: When searching for the M nearest nodes to the insertion point in each layer, it first recalls efConstruction nodes and then selects M nodes (efConstruction \u0026gt;= M). The process of selecting M can directly select Top-M, but it may reduce the overall connectivity. The article by \u0026ldquo;工牌厂程序猿\u0026rdquo; specifically lists this case:   Here, efConstruction is 4, M is 2. If Top-M is directly selected, A and B will definitely be selected, which reduces the connectivity between ABQ and CD. Here, after selecting A, when looking for the second nearest neighbor, check the distance between QA and AB. If QA \u0026gt; AB, then look for the next nearest neighbor until it is greater than QA. Here, point C is found when AC \u0026gt; AQ. 3. High degree vertex is closer to the top layer, which can reduce the search path and improve search efficiency.\nConstruction parameters:\n efConstruction: A parameter in the graph construction process, used to control the number of nearest neighbor candidate nodes considered when establishing connections for each node. This parameter specifically affects the quality of the connections between nodes during the graph construction process. A higher efConstruction value means that more candidate nodes are considered when selecting neighbors for a node, resulting in a higher quality graph structure. However, a higher efConstruction value will increase the time and space complexity of building the graph, and it will also increase the search time. m: The maximum number of neighbors added per vertex, divided into m_0=2m and m_max=m, see code.  Search parameters:\n efSearch: Used to control the quality of the search. A higher efSearch value means that more candidate nodes are considered during the search, thereby improving the quality of the search. However, a higher efSearch value will increase the search time.  HNSW is fully in-memory because the retrieval process involves an average of hundreds of disk read operations per query, and constantly looking for the next random point will cause extremely high latency on SSD.\nNSG (Navigating Spreading-out Graph) NSG optimizes graph connectivity, reduces average out-degree, shortens search paths, and reduces graph size, proposing a new graph structure called Monotonic Relative Neighborhood Graph (MRNG).\nThe specific process is as follows:\n Build a K-nearest-neighbor-graph (KNNG) as the basis for graph construction. Randomly select a point as the Navigation Point, and all newly inserted nodes will add the Navigation Point to the candidate set when selecting edges. During the graph construction process, subgraphs will gradually be connected to the Navigation Point, so other nodes only need to maintain a few edges, thereby reducing the size of the graph. Each search starts from the Navigation Point and can point to specific subgraphs, reducing invalid searches and achieving better search performance.  NSG's edge selection is different from HNSW's minimum edge selection strategy. Taking point r as an example, when r is connected to p, circles are drawn with r and p as the center and the distance between r and p as the radius. If there are no other points connected to p within the intersection of the two circles, then r is connected to p. When connecting point s, since there is already point r connected to p within the intersection circle of s and p, s is not connected to p. Point t is retained because point s has been excluded. In the figure below, only points r, t, and q are connected to point p, reducing redundant edges and average out-degree. 3.3.4 DiskANN The DiskANN series consists of three articles: DiskANN, FreshDiskANN, and FilterDiskANN, which are essentially optimizations of the HNSW or NSG algorithm.\nDiskANN introduces an SSD-friendly graph algorithm called Vamana, minimizing disk operations. The Vamana graph is similar to HNSW and NSG graphs, with the difference being the initial graph selection and the introduction of a loose parameter alpha during graph pruning to balance graph diameter and node connectivity, improving graph quality. Additionally, to avoid multiple random disk reads and writes, DiskANN combines two types of algorithms: clustering compression algorithms and graph structure algorithms. First, by compressing the original data, only the compressed codebook information and centroid mapping information are kept in memory, while the original data and the constructed graph structure data are stored on disk, only reading from disk when specific nodes are matched during queries. Second, by modifying the arrangement of vector data and graph structures, data points and their neighbor nodes are stored together, allowing a single disk operation to read the vector data and neighbor node information of a node.\nGraph construction process:\n First, build a random graph. Unlike NSG's K-nearest-neighbor graph, each node is randomly connected to R nodes. Calculate the starting point, finding the point closest to the global centroid to minimize the average search radius. Search from the starting point and perform ANN for each point, using all points on the search path as candidate neighbor sets, and execute an alpha = 1 edge pruning strategy. Repeat step 3 with alpha \u0026gt; 1 (recommended 1.2 in the paper). Since step 3 is based on a random neighbor graph, the graph quality is not high after the first iteration, so another iteration is needed to improve graph quality, which is important for recall. Taking the above figure as an example, if alpha is 1.2, the ps edge is pruned only when the distance of ps is greater than 1.2 * the distance of pr.  The figure below intuitively shows the difference between alpha=1 and alpha=1.2 graphs. The first row is alpha=1, and the second row is alpha=1.2. The alpha=1.2 graph is denser, with more long edges, reducing the search radius. At this point, you may have a question: if the graph is built this way, it is impossible to store more than 1B data on a 64GB machine. Here are some optimization techniques:\n First, perform global k-means to divide the data into k clusters, and then assign each point to the nearest I clusters, usually I=2 is enough. Build an in-memory Vamana index for each cluster, and finally merge the k Vamana indexes into one index. Use quantization methods, using the original vectors to build the index, but using compressed vectors for queries. Building the index with original vectors ensures graph quality, while searching with compressed vectors that can fit in memory allows for coarse-grained search. Although the compressed vectors lose some precision, as long as the graph quality is high enough, the general direction is correct, and the final distance calculation is still done with the original vectors. Store the neighbor sets and original vector data of each point together. This takes advantage of data locality.  If the index file is stored on SSD, to ensure search latency, minimize the number of disk accesses and disk read/write requests. Therefore, DiskANN proposes two optimization strategies:\n Cache hotspots: Keep points within C hops from the starting point in memory, usually C=3~4 is good. Beam search: Simply put, preload. When searching for point p, if the neighbor points of p are not in the cache, load the neighbor information of p from disk. Since a small number of SSD random access operations and a single SSD sector access operation take about the same time, we can load the neighbor information of W unvisited points at once. W should not be too large or too small; too large wastes computation and SSD bandwidth, too small increases search latency.  The advantage of DiskANN is that it can achieve good search performance with small memory usage and SSD, but building the index on small machines will be slow, so a trade-off is needed.\nIn summary, in terms of memory usage, HNSW is significantly larger than IVF, LSH, and Flat (KNN). In terms of recall rate and search speed, HNSW is better than IVF and LSH. DiskANN is better than HNSW in memory usage, but building the index is slower. In terms of search speed, DiskANN is better than HNSW, but the recall rate is not as good as HNSW. Therefore, when choosing a vector search algorithm, you need to choose based on your needs.\n3.4 Vector Search Algorithm Optimization By reducing the size of vectors or reducing dimensions to make searches faster, here are some common vector search algorithm optimization methods.\n3.4.1 PQ（Product Quantization） Here I borrow a diagram from a Zhihu user, as the user's diagram is very well-drawn: Construction phase:\n First, split N original vectors into multiple sub-vectors. For example, a 256-dimensional vector is split into 8 32-dimensional sub-vectors. Then perform clustering in each sub-vector space, using clustering algorithms such as KMeans. Assuming there are 1024 clusters in each subspace, encode each cluster center to get 1024 IDs. Encode the original vectors into the nearest cluster center ID, and finally concatenate them.  Retrieval phase:\n Split the retrieval vector. Calculate the distance between each subspace and each cluster center to create a distance table. Use the distance table to calculate the distance between the query and candidate samples in each subspace, accumulate them, and take the top-k.  The splitting involved can be done in parallel. PQ is generally not used directly because it still requires a lot of distance calculations. Usually, IVF is first used to find the most promising top-k clusters, and then PQ is performed.\n3.4.2 SQ（Scalar Quantization） SQ is relatively simple. Encoding: scalar = (max-min)/255, floor(value-min/scaler). If less than 0, take 0; if greater than 255, take 255. This compresses the vector to between 0-255, reducing the size of the vector but losing some information. Decoding: value = min + (code + 0.5)*(max-min)/255.\n3.4.3 RabitQ RabitQ comes from the paper RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search.\nRabitQ points out two problems with the current PQ algorithm:\n Using the centroid of kmeans as the codebook is a heuristic approximation during construction, with no theoretical guarantee. Distance estimation, using the distance between the quantized vector and the query vector to estimate the distance between the original vector and the query vector, lacks an approximate error range.  How to solve the above problems:\n Codebook construction phase  First, normalize the data vectors to align them on the unit hypersphere in D-dimensional space. Construct a set of $2^{D}$ bivalued vectors with coordinates $−1/\\sqrt{D}$ or $+1/\\sqrt{D}$ (i.e., the set consists of the vertices of a hypercube uniformly distributed on the unit hypersphere). Randomly rotate the bivalued vectors by multiplying each bivalued vector by a random orthogonal matrix (i.e., perform a Johnson-Lindenstrauss transformation). For each vector, take the closest vector in the codebook as the quantized vector. Since each quantized vector is a rotated D-dimensional bivalued vector, we represent its quantization code as a bit string of length D, where 0 and 1 represent two different values. The basic principle of codebook construction is that it has a clear geometric interpretation (i.e., the vectors in the codebook are a set of randomly rotated vectors on the unit hypersphere), allowing explicit analysis of the geometric relationships between data vectors, their quantized vectors, and query vectors.   Distance estimation  Carefully design an estimator for the distance between data vectors and query vectors based on the above geometric relationships, and prove that this estimator is unbiased and provides an error range. At the same time, when estimating distances, even with shorter quantization codes, about half of the advantages can be estimated with small empirical errors.    RaBitQ’s distance estimator：\n Single data vector uses bitwise operations. Batch data uses SIMD acceleration.  Using a random codebook avoids the poor performance of bivalued codebooks on specific vectors, such as ($1/\\sqrt{D}$\u0026hellip; $−1/\\sqrt{D}$) and (1, 0, 0, 0). We multiply this codebook by a random orthogonal matrix, allowing the unit vectors of the codebook to have the same probability of rotating to any position on the unit hypersphere.\n4. Common Vector Databases and Their Pros and Cons The following lists some common vector databases and their pros and cons. Some are dedicated vector databases, while others are extensions of existing relational databases.\n4.1 Milvus Milvus is an excellent open-source vector database that supports multiple vector search algorithms, including HNSW, DiskANN, IVF, etc. In addition to basic vector retrieval functions, it also provides sharding, streaming data ingestion, and hybrid search.\nMilvus adopts a cloud-native, shared-everything architecture with separate control and data planes. Each component is independent and horizontally scalable, including:\n Access Layer: Consists of a set of stateless proxies. It provides endpoints for user connections, verifies client requests, and merges and returns results.  It uses load balancing components such as Nginx, Kubernetes Ingress, NodePort, and LVS to provide a unified service address. Since Milvus uses a massively parallel processing (MPP) architecture, the proxy aggregates and post-processes intermediate results and then returns the final results to the client.   Coordinator Service: Responsible for assigning tasks to execution nodes, including root coord, data coord, and query coord.  root coord: Handles data definition language (DDL) and data control language (DCL) requests, such as creating or deleting collections, partitions, or indexes, and managing TSO (Timestamp Oracle) and time ticker. data coord: Manages data and index node topology, maintains metadata, and triggers background data operations such as flush, compact, and index building. query coord: Manages query node topology, load balancing, and the conversion of growing segments to sealed segments.   Worker Node: Executes tasks assigned by the coordinator service and proxy DML commands.  Query Node: Retrieves incremental log data, converts it into growing segments by subscribing to the log broker, loads historical data from object storage, and performs hybrid searches between vector and scalar data. Data Node: Obtains incremental log data by subscribing to the log broker, processes mutation requests, and packages log data into log snapshots stored in object storage. Index Node: Builds indexes. Index nodes do not need to reside in memory and can be implemented through a Serverless framework.   Storage Layer: Object storage is responsible for storing data, including data files and index files.  Meta Storage: Meta storage stores metadata snapshots, such as collection schemas and message consumption checkpoints. Storing metadata requires high availability, strong consistency, and transaction support, so Milvus chooses etcd for meta storage. Milvus also uses etcd for service registration and health checks. Object Storage: Stores log snapshot files, index files for scalar and vector data, and intermediate query results. Milvus uses MinIO as object storage, which can be easily deployed on AWS S3 and Azure Blob. However, object storage has high access latency and charges based on query counts. To improve performance and reduce costs, Milvus plans to implement cold and hot data separation on a memory or SSD-based cache pool. Log Broker: A publish-subscribe system responsible for streaming data persistence and event notification. It also ensures the integrity of incremental data when worker nodes recover from system failures. Milvus cluster uses Pulsar as the log broker; Milvus standalone uses RocksDB as the log broker. Additionally, the log broker can be easily replaced with streaming data storage platforms such as Kafka.    Milvus's cloud-native architecture is its advantage, but it also brings significant challenges to developers, such as learning new concepts and the operational management challenges brought by related components like Pulsar or etcd.\n4.4 Pinecone 4.5 Qdrant 4.6 Pgvector 4.7 Pgvecto.rs 4.8 VectorChord 5. Excellent Vector Search Libraries and Open-Source Vector Database Projects 6. What You Need to Know About Vector Database Commercialization 7. Summary Here, I have briefly introduced some basic knowledge of vector search, as well as some common vector search algorithms, vector search application scenarios, vector search algorithm optimization, common vector databases and their pros and cons, excellent vector search libraries, and open-source vector database projects. I hope to apply this knowledge to actual scenarios in the future. I hope this article can help you better understand vector search.\n8. References Thank you very much to Pinecone's articles, which gave me a deeper understanding of vector databases.\n https://www.pinecone.io/learn/series/faiss/vector-indexes/ https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/ https://zhuanlan.zhihu.com/p/379372268 https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/ https://www.xiemingzhao.com/posts/hnswAlgo.html https://whenever5225.github.io/2020/05/11/hnsw-heuristic/ Search Engine For AI：高维数据检索工业级解决方案 https://zhuanlan.zhihu.com/p/50143204 https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw  ","date":"2024-07-13","permalink":"https://blog.xieydd.top/en/vector-search/","tags":["vector search","Postgres"],"title":"The internals of Vector Databases"},{"categories":["Opinion","AI","DevTools"],"contents":"I recently built a security product using Code Agents—an LLM Security Insight platform based on eBPF + PostgreSQL. The tech stack was eclectic: eBPF, React, Go, Docker, Kubernetes, PostgreSQL, DuckDB. I was a complete beginner at React and eBPF, but the project shipped in a month.\nVS Code -\u0026gt; Cursor -\u0026gt; Gemini -\u0026gt; VS Code -\u0026gt; Codex -\u0026gt; VS Code -\u0026gt; Claude Code -\u0026gt; OpenCode -\u0026gt; OhMyOpenCode -\u0026gt; VS Code I'm a long-time VS Code user. First came VS Code Copilot—specify the file, describe the change, generate a patch. Code Assistant is the more accurate term; it's pair programming. In familiar domains, it's genuinely comfortable. Code completion + pair programming is how programming should be.\nAround March-April 2025, I tried Cursor. It's forked from VS Code, but details made it feel different. Multi-file editing and Agent mode. Many people scoffed at first—used to pair programming, multi-file editing extended review time, and model-generated code wasn't reliable enough to approve without review. But Cursor dared to be first. With Claude Sonnet's capabilities, Cursor went wild.\nHere's the fun part: in early 2025, Cursor co-founder Sualeh Asif emailed our team engineer keming about our reproducible dev infra tool (envd). Cursor had already foreseen consistency issues in large-scale Agent deployments. They used vector databases + Turbopuffer for code search in multi-tenant scenarios—the same approach we took with VectorChord. But after their user base exploded and free quotas got slashed, I couldn't renew without a credit card, so I moved on.\nThen came Gemini CLI. Google's resources and Infra were impressive—free, powerful tokens attracted tons of users including me. But I was busy with VectorChord Cloud, so I only used it for small fixes and tests. Human context worked well, but still no Agent feeling—just pair programming.\nAfter hitting Gemini's quota limit, I went back to VS Code. GitHub's free Copilot + VS Code Agent mode + multi-vendor support (OpenAI, Claude, Google) with Enterprise quota was enough. But I preferred multi-file editing, keeping every line of code in check.\nClaude Code finally gave me the real Agent experience. Leap in model capability meant better code and design docs. Smooth Plan/Auto/Edit mode switching let me build with planning and rhythm.\nClaude's region restrictions were brutal though—I had to get tokens through special channels. Then GLM came out, people said its coding and tool-calling were solid. They were right.\nLater, I saw good buzz about OpenCode on Twitter, tried it briefly—not bad. Then OhMyOpenCode plugin went viral on X. For users with multiple vendor models, it's genuinely useful—seamless model switching, unified workflow. But models iterate fast, plugins update constantly, managing subscriptions and tracking changes—it's exhausting.\nSo why back to VS Code? I've always used VS Code Remote to connect to my dev machine. Claude Code and Codex have official VS Code extensions, and I often need to feed images to the Agent. Conductor and similar Orchestration tools? Didn't use them much.\nMy Choices Tool Choices\n   Scenario Recommendation     Multi-vendor model switching OhMyOpenCode   Single-model deep work VS Code + Claude Code / Codex Extension    Model Choices\n   Dimension Recommendation     Aesthetics GPT-5.2-Codex \u0026gt; Gemini-3 Pro \u0026gt; Claude Opus 4.5 \u0026raquo; GLM 4.7   Planning Claude Opus 4.5, leagues ahead   Coding Claude Sonnet 4.5, GLM 4.7   Debugging Claude Opus/Sonnet 4.5, GLM 4.7    Multi-model collaboration is the future. Different models excel in different areas, combining them works best.\nHow I Work For the project mentioned above, I did it the Agent way:\n1. Maintain Project Context\n1 2 3 4 5 6 7 8 9 10 11  ~/llm-observability/context/ ├── agent-protocol-research.md ├── analyze.sql ├── llm_prompt_injection_analyzer_pg.py ├── pencil-designs/ ├── security-product-research.md └── designs/ ├── architecture.md ├── frontend.md ├── mcp.md └── ...   All research, design, and utility code go here. Agents read directly—huge efficiency win.\n2. Global Context Following Xuanwo's approach, my Global Context is agents.md—defining Agent behavior, constraints, and Best Practices. Shared across all projects.\n3. Design First Polish design docs with the Agent, lock in tech stack and architecture. The design doc is the Agent's manual—clearer is better.\n4. Validate Before Build Have the Agent write verification code first. Before committing to a PostgreSQL Schema, I had the Agent generate DDL, then ran a Demo to validate.\n5. Full Implementation Once validated, Agent implements everything. There are cases the Agent can't handle—eBPF kernel compatibility, for example. For those, I switch to Claude Sonnet 4.5 with full context to Debug directly. Efficient.\nGit Worktree is my safety net. Agents mess around in branches, main stays clean. But database schema changes are painful—Docker PG can't do concurrent schema changes, so it's serial.\nI used to worry about Git—fear the Agent would delete fine-tuned code. But as Agent quality improved, that mental burden faded.\nThoughts Fork is a Must-Have\n Code/File Level: Solved by Git. Runtime Level: Docker/envd solved personal use cases, but large-scale Agent scheduling has no perfect solution yet. Unikernel might work, but Firecracker isn't fast or mature enough. Database Level: ACID makes forking hard. Distributed databases mean data redundancy and consistency issues when forking. PostgreSQL's monolithic nature is an advantage. PITR + JuiceFS Snapshot enables PG forks—tools like piglet help.  Multi-Model Collaboration is the Future\nSkill Best Practices + MCP Boost Productivity\nContext/Memory Evolution and Sharing is the Battlefield\nAll in AI\nNext Step  Upgrade dev machine Mac Full automation: DNS, domain registration, Agent Notification Better Global Context + Memory management Large-scale Agent Runtime Infra research  ","date":"2026-02-02","permalink":"https://blog.xieydd.top/en/code-agent-2026-02-02/","tags":["Code Agent","VS Code","Claude Code","OpenCode","Workflow"],"title":"Coding at Agent-Speed"},{"categories":null,"contents":"📫 If you wish to contact me, you can send an email to xieydd@gmail.com, or add my WeChat echo -n 'eGlleWRkX2hhaGEK' | base64 -d.\n💻\nAs of 2026, I have over 8 years of experience in AI Infrastructure:\n2018-2021.2 (including internship) Unisound  At the AI algorithm company Unisound, I was responsible for the development and operation of the Atlas supercomputing platform, supporting NLP and CV model training. Key responsibilities included:   Developing a large-scale intelligent scheduling system to optimize multi-tenant resource allocation Enhancing the performance of the high-performance distributed file system Lustre Building a multi-layer cache cloud-native architecture to accelerate AI model training  Worked on 8 Bit training and inference optimization at Unisound, optimizing models for NPU and NVIDIA Edge Devices.  2021.2-2023.5 Tencent Cloud  Developed a large-scale AI platform for public cloud:   Built a high-performance, scalable elastic offline training platform using EKS (Elastic Kubernetes Service) Integrated public cloud object storage and the GooseFS accelerator to create a high-performance cache scheduling system on the cloud  Established FinOps infrastructure to help public cloud customers manage and optimize cloud costs more effectively, enhancing cloud resource utilization:   Optimized scheduling and rescheduling, identified high and low priority tasks, and implemented intelligent elastic scaling Combined Tencent's Ruyi kernel scheduler optimization and observability to optimize costs while maintaining service quality Launched a large-scale cost reduction initiative in the internal cloud, improving resource utilization through efficient resource allocation  2023.5-present Tensorchord  Leading the development of the Serverless Inference platform ModelZ on GCP, providing optimized cold start model service inference:   Reduced model service cold start time through cache model services and image preheating Implemented JuiceFS to build a high-performance cache scheduling system, enhancing model service performance  Cloud Team Leader, developing the vector database VectorChord's cloud service and customer support VectorChord Cloud:   Built a vector database based on PostgreSQL on AWS, achieving control and data plane separation, BYOC (Bring Your Own Cloud), BYOD (Bring Your Own Data) capabilities Implemented cloud-native architecture to achieve PostgreSQL storage and compute separation, high availability, Backup, PITR (Point-In-Time Recovery), In-Place Upgrade features Build an enterprise-level RAG (Retrieval-Augmented Generation) system based on PostgreSQL as the agent's memory module.  Skill set: Kubernetes, GCP, AWS, Kubeflow, FinOps, RAG, Vector Database, Storage Acceleration, Tensorflow, Pytorch, Cloud Native, MLOps, AI Infrastructure, PostgreSQL etc.\nOpen Source Projects 🌱 Currently focusing on MLOps and FinOps, contributing to several open source projects:\n fluid Fluid, elastic data abstraction and acceleration for BigData/AI applications in the cloud. (Project under CNCF) crane Crane is a FinOps Platform for Cloud Resource Analytics and Economics in Kubernetes clusters. The goal is to help users manage cloud costs more easily while ensuring application quality. crane-scheduler Crane scheduler is a Kubernetes scheduler that can schedule pods based on actual node load. creator Creator is the brain of the crane project, containing the core algorithm module and evaluation module. openmodelz One-click machine learning deployment (LLM, text-to-image, etc.) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine). clusternet [CNCF Sandbox Project] Managing your Kubernetes clusters (including public, private, edge, etc.) as easily as browsing the Internet vectorchord Scalable, fast, and disk-friendly vector search in Postgres, the successor of pgvecto.rs.  Recommended Blogs    Type Author/Company Blog URL     Infra Chris Riccomini https://materializedview.io/   Infra Jack Vanlightly https://jack-vanlightly.com/   Math and Science 苏剑林 https://kexue.fm/   AI Infra Colfax https://research.colfax-intl.com/blog/   Postgres Gabriele Bartolini https://www.gabrielebartolini.it/articles/   AI Sebastian Raschka https://magazine.sebastianraschka.com/archive?sort=new   AI Algorithm Tom Yeh https://www.byhand.ai/   AI Infra Chip Huyen https://huyenchip.com/blog/    ","date":"2022-12-29","permalink":"https://blog.xieydd.top/en/about/","tags":null,"title":"About xieydd"}]