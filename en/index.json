[{"categories":["2024","vector search","Postgres"],"content":"It has been over a year since I joined Tensorchord, and I haven't had the time to sit down and write some articles. Mainly because after having my daughter Tongtong, things have become much busier. During this time, I also experienced the pivot of the business from Serverless model inference Modelz to the vector search field VectorChord. The experience of this pivot might be shared in future articles, and those interested can also directly contact me. Recently, I have been developing VectorChord Cloud, so I am summarizing the ins and outs of vector databases while learning. ","date":"2024-07-13","objectID":"/en/vector-search/:0:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"1. What is a Vector The meaning of vectors in physics, mathematics, and computer science is different. Here, vectors mainly refer to vectors in computer science, which are an ordered set of numerical values. In computer science, vectors are usually used to represent data. For example, in machine learning, we usually convert an image into a vector or tokenize a piece of text and then convert it into a vector for training. In vector databases, we usually convert an image, a piece of text, or an audio segment into a vector through an embedding model and then store and retrieve it. Below is a simple example where we convert a piece of text into a vector using the all-MiniLM-L6-v2 model. all-MiniLM-L6-v2 maps sentences and paragraphs to a 384-dimensional dense vector and can be used for tasks such as clustering or semantic search. from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2') sentences = [ \"Hugging Face is creating a tool that democratizes AI.\", \"I love natural language processing.\", \"Transformers are state-of-the-art models for NLP tasks.\" ] # generate embeddings embeddings = model.encode(sentences) # print the embeddings for sentence, embedding in zip(sentences, embeddings): print(f\"Sentence: {sentence}\") print(f\"Embedding: {embedding}\\n\") In summary, vectors are actually the bridge between real-world entities and the computer world. Computers understand and process real-world data through vectors. ","date":"2024-07-13","objectID":"/en/vector-search/:1:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"2. What is a Vector Database The world originally did not have vector databases, but with more vectors, vector databases emerged, just kidding hh. Here I give a simple definition: a database that can index and store vectors to achieve fast retrieval and similarity search functions. Many people on the internet define vector databases as databases that focus on processing vector data, which is not accurate. To be precise, vectors and vector search are a new data type and query processing method, which is not fundamentally different from similar and indexing methods in traditional databases. ","date":"2024-07-13","objectID":"/en/vector-search/:2:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3. What is Vector Search Vector search, also known as vector retrieval, is a technique in Information Retrieval used to find the most similar vectors to a given query vector in a high-dimensional vector space. To measure the similarity between two vectors, we usually use cosine similarity, Euclidean distance, Manhattan distance, etc. To speed up vector search, we usually use index structures such as KD-Tree, IVF (Inverted File Index), HNSW (Hierarchical Navigable Small World), etc. Vector search has applications in many fields, such as in recommendation systems, where we can use vector search to find products most similar to a user's historical behavior and then recommend them to the user; in image retrieval, we can use vector search to find images most similar to a given image; in RAG (Retrieval Augmented Generation), we can use vector search to find text most similar to a given question, enhancing the Context of large models to improve the quality of generated answers. ","date":"2024-07-13","objectID":"/en/vector-search/:3:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.1 Vector Search Application Scenarios 3.1.1 Recommendation System As in the On-premise case of Qdrant about Video Content-based Recommendation, the multilingual universal sentence encoder is used to embed the script when uploading a video. Here, it is not simply extracting frames from the video, but more information comes from the video title, description, automatically detected tags, and content recognized by Whisper speech recognition. So the current problem is that if the video has no audio, the title and description are forced to be used for recommendation, which is a big challenge for the review team. Here, the call start issues in the recommendation field are mentioned, that is, when users first start using the system, the recommendation quality of the recommendation system is not high, and the user experience is poor at this time. On the basis of non-real-time updated collaborative recommenders and metadata recommenders, adding a content-based recommender can greatly optimize call start issues. 3.1.2 Image Retrieval immich is a high-performance open-source self-hosted image and video management solution. Imagine when you upload all your videos and images to immich, it is difficult to find the image or video you want in a short time. At this time, an efficient image retrieval system smart search is needed. Through vector search technology, you can quickly and accurately find the image or video you want through text descriptions and additional filters (tags, dates, etc.). Images from immich 3.1.3 RAG RAG (Retrieval Augmented Generation) mainly solves several problems in LLM applications: The data used to train LLM models is not real-time, in other words, it is static data, and the cost of obtaining the latest data and retraining is too high. LLM lacks domain-specific knowledge because the training corpus of LLM is mostly general datasets on the internet. In fields such as finance, healthcare, and law, private data may be the most important, and the lack of domain data will cause LLM to hallucinate. The black box problem of LLM, we cannot know how LLM generates answers, and where the source of the answers comes from. Here, I borrow two diagrams from Paul lusztin and Aurimas Griciunas to explain how RAG works: Obtain streaming real-time data of financial news and historical data. Chunk the data into inputs for the embedding model, and then store the embeddings in the vector database. User asks a question. Find the most similar news chunks through vector search, and then perform Prompt composition with the user's historical chat information and news chunks. Input into the LLM to generate an answer. Return the answer to the user. Store the new chat information in the user's historical data. Private data, such as Notion, Jira, local PDF files, etc., are chunked into inputs for the embedding model. Input the chunks into the embedding model, and then store the embeddings in the vector database. The Vector Database builds an Index. User asks a question, input into the embedding model. The embedding model outputs the query's embedding vector. Use the vector from step 5 as the Query vector and input it into the vector database. The vector database finds the most similar chunks through ANNs (Approximate Nearest Neighbors Search). Construct a Prompt with the searched chunks and the query. Input into the LLM to generate an answer. ","date":"2024-07-13","objectID":"/en/vector-search/:3:1","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.2 Similarity Metrics Cosine similarity is a method used to measure the similarity between two vectors. It is measured by calculating the angle between two vectors. The range of cosine similarity is [-1, 1], where 1 means the angle between two vectors is 0 degrees, indicating that the two vectors are identical; -1 means the angle between two vectors is 180 degrees, indicating that the two vectors are completely opposite; 0 means the angle between two vectors is 90 degrees, indicating that there is no similarity between the two vectors. The calculation formula is as follows: This formula calculates the cosine value of the angle between vectors ùê¥ and ùêµ. Euclidean distance is a method used to measure the similarity between two vectors. It is measured by calculating the distance between two vectors. The range of Euclidean distance is [0, ‚àû], where 0 means the two vectors are identical, and the larger the value, the greater the difference between the two vectors. The calculation formula is as follows: This formula calculates the Euclidean distance between vectors ùê¥ and ùêµ. Some do not take the square root, which only changes the numerical value but does not fundamentally differ. Negative inner product is measured by calculating the inner product between two vectors. The larger the value, the higher the similarity between the two vectors. The calculation formula is as follows: Manhattan distance (taxicab distance) is measured by calculating the distance between two vectors. The range of Manhattan distance is [0, ‚àû], where 0 means the two vectors are identical, and the larger the value, the greater the difference between the two vectors. The calculation formula is as follows: ","date":"2024-07-13","objectID":"/en/vector-search/:3:2","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.3 Vector Search Algorithms Intuitively, we can find the vector most similar to a given query vector by traversing all vectors, but the time complexity of this method is O(n), which is not feasible when the number of vectors is large. To speed up vector search, we usually use index structures such as IVF (Inverted File Index), HNSW (Hierarchical Navigable Small World), etc. Through ANNs (Approximate Nearest Neighbors Search) algorithms, we can find the vector most similar to a given query vector with lower time complexity, such as O(log(n)). 3.3.1 #### LSH (Locality Sensitive Hashing) Locality Sensitive Hashing (LSH) works by processing each vector with a hash function, grouping vectors into buckets, thereby maximizing hash collisions, rather than minimizing collisions as usual hash functions do. Here is a diagram from Pinecone: The specific details of LSH are as follows: Shingling: Use k-shingling and one-hot encoding to convert text into sparse vectors. k-shingling means using a sliding window of size k to extract k consecutive characters from the text. one-hot encoding means comparing the result of k-shingling with the vocabulary, and if it exists, it is represented as 1 in the vocabulary, otherwise 0. Then use MinHash to create a ‚Äúsignature‚Äù. Create a random permutation of [1‚Ä¶len(voc)+1]. Use the value from top to bottom in the random permutation as the index. If the index-1 position of the original sparse vector is 1, take the index-1 position number of the random permutation as the signature value. Repeat n times to get an n-dimensional dense vector. Band and Hash Divide the n-dimensional signature vector into b groups, each with r elements. Hash each group to get b hash values. If two vectors have the same hash value, put them in the same bucket. If in the same bucket, consider them as candidate pairs. Here, as b increases, more candidate pairs are returned, which naturally leads to more false positives. This means that as the dimension increases, the possibility of false positives increases, and more hash buckets need to be maintained, which also increases storage overhead. Therefore, LSH is more suitable for low-dimensional vector search and is not the mainstream vector search algorithm. 3.3.2 IVFÔºàInverted File IndexÔºâ The inverted index algorithm is a simple, easy-to-understand, and very easy-to-implement algorithm, and it has a good search speed, but the search accuracy is worse than HNSW, but the memory consumption is relatively less than HNSW. The core of building an IVF index is divided into two steps: Use a clustering algorithm to divide the vectors into nlist clusters. Assign the vectors to the corresponding clusters. When searching, set the number of cells to search nprobe. The impact of the parameters here is: Increasing nlist will slow down the index building speed because the vectors need to be calculated with more centroids during the clustering process; at the same time, it will reduce the search time because there are fewer vectors corresponding to the centroids, making knn faster. Increasing nprobe will improve the recall rate but will reduce the search speed because more cells need to be searched. 3.3.3 HNSW (Hierarchical Navigable Small World) HNSW combines the advantages of NSW and Skip List and is an efficient vector search algorithm. The core idea of HNSW is to build a multi-layer graph, where each layer is a small world. By searching for the nearest nodes in each layer and then searching for the nearest nodes in the next layer, the vector most similar to the given query vector is finally found. NSW is based on a theory that the distance from any point to any other point on NSW is finite and can be found with a few jumps. The construction process of NSW: Randomly select a point as the insertion point. Find the m nearest points to the insertion point. Connect the insertion point with the m points. The randomness here will increase the number of long connections in the early graph, speeding up the se","date":"2024-07-13","objectID":"/en/vector-search/:3:3","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.4 Vector Search Algorithm Optimization By reducing the size of vectors or reducing dimensions to make searches faster, here are some common vector search algorithm optimization methods. 3.4.1 PQÔºàProduct QuantizationÔºâ Here I borrow a diagram from a Zhihu user, as the user's diagram is very well-drawn: Construction phase: First, split N original vectors into multiple sub-vectors. For example, a 256-dimensional vector is split into 8 32-dimensional sub-vectors. Then perform clustering in each sub-vector space, using clustering algorithms such as KMeans. Assuming there are 1024 clusters in each subspace, encode each cluster center to get 1024 IDs. Encode the original vectors into the nearest cluster center ID, and finally concatenate them. Retrieval phase: Split the retrieval vector. Calculate the distance between each subspace and each cluster center to create a distance table. Use the distance table to calculate the distance between the query and candidate samples in each subspace, accumulate them, and take the top-k. The splitting involved can be done in parallel. PQ is generally not used directly because it still requires a lot of distance calculations. Usually, IVF is first used to find the most promising top-k clusters, and then PQ is performed. 3.4.2 SQÔºàScalar QuantizationÔºâ SQ is relatively simple. Encoding: scalar = (max-min)/255, floor(value-min/scaler). If less than 0, take 0; if greater than 255, take 255. This compresses the vector to between 0-255, reducing the size of the vector but losing some information. Decoding: value = min + (code + 0.5)*(max-min)/255. 3.4.3 RabitQ RabitQ comes from the paper RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search. RabitQ points out two problems with the current PQ algorithm: Using the centroid of kmeans as the codebook is a heuristic approximation during construction, with no theoretical guarantee. Distance estimation, using the distance between the quantized vector and the query vector to estimate the distance between the original vector and the query vector, lacks an approximate error range. How to solve the above problems: Codebook construction phase First, normalize the data vectors to align them on the unit hypersphere in D-dimensional space. Construct a set of $2^{D}$ bivalued vectors with coordinates $‚àí1/\\sqrt{D}$ or $+1/\\sqrt{D}$ (i.e., the set consists of the vertices of a hypercube uniformly distributed on the unit hypersphere). Randomly rotate the bivalued vectors by multiplying each bivalued vector by a random orthogonal matrix (i.e., perform a Johnson-Lindenstrauss transformation). For each vector, take the closest vector in the codebook as the quantized vector. Since each quantized vector is a rotated D-dimensional bivalued vector, we represent its quantization code as a bit string of length D, where 0 and 1 represent two different values. The basic principle of codebook construction is that it has a clear geometric interpretation (i.e., the vectors in the codebook are a set of randomly rotated vectors on the unit hypersphere), allowing explicit analysis of the geometric relationships between data vectors, their quantized vectors, and query vectors. Distance estimation Carefully design an estimator for the distance between data vectors and query vectors based on the above geometric relationships, and prove that this estimator is unbiased and provides an error range. At the same time, when estimating distances, even with shorter quantization codes, about half of the advantages can be estimated with small empirical errors. RaBitQ‚Äôs distance estimatorÔºö Single data vector uses bitwise operations. Batch data uses SIMD acceleration. Using a random codebook avoids the poor performance of bivalued codebooks on specific vectors, such as ($1/\\sqrt{D}$‚Ä¶ $‚àí1/\\sqrt{D}$) and (1, 0, 0, 0). We multiply this codebook by a random orthogonal matrix, allowing the unit vectors of the codebook to have the same probability of","date":"2024-07-13","objectID":"/en/vector-search/:3:4","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4. Common Vector Databases and Their Pros and Cons The following lists some common vector databases and their pros and cons. Some are dedicated vector databases, while others are extensions of existing relational databases. ","date":"2024-07-13","objectID":"/en/vector-search/:4:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.1 Milvus Milvus is an excellent open-source vector database that supports multiple vector search algorithms, including HNSW, DiskANN, IVF, etc. In addition to basic vector retrieval functions, it also provides sharding, streaming data ingestion, and hybrid search. Milvus adopts a cloud-native, shared-everything architecture with separate control and data planes. Each component is independent and horizontally scalable, including: Access Layer: Consists of a set of stateless proxies. It provides endpoints for user connections, verifies client requests, and merges and returns results. It uses load balancing components such as Nginx, Kubernetes Ingress, NodePort, and LVS to provide a unified service address. Since Milvus uses a massively parallel processing (MPP) architecture, the proxy aggregates and post-processes intermediate results and then returns the final results to the client. Coordinator Service: Responsible for assigning tasks to execution nodes, including root coord, data coord, and query coord. root coord: Handles data definition language (DDL) and data control language (DCL) requests, such as creating or deleting collections, partitions, or indexes, and managing TSO (Timestamp Oracle) and time ticker. data coord: Manages data and index node topology, maintains metadata, and triggers background data operations such as flush, compact, and index building. query coord: Manages query node topology, load balancing, and the conversion of growing segments to sealed segments. Worker Node: Executes tasks assigned by the coordinator service and proxy DML commands. Query Node: Retrieves incremental log data, converts it into growing segments by subscribing to the log broker, loads historical data from object storage, and performs hybrid searches between vector and scalar data. Data Node: Obtains incremental log data by subscribing to the log broker, processes mutation requests, and packages log data into log snapshots stored in object storage. Index Node: Builds indexes. Index nodes do not need to reside in memory and can be implemented through a Serverless framework. Storage Layer: Object storage is responsible for storing data, including data files and index files. Meta Storage: Meta storage stores metadata snapshots, such as collection schemas and message consumption checkpoints. Storing metadata requires high availability, strong consistency, and transaction support, so Milvus chooses etcd for meta storage. Milvus also uses etcd for service registration and health checks. Object Storage: Stores log snapshot files, index files for scalar and vector data, and intermediate query results. Milvus uses MinIO as object storage, which can be easily deployed on AWS S3 and Azure Blob. However, object storage has high access latency and charges based on query counts. To improve performance and reduce costs, Milvus plans to implement cold and hot data separation on a memory or SSD-based cache pool. Log Broker: A publish-subscribe system responsible for streaming data persistence and event notification. It also ensures the integrity of incremental data when worker nodes recover from system failures. Milvus cluster uses Pulsar as the log broker; Milvus standalone uses RocksDB as the log broker. Additionally, the log broker can be easily replaced with streaming data storage platforms such as Kafka. Milvus's cloud-native architecture is its advantage, but it also brings significant challenges to developers, such as learning new concepts and the operational management challenges brought by related components like Pulsar or etcd. ","date":"2024-07-13","objectID":"/en/vector-search/:4:1","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.4 Pinecone ","date":"2024-07-13","objectID":"/en/vector-search/:4:2","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.5 Qdrant ","date":"2024-07-13","objectID":"/en/vector-search/:4:3","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.6 Pgvector ","date":"2024-07-13","objectID":"/en/vector-search/:4:4","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.7 Pgvecto.rs ","date":"2024-07-13","objectID":"/en/vector-search/:4:5","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.8 VectorChord ","date":"2024-07-13","objectID":"/en/vector-search/:4:6","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"5. Excellent Vector Search Libraries and Open-Source Vector Database Projects ","date":"2024-07-13","objectID":"/en/vector-search/:5:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"6. What You Need to Know About Vector Database Commercialization ","date":"2024-07-13","objectID":"/en/vector-search/:6:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"7. Summary Here, I have briefly introduced some basic knowledge of vector search, as well as some common vector search algorithms, vector search application scenarios, vector search algorithm optimization, common vector databases and their pros and cons, excellent vector search libraries, and open-source vector database projects. I hope to apply this knowledge to actual scenarios in the future. I hope this article can help you better understand vector search. ","date":"2024-07-13","objectID":"/en/vector-search/:7:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"8. References Thank you very much to Pinecone's articles, which gave me a deeper understanding of vector databases. https://www.pinecone.io/learn/series/faiss/vector-indexes/ https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/ https://zhuanlan.zhihu.com/p/379372268 https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/ https://www.xiemingzhao.com/posts/hnswAlgo.html https://whenever5225.github.io/2020/05/11/hnsw-heuristic/ Search Engine For AIÔºöÈ´òÁª¥Êï∞ÊçÆÊ£ÄÁ¥¢Â∑•‰∏öÁ∫ßËß£ÂÜ≥ÊñπÊ°à https://zhuanlan.zhihu.com/p/50143204 https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw ","date":"2024-07-13","objectID":"/en/vector-search/:8:0","tags":["vector search","Postgres"],"title":"The Ins and Outs of Vector Databases","uri":"/en/vector-search/"},{"categories":["2022","kubernetes","documentation"],"content":"Recently, I've been working on some NUMA-aware scheduling tasks on an internally developed platform, involving the discovery of Kubernetes node resource topology and scheduling. However, due to my limited knowledge, I often find myself struggling to grasp the full picture. This article is an attempt to summarize and organize my understanding. ","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:0:0","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"Why Topology Awareness is Needed According to the official Kubernetes documentation, more and more systems are utilizing CPUs and hardware accelerators like GPUs and DPUs to support low-latency tasks and high-throughput parallel computing tasks. However, the explanation seems a bit unclear. The fundamental reason lies in the issues brought by the Von Neumann architecture. As the saying goes, there is no silver bullet. The Von Neumann architecture separates memory and processors, with both instructions and data stored in memory, laying the foundation for the universality of modern computing. However, it also poses a hidden risk: as memory capacity increases exponentially, data transfer between the CPU and memory becomes a bottleneck. Currently, most devices in servers are connected via high-speed PCIe buses, and the bus layout may vary depending on the server's purpose. As shown in the figure below (found online, not drawn by me), in the left diagram, GPUs reside in different PCIe domains, making direct P2P copying between GPU memories impossible. To copy memory from GPU 0 to GPU 2, it must first be copied via PCIe to the memory connected to CPU 0, then transferred to CPU 1 via QPI link, and finally transferred to GPU 2 via PCIe again. This process adds significant overhead in terms of latency and bandwidth, whereas the right diagram can achieve ultra-high-speed communication through GPU P2P connections. In summary, topology affects communication between devices, impacting business stability and efficiency, necessitating some technical means to make businesses topology-aware. PCIe Topo (figure 1) ","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:1:0","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"Types of Topology Currently, the types of topology that need to be aware of include: GPU Topology Awareness NUMA Topology Awareness ","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:2:0","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"GPU Topology Manager There are several implementation solutions in the industry: Volcano GPU Topology Awareness Baidu Intelligent Cloud GPU Topology-Aware Scheduling Volcano is not fully implemented yet, and Baidu's solution is closed-source, so we can only get a glimpse through shared information. Why Why is GPU topology awareness needed? Let's start with a diagram from NVIDIA, which describes the topology of the mainstream V100 GPU graphics card in servers. GPU Topo (figure 2) Each V100 GPU has 6 NVLink channels, and 8 GPUs cannot achieve full connectivity, with a maximum of 2 NVLink connections between 2 GPUs. For instance, there are 2 NVLink connections between GPU0 and GPU3, and between GPU0 and GPU4, while there is only one NVLink connection between GPU0 and GPU1, and no NVLink connection between GPU0 and GPU6. Therefore, communication between GPU0 and GPU6 still requires PCIe. The unidirectional communication bandwidth of NVLink is 25 GB/s, and the bidirectional bandwidth is 50 GB/s, while the communication bandwidth of PCIe is 16 GB/s. Thus, if GPUs are allocated incorrectly during GPU training, such as a training task Pod requesting two cards, GPU0 and GPU6, cross-GPU communication may become a bottleneck for the training task. Topology information can be viewed by executing the following command on a node: # nvidia-smi topo -m GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 GPU0 X PIX PHB PHB SYS SYS SYS SYS GPU1 PIX X PHB PHB SYS SYS SYS SYS GPU2 PHB PHB X PIX SYS SYS SYS SYS GPU3 PHB PHB PIX X SYS SYS SYS SYS GPU4 SYS SYS SYS SYS X PIX PHB PHB GPU5 SYS SYS SYS SYS PIX X PHB PHB GPU6 SYS SYS SYS SYS PHB PHB X PIX GPU7 SYS SYS SYS SYS PHB PHB PIX X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge) PIX = Connection traversing a single PCIe switch NV# = Connection traversing a bonded set of # NVLinks How I won't parse it all here, as there isn't a complete implementation to look at. I'll outline some general ideas based on my understanding. The first step is awareness, which involves using a daemon component to gather information on NVIDIA GPUs, network topology, NVLink, and PCIe. The second step is the scheduler, which defines strategies. Strategy 1: Preferably schedule GPUs with the most NVLinks under the same NUMA node to a Pod; Strategy 2: Preferably allocate GPUs and network cards under the same PCI switch to the same Pod. The general process is as follows: GPU-device-plugin or other daemon processes construct the GPU topology information CRD for the node; The Pod defines a topology strategy, such as Strategy 1 or Strategy 2; The newly defined scheduler filters nodes that do not meet the strategy during the filter and priority phases and scores nodes that meet the strategy highly; The discovery and update of GPU devices on the node are handled by the device-plugin and kubelet, as referenced in this article. Currently, GPU topology information can be queried through the official nvml (NVIDIA Management Library) interface. ","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:2:1","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"NUMA Topology Awareness Why When discussing NUMA topology awareness, it's essential to first explain what NUMA is and why it needs to be aware of it. NUMA Topo (figure 3) CPU Cache Latency (figure 4) The two figures above provide the answer. Modern CPUs often use the NUMA architecture, which stands for ‚ÄúNon-Uniform Memory Access.‚Äù Why use non-uniformity? Isn't uniformity better? The answer is no, because if UMA (Uniform Memory Access) is used, as the number of physical cores on the northbridge increases and CPU frequency rises, the bus bandwidth cannot keep up, and conflicts over accessing the same memory will become more severe. Returning to the NUMA architecture, each NUMA node has its own physical CPU cores, and the cores within each NUMA node also share the L3 Cache. Additionally, memory is distributed across each NUMA node. Some CPUs with hyper-threading enabled will present two logical cores for each physical CPU core in the operating system. From a business perspective, if programs run on the same NUMA node, they can better share some L3 Cache, which has a very fast access speed. If the L3 Cache is not hit, data can be read from memory, significantly reducing access speed. In today's container-dominated world, the issue of incorrect CPU allocation is particularly severe. Because nodes are now oversold, with many containers running simultaneously, what happens if the same process is allocated to different NUMA nodes: CPU contention leads to frequent context switching time; Frequent process switching causes CPU cache failures; Cross-NUMA memory access results in more severe performance bottlenecks. In summary, in modern CPU architectures, if NUMA topology relationships are not considered, incorrect CPU allocation can lead to performance issues, affecting business SLAs. How The previous section explained why NUMA-aware scheduling is needed. So how can NUMA topology be sensed, and what existing solutions are available? Here, I briefly list some projects in the Kubernetes ecosystem. If you have any additions, feel free to comment: Kubernetes Topology Manager Official Crane NUMA Topology Awareness Koordinator Fine-grained CPU Orchestration Kubernetes Topology Manager The Topology Manager is a kubelet component designed to coordinate a set of components responsible for these optimizations. The Topology Manager addresses a historical issue where the CPU Manager and Device Manager worked independently and were unaware of each other. Let's first look at the implementation of the Kubernetes Topology Manager. I don't want to reinvent the wheel here, so you can refer to a great article summarized by a colleague from Alibaba. Here's a summary: Find the topology hints for different resources, i.e., topology information. The CPU selection criteria prioritize the smallest number of NUMA nodes involved, with the smallest number of sockets involved as a secondary priority. The device manager prioritizes the smallest number of NUMA nodes involved while meeting resource requests. Merge hints from different topology types, i.e., union, to select the optimal strategy. If a selection is made, that's good. If not, what happens? Kubernetes provides kubelet configuration strategies: best-effort: The Kubernetes node will accept the Pod, but the effect may not meet expectations. restricted: The node will refuse to accept the Pod, and if the Pod is rejected, its status will become Terminated. single-NUMA-node: The node will refuse to accept the Pod, and if the Pod is rejected, its status will become Terminated. This is more restrictive than restricted, as the selected NUMA node count must be 1. Therefore, we see that the Kubernetes Topology Manager is still centered around NUMA, performing complete fair shortest path selection for different resources (NIC, GPU, CPU). Moreover, this process occurs after the Pod is scheduled to a specific node, which brings several issues: There is a high probability that the Pod will be Terminated, making it unusable","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:2:2","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"References Standing on the shoulders of giants, thanks again. https://kubernetes.io/ https://github.com/volcano-sh/volcano/ https://mp.weixin.qq.com/s/uje27_MHBh8fMzWATusVwQ https://www.infoq.cn/article/tdfgiikxh9bcgknywl6s https://github.com/NVIDIA/go-nvml https://gocrane.io/docs/ https://koordinator.sh/docs/user-manuals https://www.likakuli.com/posts/kubernetes-kubelet-restart/ https://zhuanlan.zhihu.com/p/121588317 ","date":"2022-12-29","objectID":"/en/kubernetes-topo-aware-all-you-need-know/:3:0","tags":["kubernetes","topo aware"],"title":"All You Need to Know About Topology Awareness in Kubernetes","uri":"/en/kubernetes-topo-aware-all-you-need-know/"},{"categories":null,"content":"About Far East","date":"2022-12-29","objectID":"/en/about/","tags":null,"title":"About xieydd","uri":"/en/about/"},{"categories":null,"content":"üì´ If you wish to contact me, you can send an email to xieydd@gmail.com, or add my WeChat echo -n 'eGlleWRkX2hhaGEK' | base64 -d. üíª As of 2024, I have over 6 years of experience in AI Infrastructure: ","date":"2022-12-29","objectID":"/en/about/:0:0","tags":null,"title":"About xieydd","uri":"/en/about/"},{"categories":null,"content":"2018-2021.2 (including internship) Unisound At the AI algorithm company Unisound, I was responsible for the development and operation of the Atlas supercomputing platform, supporting NLP and CV model training. Key responsibilities included: Developing a large-scale intelligent scheduling system to optimize multi-tenant resource allocation Enhancing the performance of the high-performance distributed file system Lustre Building a multi-layer cache cloud-native architecture to accelerate AI model training Worked on 8 Bit training and inference optimization at Unisound, optimizing models for NPU and NVIDIA Edge Devices. ","date":"2022-12-29","objectID":"/en/about/:0:1","tags":null,"title":"About xieydd","uri":"/en/about/"},{"categories":null,"content":"2021.2-2023.5 Tencent Cloud Developed a large-scale AI platform for public cloud: Built a high-performance, scalable elastic offline training platform using EKS (Elastic Kubernetes Service). Integrated public cloud object storage and the GooseFS accelerator to create a high-performance cache scheduling system on the cloud Established FinOps infrastructure to help public cloud customers manage and optimize cloud costs more effectively, enhancing cloud resource utilization: Optimized scheduling and rescheduling, identified high and low priority tasks, and implemented intelligent elastic scaling. Combined Tencent's Ruyi kernel scheduler optimization and observability to optimize costs while maintaining service quality Launched a large-scale cost reduction initiative in the internal cloud, improving resource utilization through efficient resource allocation ","date":"2022-12-29","objectID":"/en/about/:0:2","tags":null,"title":"About xieydd","uri":"/en/about/"},{"categories":null,"content":"2023.5-present Tensorchord Leading the development of the Serverless Inference platform ModelZ on GCP, providing optimized cold start model service inference: Reduced model service cold start time through cache model services and image preheating Implemented JuiceFS to build a high-performance cache scheduling system, enhancing model service performance Leading the Cloud Team, developing the vector database VectorChord's cloud service and customer support VectorChord Cloud: Built a vector database based on Postgres on AWS, achieving control and data plane separation, BYOC (Bring Your Own Cloud), BYOD (Bring Your Own Data) capabilities Implemented cloud-native architecture to achieve Postgres storage and compute separation, high availability, Backup, PITR (Point-In-Time Recovery), In-Place Upgrade features Skill set: Kubernetes, GCP, AWS, Kubeflow, FinOps, RAG, Vector Database, Storage Acceleration, Tensorflow, Pytorch, Cloud Native, MLOps, AI Infrastructure, etc. üå± Currently focusing on MLOps and FinOps, contributing to several open source projects: fluid Fluid, elastic data abstraction and acceleration for BigData/AI applications in the cloud. (Project under CNCF) crane Crane is a FinOps Platform for Cloud Resource Analytics and Economics in Kubernetes clusters. The goal is to help users manage cloud costs more easily while ensuring application quality. crane-scheduler Crane scheduler is a Kubernetes scheduler that can schedule pods based on actual node load. creator Creator is the brain of the crane project, containing the core algorithm module and evaluation module. openmodelz One-click machine learning deployment (LLM, text-to-image, etc.) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine). clusternet [CNCF Sandbox Project] Managing your Kubernetes clusters (including public, private, edge, etc.) as easily as browsing the Internet vectorchord Scalable, fast, and disk-friendly vector search in Postgres, the successor of pgvecto.rs. ","date":"2022-12-29","objectID":"/en/about/:0:3","tags":null,"title":"About xieydd","uri":"/en/about/"}]