<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>向量数据库中的门门道道 - Xieydd&#39;s Blog</title><meta name="Description" content="远东的小猪窝"><meta property="og:title" content="向量数据库中的门门道道" />
<meta property="og:description" content="加入 Tensorchord 已经一年有余，一直也没有时间静下心来写一些文章。主要是有了彤彤女儿后，事情多了很多。中间也经历过业务从 Serverless 模型推理 Modelz pivot 到向量搜索领域 VectorChord 的" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.xieydd.top/vector-search/" />
<meta property="og:image" content="https://blog.xieydd.top/logo.png"/>
<meta property="article:published_time" content="2024-07-13T14:52:44+08:00" />
<meta property="article:modified_time" content="2025-09-08T12:10:15+08:00" /><meta property="og:site_name" content="小猪窝" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.xieydd.top/logo.png"/>

<meta name="twitter:title" content="向量数据库中的门门道道"/>
<meta name="twitter:description" content="加入 Tensorchord 已经一年有余，一直也没有时间静下心来写一些文章。主要是有了彤彤女儿后，事情多了很多。中间也经历过业务从 Serverless 模型推理 Modelz pivot 到向量搜索领域 VectorChord 的"/>
<meta name="application-name" content="Xieydd">
<meta name="apple-mobile-web-app-title" content="Xieydd"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://blog.xieydd.top/vector-search/" /><link rel="next" href="https://blog.xieydd.top/kubernetes-topo-aware-all-you-need-know/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><meta name="msvalidate.01" content="166AB4EE5B2A22286AACF2A9E0CF14E8" /><meta name="baidu-site-verification" content="codeva-TuWhmJgIa4" /><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "向量数据库中的门门道道",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/blog.xieydd.top\/vector-search\/"
        },"image": ["https:\/\/blog.xieydd.top\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "vector search, Postgres","wordcount":  17267 ,
        "url": "https:\/\/blog.xieydd.top\/vector-search\/","datePublished": "2024-07-13T14:52:44+08:00","dateModified": "2025-09-08T12:10:15+08:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "xxxx","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/blog.xieydd.top\/images\/xieydd_github.jpeg",
                    "width":  400 ,
                    "height":  400 
                }},"author": {
                "@type": "Person",
                "name": "xieydd"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Xieydd&#39;s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4"
        data-srcset="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4, https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4 1.5x, https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4 2x"
        data-sizes="auto"
        alt="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4"
        title="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4" /><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw' aria-hidden='true'></i></span>远东的小猪窝</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/categories/documentation/"> 文档 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/xieydd/xieydd.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a><a href="javascript:void(0);" class="menu-item language" title="选择语言">
                    <i class="fa fa-globe" aria-hidden="true"></i>                      
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/en/vector-search/">English</option><option value="/vector-search/" selected>简体中文</option></select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Xieydd&#39;s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4"
        data-srcset="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4, https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4 1.5x, https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4 2x"
        data-sizes="auto"
        alt="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4"
        title="https://avatars.githubusercontent.com/u/20329697?u=cf81f45b2077d7041876bc26382e8f59ee185aae&amp;v=4" /><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw' aria-hidden='true'></i></span>远东的小猪窝</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/categories/documentation/" title="">文档</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/xieydd/xieydd.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/en/vector-search/">English</option><option value="/vector-search/" selected>简体中文</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">向量数据库中的门门道道</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://github.com/xieydd" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>xieydd</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/2024/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>2024</a>&nbsp;<a href="/categories/vector-search/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>vector search</a>&nbsp;<a href="/categories/postgres/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>Postgres</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-07-13">2024-07-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 17267 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 35 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-什么是向量">1. 什么是向量</a></li>
    <li><a href="#2-什么是向量数据库">2. 什么是向量数据库</a></li>
    <li><a href="#3-什么是向量搜索">3. 什么是向量搜索</a>
      <ul>
        <li><a href="#31-向量搜索应用场景">3.1 向量搜索应用场景</a>
          <ul>
            <li><a href="#311-推荐系统">3.1.1 推荐系统</a></li>
            <li><a href="#312-图像检索">3.1.2 图像检索</a></li>
            <li><a href="#313-rag">3.1.3 RAG</a></li>
          </ul>
        </li>
        <li><a href="#32-相似度指标">3.2 相似度指标</a>
          <ul>
            <li><a href="#321-余弦相似度">3.2.1 余弦相似度</a></li>
            <li><a href="#322-欧氏距离">3.2.2 欧氏距离</a></li>
            <li><a href="#323-负内积">3.2.3 负内积</a></li>
            <li><a href="#324-曼哈顿距离">3.2.4 曼哈顿距离</a></li>
          </ul>
        </li>
        <li><a href="#33-向量搜索算法">3.3 向量搜索算法</a>
          <ul>
            <li><a href="#331-lsh-locality-sensitive-hashing">3.3.1 LSH (Locality Sensitive Hashing)</a></li>
            <li><a href="#332-ivfinverted-file-index">3.3.2 IVF（Inverted File Index）</a></li>
            <li><a href="#333-hnsw-hierarchical-navigable-small-world">3.3.3 HNSW (Hierarchical Navigable Small World)</a></li>
            <li><a href="#nsg-navigating-spreading-out-graph">NSG (Navigating Spreading-out Graph)</a></li>
            <li><a href="#334-diskann">3.3.4 DiskANN</a></li>
            <li><a href="#335-summary">3.3.5 Summary</a></li>
          </ul>
        </li>
        <li><a href="#34-向量搜索算法优化">3.4 向量搜索算法优化</a>
          <ul>
            <li><a href="#341-pqproduct-quantization">3.4.1 PQ（Product Quantization）</a></li>
            <li><a href="#342-sqscalar-quantization">3.4.2 SQ（Scalar Quantization）</a></li>
            <li><a href="#343-rabitq">3.4.3 RabitQ</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#4-常见的向量数据库极其优劣">4. 常见的向量数据库极其优劣</a>
      <ul>
        <li><a href="#41-milvushttpsmilvusio">4.1 Milvus</a></li>
        <li><a href="#44-pineconehttpswwwpineconeio">4.4 Pinecone</a></li>
        <li><a href="#45-qdranthttpsqdranttech">4.5 Qdrant</a></li>
        <li><a href="#46-pgvectorhttpsgithubcompgvectorpgvector">4.6 Pgvector</a></li>
        <li><a href="#47-pgvectorshttpsgithubcomtensorchordpgvectors">4.7 Pgvecto.rs</a></li>
        <li><a href="#48-vectorchordhttpsgithubcomtensorchordvectorchord">4.8 VectorChord</a></li>
      </ul>
    </li>
    <li><a href="#5-优秀的向量搜索库以及向量数据库开源项目">5. 优秀的向量搜索库以及向量数据库开源项目</a></li>
    <li><a href="#6-向量数据库商业化你需要知道什么">6. 向量数据库商业化你需要知道什么</a></li>
    <li><a href="#7-总结">7. 总结</a></li>
    <li><a href="#8-引用">8. 引用</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>加入 <a href="https://tensorchord.ai" target="_blank" rel="noopener noreffer ">Tensorchord</a> 已经一年有余，一直也没有时间静下心来写一些文章。主要是有了彤彤女儿后，事情多了很多。中间也经历过业务从 Serverless 模型推理 <a href="https://modelz.ai/" target="_blank" rel="noopener noreffer ">Modelz</a> pivot 到向量搜索领域 <a href="https://vectorchord.ai/" target="_blank" rel="noopener noreffer ">VectorChord</a> 的过程。Pivot 的经历或许可以在之后的文章中和大家分享，感兴趣的也可以直接<a href="../../about/index.zh-cn.md" rel="">联系</a>我。最近半年一直在开发 <a href="https://cloud.vectorchord.ai/" target="_blank" rel="noopener noreffer ">VectorChord Cloud</a>, 所以在这里边学边总结向量数据库中的门门道道。</p>
<h2 id="1-什么是向量">1. 什么是向量</h2>
<p>向量在物理，数学，以及计算机科学等领域的含义都有所不同。这里的向量主要指的是计算机科学中的向量，也就是一组有序的数值。在计算机科学中，向量通常用来表示数据，比如在机器学习中，我们通常会将一张图片转换成一个向量，或者将一段文字 tokenizer 之后转换成一个向量，然后再进行训练。在向量数据库中，我们通常会将一张图片，一段文本，或者一段音频通过 embedding 模型转换成一个向量，然后再进行存储和检索。下面是一个简单的例子，我们通过 <code>all-MiniLM-L6-v2</code> 模型将一段文本转换成一个向量。<code>all-MiniLM-L6-v2</code> 将句子和段落映射到 384 维 dense vector，并可用于聚类或语义搜索等任务。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># 初始化模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="sa"></span><span class="s1">&#39;</span><span class="s1">all-MiniLM-L6-v2</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># 要嵌入的文本示例</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sa"></span><span class="s2">&#34;</span><span class="s2">Hugging Face is creating a tool that democratizes AI.</span><span class="s2">&#34;</span><span class="p">,</span>
    <span class="sa"></span><span class="s2">&#34;</span><span class="s2">I love natural language processing.</span><span class="s2">&#34;</span><span class="p">,</span>
    <span class="sa"></span><span class="s2">&#34;</span><span class="s2">Transformers are state-of-the-art models for NLP tasks.</span><span class="s2">&#34;</span>
<span class="p">]</span>

<span class="c1"># 生成嵌入</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

<span class="c1"># 打印嵌入</span>
<span class="k">for</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">Sentence: {sentence}</span><span class="s2">&#34;</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="sa"></span><span class="s2">&#34;</span><span class="s2">Embedding: {embedding}</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</code></pre></td></tr></table>
</div>
</div><p>总结一下,向量其实是真实世界的实体和计算机世界的桥梁, 计算机通过向量来理解和处理真实世界的数据。</p>
<h2 id="2-什么是向量数据库">2. 什么是向量数据库</h2>
<p>世界本没有向量数据库，只是向量多了，就成了向量数据库，开个玩笑hh。这里我给个简单的定义：能够索引并存储 vector，以实现快速检索和相似性搜索功能的数据库。网络上很多人将向量数据库定义为专注于处理向量数据的数据库，这个定义是不准确的。准确的说向量与向量搜索是一种新的数据类型和查询处理方法，和传统数据库的类似和索引方法并无本质区别。</p>
<h2 id="3-什么是向量搜索">3. 什么是向量搜索</h2>
<p>向量搜索也叫向量检索，是一种 Information Retrieval 的技术，用于在高维向量空间中查找与给定查询向量最相似的向量。为了衡量两个向量之间的相似性，我们通常会使用余弦相似度，欧氏距离，曼哈顿距离等。为了加速向量搜索，我们通常会使用索引结构，比如 KD-Tree，IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。向量搜索在很多领域都有应用，比如在推荐系统中，我们可以使用向量搜索来查找与用户历史行为最相似的商品，然后推荐给用户；在图像检索中，我们可以使用向量搜索来查找与给定图片最相似的图片；在 RAG（Retrieval Augmented Generation）中，我们可以使用向量搜索来查找与给定问题最相似的文本，增强大模型的 Context 从而提高生成答案的质量。</p>
<h3 id="31-向量搜索应用场景">3.1 向量搜索应用场景</h3>
<h4 id="311-推荐系统">3.1.1 推荐系统</h4>
<p>如 Qdrant 关于 <a href="https://qdrant.tech/blog/vector-search-vector-recommendation/" target="_blank" rel="noopener noreffer ">Video Content-based Recommendation</a> 的 On-premise 案例，通过 multilingual universal sentence encoder 来对上传视频时候的脚本进行嵌入。这里不是简单的对视频进行抽帧，更多的信息来自于上传时候的视频标题，描述，自动检测标签以及通过 whisper 语音识别的内容。所以目前遇到的问题是如果视频是无音频，被迫使用标题以及描述进行推荐，这样对于审核团队来说是一个很大的挑战。这里提到了推荐领域的 call start issues, 也就是用户在刚开始使用的时候，推荐系统的推荐质量不高，这个时候用户体验会很差。在非即时更新的协作推荐器以及元数据推荐器的基础上，增加基于内容的推荐器，可以大大优化 call start issues。</p>
<h4 id="312-图像检索">3.1.2 图像检索</h4>
<p><a href="https://github.com/immich-app/immich" target="_blank" rel="noopener noreffer ">immich</a> 是一个高性能的开源 self-hosted 图像以及视频管理解决方案。试想当你把你所有的视频和图片都上传到 immich 之后，你很难在很短的时间内找到你想要的图片或者视频。这个时候就需要一个高效的图像检索系统 <a href="https://immich.app/docs/features/smart-search" target="_blank" rel="noopener noreffer ">smart search</a>，通过向量搜索技术，你可以通过文本描述以及额外的过滤器（标签，日期等）来快速精准的找到你想要的图片或者视频。</p>
<div align="center">
  <img src="https://immich.app/assets/images/advanced-search-filters-1d755f0d492deb304a7bb924e5f4f046.webp" alt="image tag filter search" />
</div>
<div align="center">
  <img src="https://immich.app/assets/images/search-ex-1-3d2c72e3921cb56afcfdbf25369107fb.png" alt="image search" />
</div>
<blockquote>
<p>图片来自于 <a href="https://immich.app/docs/features/smart-search" target="_blank" rel="noopener noreffer ">immich</a></p>
</blockquote>
<h4 id="313-rag">3.1.3 RAG</h4>
<p>RAG（Retrieval Augmented Generation）主要解决在 LLM 应用中的几个问题：</p>
<ol>
<li>LLM 训练模型的数据不是即时的，换句话说是静态的数据，获取最新数据重新进行训练的成本太大。</li>
<li>LLM 缺乏特定领域的知识，因为 LLM 的训练语料大都是网络上通用的数据集。而在比如金融，医疗，法律等领域，私域中的数据或许是最重要的，缺乏领域内数据会让 LLM 出现幻觉问题。</li>
<li>LLM 的黑匣子问题，我们无法知道 LLM 是如何生成答案的，其答案的来源来自何处。</li>
</ol>
<p>这里借用 Paul lusztin 和 Aurimas Griciunas 的两张图来解释 RAG 的工作原理：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="./rag-1.png"
        data-srcset="./rag-1.png, ./rag-1.png 1.5x, ./rag-1.png 2x"
        data-sizes="auto"
        alt="./rag-1.png"
        title="alt text" /></p>
<ol>
<li>获取金融新闻的流式即时数据，以及历史数据</li>
<li>将数据进行 chunking 转换成 embedding 模型的输入，然后将 embedding 存储到向量数据库中</li>
<li>用户提问</li>
<li>通过向量搜索找到最相似的新闻 chunks，然后将用户历史的 chat 信息和新闻 chunks 进行 Prompt composition</li>
<li>输入到 LLM 中生成答案。</li>
<li>将答案返回给用户</li>
<li>将新的 chat 信息存储到用户历史数据中</li>
</ol>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="./rag-2.png"
        data-srcset="./rag-2.png, ./rag-2.png 1.5x, ./rag-2.png 2x"
        data-sizes="auto"
        alt="./rag-2.png"
        title="alt text" /></p>
<ol>
<li>私域中的数据，例如 Notion, Jira,本地 pdf 文件等等进场 chunking 转换成 embedding 模型的输入</li>
<li>将 chunk 输入到 embedding 模型中，然后将 embedding 存储到向量数据库中</li>
<li>Vector Database 构建 Index</li>
<li>用户提问, 输入到 embedding 模型</li>
<li>embedding 输出 query 的 embedding vector</li>
<li>将 5 中的 vector 作为 Query vector 输入到向量数据库中</li>
<li>向量数据库通过 ANNs（Approximate Nearest Neighbors Search）找到最相似的 chunks</li>
<li>将搜索到的 chunks 和 query 构建 Prompt</li>
<li>输入到 LLM 中生成答案</li>
</ol>
<h3 id="32-相似度指标">3.2 相似度指标</h3>
<h4 id="321-余弦相似度">3.2.1 余弦相似度</h4>
<p>余弦相似度是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的夹角来衡量的。余弦相似度的取值范围是[-1, 1]，其中1表示两个向量之间的夹角为0度，表示两个向量完全相同；-1表示两个向量之间的夹角为180度，表示两个向量完全相反；0表示两个向量之间的夹角为90度，表示两个向量之间没有相似性。计算公式如下：</p>
<div align="center">
  <img src="cosine_distance.png" alt="cos" />
</div>
<p>这个公式计算了向量 𝐴 和 𝐵 之间的夹角余弦值。</p>
<h4 id="322-欧氏距离">3.2.2 欧氏距离</h4>
<p>欧氏距离是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的距离来衡量的。欧氏距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下：</p>
<div align="center">
  <img src="euclidean_distance.png" alt="l2" />
</div>
<p>这个公式计算了向量 𝐴 和 𝐵 之间的欧氏距离, 有些直接不开根号其只是数值不同，并无本质区别。</p>
<h4 id="323-负内积">3.2.3 负内积</h4>
<p>负内积（Negative inner product），它是通过计算两个向量之间的内积来衡量的。数值越大则表示两个向量之间的相似性越高。计算公式如下：</p>
<div align="center">
  <img src="negative_inner_product.png" alt="negative_inner_product" />
</div>
<h4 id="324-曼哈顿距离">3.2.4 曼哈顿距离</h4>
<p>曼哈顿距离（taxicab distance），它是通过计算两个向量之间的距离来衡量的。曼哈顿距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下：</p>
<div align="center">
  <img src="taxicab_distance.png" alt="taxicab_distance" />
</div>
<h3 id="33-向量搜索算法">3.3 向量搜索算法</h3>
<p>直觉上，我们可以通过遍历所有的向量来找到与给定查询向量最相似的向量，但是这种方法的时间复杂度是 O(n)。当向量的数量很大时，这种方法是不可行的，对于你的应用延迟不可接受。为了加速向量搜索，我们通常会使用索引结构，比如 IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。通过 ANNs (Approximate Nearest Neighbors Search) 算法，我们可以在更低的时间复杂度，比如 O(log(n))，找到与给定查询向量最相似的向量。</p>
<h4 id="331-lsh-locality-sensitive-hashing">3.3.1 LSH (Locality Sensitive Hashing)</h4>
<p>局部敏感哈希 (LSH) 的工作原理是通过哈希函数处理每个向量，将向量分组到存储桶中，从而最大化哈希冲突，而不是像通常的哈希函数那样最小化冲突。</p>
<p>这里引用 Pinecone 的一张图：</p>
<p><img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4c72c87199038e6875710893c81eb0291019756e-700x393.png&w=750&q=75" alt="lsh" /></p>
<p>LSH 的具体细节如下图所示：</p>
<p><img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F89413953597fbfdd36c4fa77ca0eeafaf6cf944a-1280x980.png&w=1920&q=75" alt="lsh_details" /></p>
<ol>
<li>Shingling：使用 k-shingling 以及 one-hot encoding 将文本转换成稀疏向量
<ul>
<li>k-shingling 的意思是以窗口大小为 k 的滑动窗口，在文本中提取 k 个连续的字符</li>
<li>one-shot encoding 的意思是，将 k-shingling 的结果和词汇表进行比较，如果存在则在词汇表表示为1，不存在则为0</li>
</ul>
</li>
</ol>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75"
        data-srcset="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75, https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75 1.5x, https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75 2x"
        data-sizes="auto"
        alt="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75"
        title="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc259242606006f5c5505a6e677f3d05be75a26da-1280x720.png&amp;w=1920&amp;q=75" /></p>
<ol start="2">
<li>后使用 MinHash 创建“签名”
<ul>
<li>创建 [1&hellip;len(voc)+1] 的随机排列</li>
<li>随机排列中从上到下的值作为 index ，如果原始稀疏 vector 的 index-1 位置为1则取随机排列的 index-1 位置数为签名值</li>
<li>重复 n 次得到 n 维度稠密向量
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75"
        data-srcset="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75, https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75 1.5x, https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75 2x"
        data-sizes="auto"
        alt="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75"
        title="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F866cea917043cfd7eb8221fc1a3b715a61e9d14f-1280x720.png&amp;w=1920&amp;q=75" /></li>
</ul>
</li>
<li>Band and Hash
<ul>
<li>将 n 维度的签名向量分成 b 组，每组 r 个</li>
<li>对每组进行 hash，得到 b 个 hash 值</li>
<li>如果两个向量的 hash 值相同，则将这两个向量放到同一个桶中</li>
<li>如果在同一个桶中，则认为其为候选对</li>
</ul>
</li>
</ol>
<div align="center">
  <img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F00a27d5963a54c82b9f751845218b6beb8c09324-1280x720.png&w=1920&q=75" alt="lsh5" />
</div>
<p>这里随着 b 的增大返回更多的候选对，这自然会导致更多的误报</p>
<div align="center">
  <img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fd6b9466efa2e6875ff98f4cce94ae1737e36c53b-1280x720.png&w=1920&q=75" alt="lsh6" />
</div>
<p>这意味着随着纬度的增加，误报的可能性越大，而且维数增大后需要维护更多的 hash 桶，存储的开销也会增大。所以 LSH 更适合低维度的向量搜索，不是目前的主流向量搜索算法。</p>
<h4 id="332-ivfinverted-file-index">3.3.2 IVF（Inverted File Index）</h4>
<p>倒排索引算法是一个简单、易懂而且非常容易实现的算法，而且有着不错的搜索速度，但是搜索的精度较 HNSW 较差些，但是内存消耗相对 HNSW 更少。</p>
<p>构建 IVF 索引的核心分为两个步骤：</p>
<ol>
<li>通过聚类算法将向量分成 nlist 个簇</li>
<li>将向量分配到对应的簇中</li>
</ol>
<p>搜索时候，设定需要搜索的聚类个数 nprobe</p>
<div align="center">
  <img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe68c5241fd2726395449721f5414bc21b038f615-2020x1270.png&w=2048&q=75" alt="ivf" />
</div>
<p>这里参数的影响是：</p>
<ul>
<li>增大 nlist 会降低构建索引的速度，因为在聚类过程中向量需要跟更多的中心点进行计算；同时会降低搜索时间，因为对应中心点的向量更少了，做 knn 的时候更快。</li>
<li>增大 nprobe 会提高召回率但是会降低搜索速度，因为需要搜索更多的单元格。</li>
</ul>
<div align="center">
  <img src="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F8432b8975f19db1a0bc3fb4ced7f62ab7ab783f4-700x568.png&w=750&q=75" alt="ivf2" />
</div>
<h4 id="333-hnsw-hierarchical-navigable-small-world">3.3.3 HNSW (Hierarchical Navigable Small World)</h4>
<p>HNSW 结合了 NSW 以及 Skip List 的优点，是一种高效的向量搜索算法。HNSW 的核心思想是通过构建一个多层的图，每一层都是一个小世界，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。</p>
<p>NSW 是建立在一个理论的基础上，NSW 上的点到任意点的距离都是有限的，而且是通过很少的几次跳跃就能找到的。</p>
<p>NSW 的构造过程：</p>
<ol>
<li>随机选择一个点作为插入点</li>
<li>查找与插入点最近的 m 个点</li>
<li>将插入点与 m 个点相连</li>
</ol>
<p>这里的随机性会让前期的图中长连接线增多，加速搜索，可以理解成“高速公路”，下图中的红色线就是长连接线：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="./nsw_graph.png"
        data-srcset="./nsw_graph.png, ./nsw_graph.png 1.5x, ./nsw_graph.png 2x"
        data-sizes="auto"
        alt="./nsw_graph.png"
        title="./nsw_graph.png" /></p>
<p>NSW 的搜索过程如下，这里借用知乎网友“工牌厂程序猿”的一张图：</p>
<div align="center">
  <img src="https://pic4.zhimg.com/80/v2-b8f847192cdd6fd0e0fe45158910dce7_720w.webp" alt="nsw" />
</div>
<ol>
<li>初始化三个集合，分别是 visited，candidate，result（定长）；随机选择初始点进入，并加入 visited 以及 candidate 集合, candidate 保存和 query 点的距离</li>
<li>寻找初始点的 n 最近邻点，加入到 visited 集合，注意如果友点在 visited 集合中则废弃，，对 n 个近邻点并行计算和 query 的距离，进行升序排序（由近到远）加入 candidate 集合</li>
<li>寻找 candidate 集合中的 n 个近邻点，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 C 点，只有 D 点没访问过，因为 D 点距离 query 点距离小于 C 到 query 点距离，所以 result 中将 C 换成 D 点，candidate 中将 C 换成 D 点</li>
<li>重复 3 步骤，寻找 D 的 n 哥最近邻，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 E 点和 G 点，因为 E 点距离 query 点距离小于 result 集合中的最大距离，所以 result 中将 H 换成 E 点，candidate 中将 E 点剔除</li>
<li>重复 3 candidate 集合中距离 query 最小距离的点 H 的距离比  result 集合中距离 query 最大的点 E 的距离还大，则停止查询</li>
</ol>
<p>Skip List 是一种高效的数据结构，可以在 O(log(n)) 的时间复杂度内找到与给定查询向量最相似的向量。Skip List 的核心思想是通过构建一个多层的链表，每一层都是一个有序的链表，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。</p>
<div align="center">
  <img src="https://pic1.zhimg.com/80/v2-714a3944759f3c85e7eb0d8a50c29b70_720w.webp" alt="skiplist" />
</div>
<p>这里需要注意 HNSW 的几个点：</p>
<ol>
<li>需要控制 HNSW 每一层的点最大连接数 Max, 在随机（越底层概率越大）插入节点时，如果有邻居节点 N 的连接数大于 Max，则对 N 进行 KNN 搜索重新与新的邻居建立连接。</li>
<li>启发式选边策略：在每一层搜索与插入点最邻近的 M 个节点的时候，它是先召回了 efConstruction 个，然后再选择出 M 个(efConstruction &gt;= M)，选择 M 的过程可以直接选择 Top-M 但是可能会降低整体的连通性，“工牌厂程序猿” 的文章具体列举了这个 case:</li>
</ol>
<div align="center">
  <img src="https://pic3.zhimg.com/80/v2-239f6f6e2790a8a130fc72c97756c68a_720w.webp" alt="hnsw" />
</div>
<p>这里的 efConstruction 是 4，M 是 2，如果直接选择 Top-M 的话，一定会选择 A 和 B, 这样 ABQ 和 CD 的连通性就降低了，这里在选择 A 后寻找第二个最近邻的时候检测 QA 和 AB 距离，如果 QA &gt; AB 则再寻找下一个最近邻，知道大于 QA 为止,这里找到 C 点时 AC &gt; AQ。</p>
<ol start="3">
<li>High degree vertex 越靠近最上层，这样可以减少搜索路径，提高搜索效率</li>
</ol>
<p>构建参数：</p>
<ul>
<li>efConstruction: 图构建过程中的一个参数，用来控制在为每个节点建立连接时，考虑的最近邻候选节点的数量。该参数具体影响的是图在构建过程中节点之间连接的质量。较高的 efConstruction 值意味着在为一个节点选择邻居时会考虑更多的候选节点，从而生成更优质的图结构。但是，较高的 efConstruction 值会增加构建图的时间和空间复杂度，而且在搜索时也会增加搜索时间。</li>
<li>m: 每个顶点添加的最大邻居数，分为 m_0=2m 以及 m_max=m, 参看<a href="https://github.com/facebookresearch/faiss/blob/main/faiss/impl/HNSW.cpp#L76" target="_blank" rel="noopener noreffer ">代码</a>.</li>
</ul>
<p>搜索参数：</p>
<ul>
<li>efSearch: 在搜索时，用来控制搜索的质量。较高的 efSearch 值意味着搜索时会考虑更多的候选节点，从而提高搜索的质量。但是，较高的 efSearch 值会增加搜索时间。</li>
</ul>
<p>HNSW 由于检索过程中涉及平均单条查询会产生数百个读磁盘操作，需要不停的寻找下一个随机点，放到 SSD 会导致极高的时延，所以是全内存的。</p>
<h4 id="nsg-navigating-spreading-out-graph">NSG (Navigating Spreading-out Graph)</h4>
<p>虽然 HNSW 的查询速度快，精度高，但是并非完美，它主要有以下几个痛点：</p>
<ul>
<li>索引复杂且占用内存大：HNSW 的核心是“多层高速公路”网络。这个多层结构虽然效果好，但实现起来相对复杂，而且为了存储每一层的图结构，需要占用不小的内存（RAM）。当数据量达到十亿、百亿级别时，内存成本会非常高。</li>
<li>构建过程复杂：在构建 HNSW 索引时，需要为每个新加入的节点随机选择一个层级，然后在该层级及以下的所有层级中寻找邻居并连接，这个过程也比较耗时。</li>
</ul>
<p>NSG 论文要解决的核心问题是：能不能设计一种**结构更简单、索引更小（更省内存）**的图，但搜索性能却能和复杂、庞大的 HNSW 相媲美，甚至超越它？
NSG 的创新可以总结为“一个理论，一个枢纽，一个技巧”。</p>
<ol>
<li>核心理论：从“任意两点”到“一个中心”的思想转变 (MRNG)</li>
</ol>
<ul>
<li>理想的图 (MSNET)：学术界早就提出过一种理想的图，叫做“单调搜索网络”（Monotonic Search Network, MSNET）。在这种图上搜索，你从任何一个点出发，每走一步都必然离你的目标更近。就像在一个山谷里找最低点，你只要一直往下走，保证能找到。这种图上搜索不会“走回头路”，效率极高。但问题是，构建一个完美的 MSNET 非常非常耗时，不切实际。</li>
<li>NSG的理论基础 (MRNG)：论文作者基于 MSNET 的思想，提出了一种新的、更稀疏的图叫 MRNG (Monotonic Relative Neighborhood Graph)。它也是一种能保证“单调性”的图，但构建规则更巧妙。然而，构建一个完整的 MRNG 仍然太慢了。</li>
<li>思想的飞跃：作者们意识到，我们真的需要保证任意两个点之间都有单调路径吗？在实际搜索中，我们总是从一个（或少数几个）入口点开始搜索。
<ul>
<li>HNSW 的思路：从顶层图的入口点开始，层层下沉。</li>
<li>NSG 的革命性思路：我们干脆只设置一个全局的“入口点”，我们称之为“导航节点”（Navigating Node）。我们不再追求构建一个“任意两点都能高效互通”的复杂网络，而是构建一个“从这个导航节点出发，到任何其他节点都有一条高效路径”的中心化网络。
这就好比城市交通规划：</li>
</ul>
</li>
<li>HNSW：建立了一个“地铁 + 公交 + 快速路”的立体交通网，你从任何地方到任何地方都很方便，但系统复杂，建设成本高。</li>
<li>NSG：在城市正中心建立一个巨大的“宇宙中心交通枢纽”。所有远距离出行，都先到这个枢纽，再从枢纽出发去目的地。整个网络结构变成了以枢纽为中心的放射状，大大简化了。</li>
</ul>
<ol start="2">
<li>核心设计：导航节点 (Navigating Node)</li>
</ol>
<p>这个“宇宙中心交通枢纽”就是 NSG 的灵魂——导航节点。</p>
<ul>
<li>如何选择？ 通常选择整个数据集的“质心”（几何中心）附近的一个点。这个点具有最好的全局视野。</li>
<li>它有什么用？
<ul>
<li>搜索的唯一入口：所有的查询，无论目标在哪里，都统一从这个导航节点开始。</li>
<li>建图的绝对核心：在为每个节点 p 选择邻居时，NSG 不再是盲目地在全局找，而是利用一个非常聪明的技巧。</li>
</ul>
</li>
</ul>
<ol start="3">
<li>核心技巧：高效的邻居选择策略</li>
</ol>
<p>这是 NSG 能够兼顾“图的稀疏性”（省内存）和“搜索的高效性”的关键。当给节点 p 构建边时，如何为它选择邻居？</p>
<ul>
<li>传统方法：在 p 周围一定范围内找最近的几个点。范围选多大？邻居选多少个？很难权衡。</li>
<li>NSG 的天才技巧：
<ul>
<li>我们先在图上（一个预先构建的粗糙 k-NN 图）模拟一次从“导航节点”到 p 的搜索。</li>
<li>这次搜索会经过一条路径，比如 导航节点 → a → b → &hellip; → p。</li>
<li>NSG 认为，这条路径上经过的所有节点 (a, b, &hellip;)，都是 p 的“绝佳邻居候选人”。为什么？因为这些点是连接全局中心和局部点 p 的关键桥梁。</li>
<li>最后，NSG 只需要在这些少量的“候选人”中，再使用 MRNG 的规则精选出最终的邻居。
这个技巧的妙处在于，它把一个全局的、开放的邻居选择问题，变成了一个局部的、数量有限的优化问题，极大地降低了建图的计算复杂度。</li>
</ul>
</li>
</ul>
<p>结合上面的创新点，我们来梳理一下 NSG 的完整流程：</p>
<ol>
<li>构建 (Build)</li>
</ol>
<ul>
<li>步骤一：找枢纽。计算数据质心，找到离质心最近的点作为“导航节点”。</li>
<li>步骤二：打基础。构建一个基础的、粗糙的 k-近邻图（k-NN Graph）。这一步是为了后续的路径搜索。</li>
<li>步骤三：连主路 (核心)。遍历图中除导航节点外的每一个节点 p：
<ol>
<li>以 p 为目标，从“导航节点”在上一步的 k-NN 图上进行一次搜索。</li>
<li>把搜索路径上访问过的所有节点都收集起来，形成 p 的“候选邻居池”。</li>
<li>从这个池子里，为 p 精心挑选出固定数量（比如 m 个）的邻居并连接。同时，为了保证图的稀疏性，严格控制每个节点出度的上限。</li>
</ol>
</li>
<li>步骤四：查漏补缺。由于严格限制了出度，可能会导致图的某些部分断开连接。最后，从导航节点开始做一次深度优先搜索（DFS），检查是否所有点都可达，如果发现有“孤岛”，就强行给孤岛上的点连一条边到主网络，确保图的连通性。</li>
</ul>
<ol start="2">
<li>搜索 (Search) NSG 的搜索过程异常简洁：</li>
</ol>
<ul>
<li>从“导航节点”出发。</li>
<li>不断地、贪心地跳到当前节点的所有邻居中，离查询目标最近的那个邻居。</li>
<li>重复上一步，直到找不到比当前节点更近的邻居为止，此时就到达了局部最优，也就是搜索结果。</li>
</ul>
<p>NSG 选边跟 HNSW 选择最小边策略不同。以点 r 为例，当 r 与 p 建立连接时，以 r 和 p 为圆心，r 和 p 的距离为半径，分别做圆，如果两个圆的交集内没有其他与 p 相连接的点，则 r 与 p 相连。在连接点 s 时，由于以 s 和 p 距离为半径的交集圆内，已有点 r 与 p 相连，所以 s 和 p 不相连。t点因为s点已经排除，所以保留,下图中最终与点 p 相连的点只有r, t 和 q，这样减少了冗余边，减少了平均出度。</p>
<div align="center">
  <img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/2/18/17057379bc7bdab3~tplv-t2oaga2asx-jj-mark:3024:0:0:0:q75.awebp" alt="nsg" />
</div>
<p>NSG 相对 HNSW 的优势：</p>
<ul>
<li>索引大小 (Index Size)：NSG 完胜。NSG 的索引（图占用的内存）通常只有 HNSW 的 1/2 到 1/3。这是 NSG 最大的优势，尤其是在处理超大规模数据集时，能节省巨大的硬件成本。</li>
<li>搜索性能 (Performance)：两者旗鼓相当，NSG 略有优势。在很多数据集上，NSG 的搜索速度（QPS, Queries Per Second）都超过了 HNSW，尤其是在要求高精度（High Recall）的场景下。</li>
<li>简洁性 (Simplicity)：NSG 完胜。NSG 是单层图结构，没有 HNSW 复杂的层级概念，无论是理解还是实现都更加简单。</li>
</ul>
<p>但是业界普遍还是采用 HNSW, 主要原因有以下几点：</p>
<ol>
<li>增量索引 (Incremental Indexing) 的能力至关重要</li>
</ol>
<ul>
<li>工业场景：在实际应用中，数据是动态变化的。比如，电商网站每秒都有新商品上架，社交网络每秒都有新图片发布。我们不可能每次都为了新增的几个数据点，就花几小时甚至几天去重建整个索引。我们需要一个能高效“添加”和“删除”单个数据点的索引。</li>
<li>HNSW 的优势：HNSW 的多层结构天然支持增量索引。当一个新节点加入时，算法会给它随机分配一个层级，然后从顶层开始，像普通查询一样找到它应该插入的位置，并连接附近的邻居。这个过程是局部化的，非常高效，不会影响整个图的结构。删除节点也相对容易处理。</li>
<li>NSG 的劣势：NSG 的设计哲学——依赖一个固定的全局“导航节点”——让增量索引变得非常困难。
<ul>
<li>添加节点：新节点 p 的邻居是通过从“导航节点”到 p 的路径来决定的。但新节点 p 还没有加入图，怎么找路径？即便加入，它的存在也可能会改变其他节点的最优路径，理论上可能需要更新大量节点的边，这违背了增量索引的初衷。</li>
<li>删除节点：如果删除的节点恰好是某个关键路径上的一环，可能会导致图的连通性被破坏，或者搜索效率下降。
这是 HNSW 在工业界占据主导地位的最核心、最关键的原因。 对于静态数据集（比如一次性构建好，不再改变的图像库），NSG 是个好选择。但对于绝大多数需要持续更新的在线服务，HNSW 的灵活性是不可替代的。</li>
</ul>
</li>
</ul>
<ol start="2">
<li>生态系统和成熟度 (Ecosystem and Maturity)</li>
</ol>
<ul>
<li>先发优势：HNSW 提出得更早，并且其作者很快将其集成到了当时非常流行的 nmslib 库中。随后，像 Faiss (Facebook AI)、Lucene (Apache) 等业界顶级的开源库都迅速跟进，提供了稳定、高效的 HNSW 实现。</li>
<li>社区和文档：有了这些顶级库的支持，HNSW 迅速积累了庞大的用户社区、丰富的文档、教程和生产环境下的实践经验。当一个工程师遇到问题时，他能很容易地找到解决方案。</li>
<li>信任和稳定性：HNSW 已经在无数公司的生产环境中经受了多年的考验，其鲁棒性（robustness）和稳定性得到了充分验证。对于追求稳定压倒一切的工业界来说，选择一个“久经沙场的老将”远比选择一个“初出茅庐的天才”要稳妥。</li>
</ul>
<ol start="3">
<li>鲁棒性和参数敏感度 (Robustness &amp; Parameter Sensitivity)</li>
</ol>
<ul>
<li>HNSW：其多层结构提供了很好的容错性。即使在高层走错了一步，下沉到更稠密的低层后，仍然有很大的机会修正路线，最终找到正确的目标。它的性能对参数（如 M, efConstruction）虽然敏感，但调整起来相对直观。</li>
<li>NSG：其性能高度依赖于“导航节点”的选择和图的连通质量。如果导航节点选得不好（比如选在了数据分布的边缘），或者在建图过程中为了稀疏性而剪枝过多，导致关键路径丢失，那么搜索性能可能会急剧下降。它的“天花板”可能很高，但“地板”也可能更低。</li>
</ul>
<h4 id="334-diskann">3.3.4 DiskANN</h4>
<p>DiskANN 是建立在这样一个背景下：</p>
<ol>
<li>ANNS 算法（如 HNSW、NSG）为了保证速度，都依赖于将索引存储在内存中。这导致存储成本非常高，限制了可以处理的数据集大小。</li>
<li>SSD 的潜力： 固态硬盘（Solid-State Drives, SSDs）相比传统的硬盘更快，但仍然比内存慢很多。大家普遍认为，将索引放在 SSD 上会导致搜索速度大幅下降，无法满足实际应用的需求。</li>
</ol>
<p>DiskANN 要解决的问题：如何设计一种新的 ANNS 索引，突破内存限制，使得我们能够在单台机器上，利用廉价的 SSD 存储十亿级别的数据，并保持高精度和低延迟？</p>
<p>DiskANN 系列有三篇文章，DiskANN，FreshDiskANN，FilterDiskANN。</p>
<p>DiskANN 的核心创新在于：算法 (Vamana 图) + 系统设计（SSD 优化）。</p>
<ol>
<li>核心算法：Vamana 图</li>
</ol>
<p>Vamana 图是 DiskANN 论文提出的新型图索引，它主要解决了以下两个问题：</p>
<ul>
<li>优化 SSD 访问模式： 普通的图索引（如 NSG、HNSW）在 SSD 上进行搜索时，会产生大量的随机读取，导致延迟很高。Vamana 图旨在减少 SSD 的随机读取次数，使其更倾向于顺序读取。</li>
<li>更好的搜索性能： 相比于 NSG 和 HNSW，Vamana 图在特定场景下可以提供更好的搜索精度和速度。
为了实现这些目标，Vamana 图的设计有以下几个关键点：</li>
<li>Robust Prune: 在构建图的过程中，Vamana 图使用了一种名为 Robust Prune 的策略来选择每个节点的邻居。这个策略的目标是确保搜索路径上的节点能够以一定的比例因子 alpha 更快地接近查询目标，从而减少搜索的跳数和 SSD 的读取次数。</li>
<li>Iterative Construction: Vamana 图的构建是一个迭代的过程。首先，图被初始化为随机连接的图，然后通过迭代地优化每个节点的邻居来改进图的结构，使其更适合于近似最近邻搜索。</li>
<li>Graph Merging: 为了处理大规模数据集，Vamana 图支持将多个小的图索引合并成一个大的图索引。这个特性使得我们可以并行地构建索引，并将其合并成一个完整的索引，从而提高构建效率。</li>
</ul>
<ol start="2">
<li>系统设计：SSD 优化策略</li>
</ol>
<p>DiskANN 的另一大创新在于针对 SSD 的特性进行了一系列优化，从而最大限度地发挥 SSD 的性能。这些优化策略包括：</p>
<ul>
<li>Beam Search: 为了减少 SSD 的随机读取次数，DiskANN 使用了一种名为 Beam Search 的策略。在搜索过程中，算法维护一个包含多个候选节点的集合（beam），并并行地从 SSD 中读取这些节点的邻居信息。这样可以减少搜索过程中的 round-trip 次数，从而降低延迟。</li>
<li>Caching: DiskANN 利用内存来缓存频繁访问的节点信息。通过将热点数据缓存在内存中，可以减少对 SSD 的访问次数，从而提高搜索速度。</li>
<li>Implicit Re-Ranking: DiskANN 使用 Product Quantization (PQ) 等向量压缩技术来减少内存占用。但是，压缩后的向量会损失精度。为了弥补这个损失，DiskANN 在从 SSD 中读取邻居信息时，同时读取原始的、未压缩的向量，并使用这些向量来重新排序候选结果，从而提高搜索精度。</li>
<li>Full-Precision with Neighborhoods: 将全精度向量和邻居信息一起存储在 SSD 上，读取邻居信息时也可以一起读取全精度信息，无需额外开销，便于使用全精度信息rerank。</li>
</ul>
<p>构图过程:</p>
<ol>
<li>首先构建随机图，这里和 NSG 的K最近邻图不一样，对每一个节点随机选择 R 个节点相连接</li>
<li>计算起点，找全局质心最近的点，目的是尽量减少平均搜索半径</li>
<li>搜索起点对每个点做 ANN，将搜索路径上所有的点作为候选邻居集，执行 alpha = 1 的裁边策略 （参考 NSG）</li>
<li>整 alpha &gt; 1(论文推荐 1.2）重复步骤 3。因为 3 是基于随机近邻图做的，第一次迭代后图的质量不高，所以需要再迭代一次来提升图的质量，这个对召回率很重要。这里拿上图举例，如果 alpha为1.2，当 ps 的距离大于 1.2 * pr 的距离，才会裁撤 ps 边。</li>
</ol>
<p>下图可以直观感受下，alpha=1 以及 alpha=1.2 最后图的区别,第一行是 alpha=1, 第二行是 alpha=1.2，alpha=1.2 的图更加稠密，明显多了长边，减少了搜索半径。</p>
<div align="center">
  <img src="https://pic3.zhimg.com/80/v2-2d4c5810d8074f4faac3ed9f4ae7ad1e_720w.webp" alt="diskann" />
</div>
<p>这时候你可能有疑问，如果按照这样建图，根本不可能在 64GB 的机器上存放超过 1B 的数据，这里就要有一些优化手段：</p>
<ol>
<li>先做全局 kmeans，将数据分成 k 个簇，然后将每个点分到距离最近的 I 个簇中，一般 I 取 2 就够了。对每个簇建基于内存的 Vamana 索引，最后将 k 个 Vamana 索引合并成一个索引。</li>
<li>使用量化的方法，建索引时用原始向量，查询的时候用压缩向量。因为建索引使用原始向量保证图的质量，搜索的时候使用内存可以 hold 住的压缩向量进行粗粒度搜索，这时的压缩向量虽然有精度损失，但是只要图的质量足够高，大方向上是对的就可以了，最后的距离结果还是用原始向量做计算的。</li>
<li>每个点的邻居集和原始向量数据存在一起。这样做的好处是可以利用数据的局部性。</li>
</ol>
<p>如果索引文件放在 SSD 上，为了保证搜索时延，尽可能减少磁盘访问次数和减少磁盘读写请求。因此 DiskANN 提出两种优化策略：</p>
<ol>
<li>缓存热点：将起点开始 C 跳内的点常驻内存，C 取 3~4 就比较好。</li>
<li>beam search： 简单的说就是预加载，搜索 p 点时，如果 p 的邻居点不在缓存中，需要从磁盘加载 p 点的邻居点。由于一次少量的 SSD 随机访问操作和一次 SSD 单扇区访问操作耗时差不多，所以我们可以一次加载 W 个未访问点的邻居信息，W 不能过大也不能过小，过大会浪费计算和 SSD 带宽，太小了也不行，会增加搜索时延。</li>
</ol>
<p>此外 DiskANN 相对于 HNSW 或者 NSW 还有一个优势就是，其删除一个节点是真删除，而且删除后不会影响图的连通性。HNSW 删除一个节点后，可能会导致图的连通性被破坏，需要重新构建索引。而 DiskANN 删除一个节点后，策略是这样的：假设我们要删除的节点是**“郑州”**这个交通枢纽。郑州在中国的交通网络中是一个至关重要的“十字路口”。</p>
<ul>
<li>有高速公路从“北京”、“西安”开往郑州（这些是郑州的in-neighbors）。</li>
<li>也有高速公路从郑州通往“武汉”、“南京”（这些是郑州的out-neighbors）。
当我们要删除“郑州”时：
<ol>
<li>确定“受影响的节点”：谁最受伤？是那些原本有路通往郑州的城市，比如“北京”和“西安”。它们的出行选择变少了。</li>
<li>提供“修复候选方案”：规划师对北京说：“你原来可以去郑州，现在郑州没了。但是郑州原来可以通往‘武汉’和‘南京’。现在我把‘武汉’和‘南京’作为新的高速公路修建提案交给你。”</li>
<li>运行 RobustPrune 进行智能决策：现在，北京的工程师（RobustPrune 算法）开始工作了。他会评估：</li>
</ol>
<ul>
<li>输入：我（北京）现有的所有高速公路 + 新的提案（通往武汉、南京）。</li>
<li>决策过程：在所有这些选项中，根据 RobustPrune 的核心原则（即选择那些能最大化导航效率、提供“捷径”且不冗余的连接），重新选出不超过 R 条最优的高速公路。</li>
<li>可能的结果：
<ul>
<li>北京的工程师发现，修一条到“武汉”的高速确实是个好捷径，于是决定修建。</li>
<li>他发现到“南京”的提案并不比现有路线好，于是放弃。</li>
<li>他甚至可能为了给到武汉的新路腾出名额（保持总数不超过R），拆掉了一条原来通往某个小城市的、不太重要的旧路。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>对所有受影响节点重复此过程：“西安”的工程师也会做同样的事情。</li>
</ol>
</li>
</ul>
<p>最终结果：
通过在每个受影响的节点上运行 RobustPrune，我们不是盲目地添加所有可能的连接，而是让每个节点根据自己的局部情况和全局导航目标，自主地、智能地选择最优的连接来“修复”网络破损的地方。
这样，我们既成功删除了目标节点“郑州”，又没有破坏网络的稀疏性（度限制 R），同时还最大程度地保持了整个高速公路网的导航质量。这就是 Vamana 删除操作的精髓所在。</p>
<p>DiskANN 的优势在于可以在很小的内存占用下配合 SSD 达到不错的搜索性能，但是规格小的机器在构建索引的时候会比较慢，这里需要做好权衡。</p>
<p>FreshDiskANN 在 DiskANN 基础上做了进一步的优化，主要是针对数据更新的场景。FreshDiskANN 通过引入一个新的索引结构，称为 FreshVamana，并引入删除列表，来支持并加速对数据的的插入和删除。于此同时采用两阶段 StreamingMerge 算法，将索引分为长期索引，临时索引，通过分层支持大规模数据。</p>
<p>FilterDiskANN 从满足查询标签条件（如日期、价格范围、语言）的索引点中找到查询嵌入的最近邻，支持两种过滤算法：</p>
<ol>
<li>FilteredVamana：构建单一索引，其中每个顶点的邻居基于几何结构和共同标签来决定。该算法在图构建过程中同时考虑向量距离和标签信息，形成既考虑几何邻近性又考虑标签相关性的连接</li>
<li>StitchedVamana：为每个标签构建单独的子图，然后将这些子图叠加并剪枝形成最终图。这种方法在实际数据集上consistently提供比FilteredVamana高2倍的QPS，但构建时间更长</li>
</ol>
<h4 id="335-summary">3.3.5 Summary</h4>
<p>这里总结下，内存占用上 HNSW 明显大于 IVF，LSH 以及 Flat(KNN)，召回率以及搜索速度上 HNSW 优于 IVF，LSH。DiskANN 在内存占用上优于 HNSW，但是在构建索引的时候会比较慢，搜索速度上优于 HNSW，但是召回率上不如 HNSW。所以在选择向量搜索算法的时候需要根据自己的需求来选择。</p>
<h3 id="34-向量搜索算法优化">3.4 向量搜索算法优化</h3>
<p>通过减少 Vector 大小，或者通过降维让搜索更快，这里列举了一些常见的向量搜索算法优化方法。</p>
<h4 id="341-pqproduct-quantization">3.4.1 PQ（Product Quantization）</h4>
<p>这里借用知乎网友的一张图，没办法，网友的图画的太好了：
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg"
        data-srcset="/vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg, /vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg 1.5x, /vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg 2x"
        data-sizes="auto"
        alt="/vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg"
        title="/vector-search/v2-8f9324cc9ee6d50b4f72eea54ff4abdc_b.jpg" width="600" height="196" /></p>
<p>构建阶段：</p>
<ol>
<li>首先将N个原始向量，各自切分为多个子向量。比如256维向量，切分为8个32维子向量</li>
<li>然后在每个子向量空间内进行聚类，可以采用KMeans等聚类算法。假设每个子空间有1024个聚类，对每个聚类中心编码，得到1024个ID</li>
<li>将原始向量编码成最近的聚类中心ID，最后做拼接</li>
</ol>
<p>检索阶段：</p>
<ol>
<li>检索向量进行切分</li>
<li>切分的子空间和计算每个聚类中心的距离，做成距离表</li>
<li>利用距离表计算query和候选样本在每个子空间的距离，累加后取 top-k</li>
</ol>
<p>其中涉及到切分都可以使用并行求解，这里一般不直接使用 PQ 因为依旧需要很多的距离计算，这里一般先进行 IVF 找到最有希望的top-k cluster 然后再进行 PQ。</p>
<h4 id="342-sqscalar-quantization">3.4.2 SQ（Scalar Quantization）</h4>
<p>SQ 比较简单
编码： scalar = (max-min)/255, floor(value-min/scaler) 如果小于0 则取 0，大于 255 则取 255，这样就将向量压缩到 0-255 之间，这样可以减少向量的大小，但是会损失一些信息。
解码：value = min + (code + 0.5)*(max-min)/255</p>
<h4 id="343-rabitq">3.4.3 RabitQ</h4>
<p>RabitQ 来源于论文 <a href="https://arxiv.org/abs/2405.12497" target="_blank" rel="noopener noreffer ">RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search</a>, 具体理论细节可以参考 RabitQ 的作者 Jianyang Gao 的 <a href="https://dev.to/gaoj0017/quantization-in-the-counterintuitive-high-dimensional-space-4feg" target="_blank" rel="noopener noreffer ">blog</a>。</p>
<p>RabitQ 指出现阶段 PQ 算法的两个问题：</p>
<ol>
<li>用 kmeans 的质心作为 codebook， 构建时启发式的近似估计，没有理论保证</li>
<li>距离估计，用量化后的向量和 query向量的距离估计原始向量和 query 向量的距离，缺乏近似误差范围
上述的两个问题就导致 PQ 可能在一些数据集或者真实场景下的误差非常大。</li>
</ol>
<p>如何解决上述问题：</p>
<ol>
<li>codebook 构建阶段
<ol>
<li>首先对数据向量进行归一化，以便将它们对齐在单位超球面上D维空间</li>
<li>构建一组 $2^{D}$ 坐标为的双值向量 $−1/\sqrt{D}$或者 $+1/\sqrt{D}$（即，该集合由超立方体的顶点组成，这些顶点均匀地分布在单位超球面上）</li>
<li>通过将每个双值向量乘以随机正交矩阵来随机旋转双值向量（即执行一种 Johnson-Lindenstrauss 变换)，为了消除确定性码本对特定向量的偏好</li>
<li>对于每个向量，将其与码本最接近的向量作为量化向量， 最小化内积。 由于每个量化向量都是旋转的 D 维双值向量，我们将其量化码表示为长度的位串D，其中 0 和 1 表示两个不同的值。 码本构造的基本原理是它具有清晰的几何解释（即码本中的向量是单位超球面上的一组随机旋转向量） 使得可以明确地分析数据向量、它们的量化向量和查询向量之间的几何关系。</li>
</ol>
</li>
<li>距离估计
<ol>
<li>根据上述几何关系仔细设计了数据向量和查询向量之间距离的估计器，并证明这个估计器是无偏的，而且提供了误差范围。</li>
<li>于此同时在估计距离时，即使使用较短的量化代码，也能以较小的经验误差估计出大约一半的优势</li>
</ol>
</li>
</ol>
<p>RaBitQ’s distance estimator：</p>
<ul>
<li>单个 data vector 使用 bitwise 按位操作</li>
<li>批量数据使用 SIMD 加速</li>
</ul>
<p>使用随机 codebook 避免双值 codebook 在一些特定向量表现不佳，比如 $1/\sqrt{D}$&hellip; $−1/\sqrt{D}$ 和 (1, 0, 0, 0) 的量化，我们使用随机正交矩阵去乘这个 codebook, 让 codebook 单位向量有相同的概率旋转到单位超球面上的任意位置</p>
<p>这里使用通俗的话讲解下 RabitQ 的核心思想：</p>
<p><strong>终极解说：高维空间中的闪电搜图术 (RaBiT-Q 算法)</strong></p>
<p>想象一下，我们正在为世界上最大的图片分享网站（比如Instagram或Pinterest）构建一个“以图搜图”的功能。这意味着，当用户上传一张“沙滩上的柯基犬”的照片时，我们要能从包含数十亿张图片的数据库里，瞬间找出所有其他“沙舍上的柯基犬”或者类似的图片。</p>
<p>这是一个巨大的挑战。计算机是如何“看懂”并比较图片的呢？</p>
<ol>
<li><strong>第一步：把图片变成“数字指纹”——向量化</strong></li>
</ol>
<p>计算机无法直接理解图片。它首先需要通过一个复杂的神经网络（比如 ResNet），将每一张图片都转换成一个由很多数字组成的“列表”，我们称之为<strong>向量 (Vector)</strong>。这个向量就是这张图片的“数字指纹”，它捕捉了图片的核心特征。</p>
<ul>
<li><strong>小白理解：</strong> 就像我们给人办身份证，除了照片，还会记录身高、体重、血型等信息。这里的“向量”就是一张图片的“数字身份证”，包含了成百上千个描述它的“特征值”。</li>
<li><strong>专业说明：</strong> 我们使用预训练的深度学习模型（如CNN）提取图像的高维特征嵌入（Embedding）。例如，一张图片可以被表示为一个128维或更高维度的浮点数向量 <code>o_r</code>。我们的目标是，对于一个查询向量 <code>q_r</code>，找到数据库中与它<strong>欧几里得距离</strong> <code>||o_r - q_r||</code> 最小的那些 <code>o_r</code>。</li>
</ul>
<ol start="2">
<li><strong>第二步：数据大扫除——归一化</strong></li>
</ol>
<p>直接比较这些原始的“数字指纹”既慢又困难。数据可能存在整体的偏移，而且每个向量的“长度”（数值大小的综合体现）也各不相同，会干扰我们对“方向”（内容相似度）的判断。</p>
<p><strong>操作：</strong></p>
<ol>
<li><strong>中心化 (Centering):</strong> 找到所有图片向量的“平均位置”（即<strong>质心 <code>c</code></strong>），然后让每个向量都减去这个中心。这相当于把整个坐标系的原点“搬”到了数据中心。</li>
<li><strong>归一化 (Normalization):</strong> 将每个中心化后的向量，都除以它自身的长度。</li>
</ol>
<p><strong>结果：</strong> 经过这一步，所有的“数字指纹”都发生了两个变化：</p>
<ol>
<li>它们不再是散乱的，而是都围绕着同一个中心。</li>
<li>它们的“长度”都变成了 <strong>1</strong>。它们现在都“趴”在一个巨大的、高维的“单位球”的球面上。这个操作保留了向量的<strong>方向</strong>（代表图片内容），而舍弃了其原始的<strong>长度</strong>。</li>
</ol>
<ul>
<li><strong>小白理解：</strong> 想象一下，我们把所有人的身高都统一拉伸或压缩到1米8，但保持他们高矮胖瘦的“比例”不变。这样一来，大家就都站在了同一个起跑线上，比较起来更公平。</li>
<li><strong>专业说明：</strong> 我们将欧氏距离的计算问题，通过 <code>||o_r - q_r||² = ||o_r - c||² + ||q_r - c||² - 2 · ||o_r - c|| · ||q_r - c|| · &lt;q, o&gt;</code> 这个公式，转化为了一个求解归一化后的单位向量 <code>q</code> 和 <code>o</code> 之间<strong>内积 <code>&lt;q, o&gt;</code></strong> 的问题。<code>||o_r - c||</code> 可以在索引时预计算，<code>||q_r - c||</code> 在查询时计算一次即可。问题的核心变成了如何快速估算 <code>&lt;q, o&gt;</code>。</li>
</ul>
<ol start="3">
<li><strong>第三步：建立“宇宙坐标系”——码本构建与随机化</strong></li>
</ol>
<p>即使所有向量都在单位球面上，它们的数量依然是海量的。逐一比较内积还是太慢。我们需要一套“参考坐标系”来快速给向量定位。</p>
<p><strong>操作：</strong></p>
<ol>
<li><strong>码本 (Codebook):</strong> 我们在球面上建立一套“参考点”。一个绝妙的方法是，在球内部嵌入一个<strong>超立方体 (Hypercube)</strong>，它的所有顶点正好落在球面上。这些顶点非常均匀、对称地分布，是理想的参考点。这些顶点的集合，就是我们的“码本”。</li>
<li><strong>随机化 (Randomization):</strong> 一个固定的码本有其“盲区”。为了解决这个问题，我们不使用一个固定的立方体，而是将它进行<strong>随机旋转</strong>。我们生成M个（比如16个）不同的随机旋转，得到<strong>M个</strong>姿态各异的“码本”。</li>
</ol>
<ul>
<li><strong>小白理解：</strong> 为了快速找到地球上的某个城市，我们不会去拿尺子量。我们会先看它属于哪个大洲（亚洲、欧洲&hellip;）。这里的“码本”就是“七大洲”的划分。但固定的七大洲划分可能对某些边界城市不友好。于是我们干脆设计了M套不同的、随机的“大洲划分法”，从多个角度来给城市定位。</li>
<li><strong>专业说明：</strong> 码本由超立方体的顶点 <code>{-1, 1}^D</code> 构成。我们生成M个随机正交旋转矩阵 <code>R_1, ..., R_M</code>。这样我们就拥有了M个码本 <code>C_1, ..., C_M</code>，其中 <code>C_i</code> 是由 <code>R_i</code> 旋转后的超立方体顶点组成。</li>
</ul>
<ol start="4">
<li><strong>第四步：给每张图片打上“压缩编码”——索引构建</strong></li>
</ol>
<p>这是离线完成的工作，为整个数据库建立一个可以被闪电搜索的索引。</p>
<p><strong>操作：</strong>
对于数据库中的<strong>每一张</strong>图片（即每一个向量 <code>o</code>），我们：</p>
<ol>
<li>用之前生成的那M个旋转矩阵，去旋转 <code>o</code>。</li>
<li>将M个旋转后的 <code>o</code>，分别在M个码本中找到离它<strong>最近的那个顶点</strong>。</li>
<li>因为超立方体的顶点坐标只有+1和-1，我们可以用<strong>1个比特</strong>（0或1）来表示。比如+1记为1，-1记为0。</li>
<li>这样，每个 <code>o</code> 就被转换成了 <strong>M个</strong>D维的二进制码。</li>
</ol>
<p><strong>结果：</strong> 一个原本需要数千比特存储的浮点数向量，现在被压缩成了M个D比特的二进制码。存储空间大大减少，而且为后续的快速位运算比较铺平了道路。</p>
<ul>
<li><strong>小白理解：</strong> 我们为公司里的每个员工，都从M个不同的角度拍了一张照片，并根据照片生成了一个极简的“素描码”（二进制码），存入他的档案。</li>
<li><strong>专业说明：</strong> 对于每个 <code>o</code>，我们计算 <code>b_i = Quantize(R_i * o)</code>，其中 <code>i=1...M</code>。<code>Quantize</code> 函数找到超立方体中最近的顶点，并将其表示为D比特的二进制码 <code>b_i</code>。同时，我们也预计算并存储 <code>&lt;ō, o&gt;</code>，即量化误差项，为后续的精确估算做准备。</li>
</ul>
<ol start="5">
<li><strong>第五步：闪电搜索——查询与估算</strong></li>
</ol>
<p>当用户上传新图片（查询向量 <code>q</code>）时，实时搜索开始。</p>
<ol>
<li><strong>查询编码：</strong> <code>q</code> 必须经历和数据库图片完全相同的处理：归一化、用M个相同的旋转矩阵旋转、生成M个二进制码。</li>
<li><strong>粗筛 (Candidate Search):</strong> 我们拿着 <code>q</code> 的M个二进制码，去数据库中进行M次搜索。搜索方式不是慢速的浮点数计算，而是超高速的<strong>位运算</strong>。我们计算查询码和数据库码的<strong>汉明距离</strong>（有多少位不同），快速捞出几百个最相似的“候选图片”。</li>
<li><strong>精排 (Re-ranking &amp; Estimation):</strong> 对这几百个候选图片，我们不能再用粗糙的二进制码了。我们需要一个更精确的估算。这时，那个神奇的<strong>估算公式</strong>登场了：
<strong><code>&lt;o, q&gt; ≈ &lt;ō, q&gt; / &lt;ō, o&gt;</code></strong>
<ul>
<li>这个公式告诉我们，我们想知道的真实内积 <code>&lt;o, q&gt;</code>，可以用两个可以被高效计算或提前算好的值来近似！</li>
<li><strong>为什么可以这么近似？</strong> 因为我们天才地利用了<strong>高维空间的集中现象</strong>。我们证明了，在随机旋转下，那个导致公式无法简化的“麻烦项”<code>&lt;ō, e₁&gt;</code> 的值会高度集中于0，可以忽略不计。这是整个算法的理论基石和点睛之笔。</li>
</ul>
</li>
<li><strong>最终排序：</strong> 我们用这个估算公式，为几百个候选图片计算出近似的内积，再结合之前的第一步公式，得到最终的距离排序。将最靠前的结果返回给用户。</li>
</ol>
<ul>
<li><strong>小白理解：</strong>
<ol>
<li>新来的访客也被从M个角度拍了照，生成M个“素描码”。</li>
<li>安保拿着这些素描码去档案室飞快地比对（位运算），找出几百个长得最像的员工。</li>
<li>对于这几百个员工，安保不再看素描了，而是拿出一个“神奇估算器”，通过一个简单的除法，就估算出了访客和每个候选员工的真实“亲近度”，然后排出名次。</li>
</ol>
</li>
</ul>
<p><strong>总结</strong></p>
<p>这个算法的精髓在于，它完美地融合了多个强大的思想：</p>
<ol>
<li><strong>降维思想：</strong> 将复杂的欧氏距离问题，转化为更简单的单位向量内积问题。</li>
<li><strong>量化压缩：</strong> 用超立方体码本将浮点向量压缩成二进制码，极大节省了存储和计算。</li>
<li><strong>随机化思想：</strong> 通过随机旋转码本，避免了算法的“盲区”，并激活了高维空间的奇特性质。</li>
<li><strong>高维几何：</strong> 深刻利用了“集中现象”这一反直觉的数学事实，推导出了一个极为高效的近似估算公式，完成了从“粗筛”到“精排”的最后一跃。</li>
</ol>
<p>最终，一个原本需要在数十亿数据上进行慢速、精确计算的“不可能完成的任务”，被巧妙地分解为<strong>离线编码</strong>和<strong>在线闪搜</strong>两部分，实现了在保证高精度的前提下的数量级加速。</p>
<h2 id="4-常见的向量数据库极其优劣">4. 常见的向量数据库极其优劣</h2>
<p>以下列举了一些常见的向量数据库，以及他们的优劣势, 有些是专用向量数据库，有些是对现有关系型数据库的扩展。</p>
<h3 id="41-milvushttpsmilvusio">4.1 <a href="https://milvus.io/" target="_blank" rel="noopener noreffer ">Milvus</a></h3>
<p>Milvus 是一款非常优秀的开源向量数据库，支持多种向量搜索算法，包括 HNSW，DiskANN，IVF 等。 除了向量检索的基本功能，还提供 sharding, streaming data ingestion 以及 hybrid search 等功能。</p>
<p>Milvus 采用的是云原生的存算分离，shared-everthing 的架构，且控制面和数据面分离。各个组件都是独立且可横向拓展的，分别为：</p>
<ul>
<li>接入层(Access Layer)：由一组无状态 proxy 组成。它对外提供用户连接的 endpoint，负责验证客户端请求并合并返回结果。
<ul>
<li>它使用Nginx、Kubernetes Ingress、NodePort、LVS 等负载均衡组件提供统一的服务地址。</li>
<li>由于 Milvus 采用大规模并行处理 (MPP) 架构，代理会聚合并后处理中间结果，然后将最终结果返回给客户端</li>
</ul>
</li>
<li>协调服务(Coordinator Service)：负责分配任务给执行节点，分别有 root coord、data coord、query coord。
<ul>
<li>root coord: 处理数据定义语言 (DDL) 和数据控制语言 (DCL) 请求，例如创建或删除集合、分区或索引，以及管理 TSO（时间戳 Oracle）和 time ticker。</li>
<li>data coord: 管理 data 以及 index 节点拓扑，维护元数据，触发flush、compact、索引构建等后台数据操作。</li>
<li>query coord: 管理查询节点 topology ，load balancing 以及 growing segments 到 sealed segments 转换.</li>
</ul>
</li>
<li>执行节点(Worker Node)：执行从协调服务分配的任务以及 proxy DML 命令。
<ul>
<li>Query Node：检索增量日志数据，并通过订阅 log broker 将其转换为不断增长的段，从对象存储加载历史数据，并在向量和标量数据之间运行混合搜索。</li>
<li>Data Node：通过订阅 log broker 来获取增量日志数据，处理mutation 请求，并将日志数据打包成日志快照并存储在对象存储中</li>
<li>Index Node：构建索引。索引节点不需要常驻内存，可以通过Serverless 框架来实现。</li>
</ul>
</li>
<li>存储层(Storage)：对象存储负责存储数据，存储 Data files 和 Index files。
<ul>
<li>Meta Storage: 元存储存储元数据的快照，例如 collection schema 和 message consumption checkpoints。存储元数据需要极高的可用性、强一致性和事务支持，因此 Milvus 选择了 etcd 进行元存储。 Milvus 还使用 etcd 进行服务注册和健康检查。</li>
<li>Object Storage: 存储日志的快照文件、标量和向量数据的索引文件以及中间查询结果。 Milvus 使用 MinIO 作为对象存储，可以轻松部署在 AWS S3 和 Azure Blob。但对象存储的访问延迟较高，且按查询次数收费。为了提高性能并降低成本，Milvus 计划在基于内存或 SSD 的缓存池上实现冷热数据分离。</li>
<li>Log Broker: 发布-订阅系统，它负责流数据持久化和事件通知。当工作节点从系统故障中恢复时，它还能确保增量数据的完整性。 Milvus cluster 使用 Pulsar 作为 log broker； Milvus standalone 使用 RocksDB 作为 log broker。此外，log broker 可以很容易地替换为Kafka等流数据存储平台。</li>
</ul>
</li>
</ul>
<p>Milvus 的云原生架构是其优点，同时也给开发者带来了不小的挑战，例如新概念的学习，以及相关组件 Pulsar 或者 etcd 带来的运维管理上的挑战。</p>
<h3 id="44-pineconehttpswwwpineconeio">4.4 <a href="https://www.pinecone.io/" target="_blank" rel="noopener noreffer ">Pinecone</a></h3>
<h3 id="45-qdranthttpsqdranttech">4.5 <a href="https://qdrant.tech/" target="_blank" rel="noopener noreffer ">Qdrant</a></h3>
<h3 id="46-pgvectorhttpsgithubcompgvectorpgvector">4.6 <a href="https://github.com/pgvector/pgvector" target="_blank" rel="noopener noreffer ">Pgvector</a></h3>
<h3 id="47-pgvectorshttpsgithubcomtensorchordpgvectors">4.7 <a href="https://github.com/tensorchord/pgvecto.rs" target="_blank" rel="noopener noreffer ">Pgvecto.rs</a></h3>
<h3 id="48-vectorchordhttpsgithubcomtensorchordvectorchord">4.8 <a href="https://github.com/tensorchord/VectorChord" target="_blank" rel="noopener noreffer ">VectorChord</a></h3>
<p>如何选型参考 <a href="https://mp.weixin.qq.com/s/LtMCoSgGLSSWJU4IceRwiA" target="_blank" rel="noopener noreffer ">大模型时代如何选向量数据库？Milvus（Zilliz）、LanceDB、Chroma、Pinecone四大热门技术全解析！</a>。</p>
<h2 id="5-优秀的向量搜索库以及向量数据库开源项目">5. 优秀的向量搜索库以及向量数据库开源项目</h2>
<h2 id="6-向量数据库商业化你需要知道什么">6. 向量数据库商业化你需要知道什么</h2>
<h2 id="7-总结">7. 总结</h2>
<p>在这里我也是走马观花的介绍了一些向量搜索的基础知识，以及一些常见的向量搜索算法，向量搜索应用场景，向量搜索算法优化，常见的向量数据库极其优劣势，优秀的向量搜索库以及向量数据库开源项目，后面还是希望可以应用到实际的场景中。希望这篇文章能够帮助你更好的了解向量搜索。</p>
<h2 id="8-引用">8. 引用</h2>
<p>非常感谢 Pinecone 的文章，让我对向量数据库有了更深的了解。</p>
<ul>
<li><a href="https://www.pinecone.io/learn/series/faiss/vector-indexes/">https://www.pinecone.io/learn/series/faiss/vector-indexes/</a></li>
<li><a href="https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/">https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/379372268">https://zhuanlan.zhihu.com/p/379372268</a></li>
<li><a href="https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/">https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/</a></li>
<li><a href="https://www.xiemingzhao.com/posts/hnswAlgo.html">https://www.xiemingzhao.com/posts/hnswAlgo.html</a></li>
<li><a href="https://whenever5225.github.io/2020/05/11/hnsw-heuristic/">https://whenever5225.github.io/2020/05/11/hnsw-heuristic/</a></li>
<li>Search Engine For AI：高维数据检索工业级解决方案 <a href="https://zhuanlan.zhihu.com/p/50143204">https://zhuanlan.zhihu.com/p/50143204</a></li>
<li><a href="https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw">https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw</a></li>
<li><a href="https://blog.wilsonl.in/graph-vector-search/">https://blog.wilsonl.in/graph-vector-search/</a></li>
</ul>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2025-09-08&nbsp;<a class="git-hash" href="https://github.com/xieydd/xieydd.github.io/commit/15b6eabf1a4dc01fb93dc13d98f27ca3063304a4" target="_blank" title="commit by xieydd(xieydd@gmail.com) 15b6eabf1a4dc01fb93dc13d98f27ca3063304a4: update">
                                    <i class="fas fa-hashtag fa-fw" aria-hidden="true"></i>15b6eab</a></span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/vector-search/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://blog.xieydd.top/vector-search/" data-title="向量数据库中的门门道道" data-hashtags="vector search,Postgres"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://blog.xieydd.top/vector-search/" data-hashtag="vector search"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://blog.xieydd.top/vector-search/" data-title="向量数据库中的门门道道"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://blog.xieydd.top/vector-search/" data-title="向量数据库中的门门道道"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://blog.xieydd.top/vector-search/" data-title="向量数据库中的门门道道"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/vector-search/">vector search</a>,&nbsp;<a href="/tags/postgres/">Postgres</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav">
            <a href="/kubernetes-topo-aware-all-you-need-know/" class="next" rel="next" title="关于 Kubernetes 中，拓扑感知你需要知道的一切">关于 Kubernetes 中，拓扑感知你需要知道的一切<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="disqus_thread" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://disqus.com/?ref_noscript">Disqus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.62.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://blog.xieydd.top/" target="_blank">xieydd</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><script type="text/javascript" src="https://xieydd.disqus.com/embed.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{},"search":{"algoliaAppID":"PASDMWALPK","algoliaIndex":"index.zh-cn","algoliaSearchKey":"b42948e51daaa93df92381c8e2ac0f93","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-T4DQ9F4S5V', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-T4DQ9F4S5V" async></script></body>
</html>
