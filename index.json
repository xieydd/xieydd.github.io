[{"categories":["tech","2025","training","serving"],"contents":"LLM Infra 你需要知道的一切 GPT 3 GPT 模型是 LLM 模型的起点，你如果说 Transformer 是，那也没错。既然我们要讨论 LLM Infra，那么我们需要知道我们的系统架构以及设计是给谁服务的。如果不了解模型架构，你对于后面介绍的 Infra 设计，比如推理阶段的 Prefill-Decode Disaggregation，Context Cache Migration，Traffic-Adaptive Request Routing 等，就会感到困惑。知其然，知其所以然。我们从 GPT3 开始，因为 GPT1 以及 GPT2 的参数量都太小了，GPT2 参数只有 1.5B, 而 GPT3 参数有 175B。如果对 Transformer 架构不熟悉，推荐这篇文章 How Transformers Work;中文用户推荐知乎文章 Transformer模型详解（图解最完整版）。\n 首先确定 GPT3 的输入输出：\n 输入：一段文本，长度为 2048 个 token 输出：输入 2048个 token 下一个 token 的概率分布  这里的 token 是使用 Byte Pair Encoding (BPE) tokenizer 进行标记的。比如我使用 OpenAI Tokenizer 对 I am xieydd, I love GPT3. 进行标记后的结果如下, token ids 为 [40, 716, 2124, 494, 88, 1860, 11, 314, 1842, 402, 11571, 18, 13]：\n Encoding Tokenizer 后的 token ids 还只是一串数字，并非向量。我们需要将其向量化，每一个 token 转换成 50257 维向量，向量 token id 位置为 1， 其他位置为 0。这样我们就得到了输入向量，长度为 2048 * 50257 = 102760896。\n Embedding 这个向量太稀疏了，我们需要将其降维，将其降维到 2048 * 12288。\n Positional Encoding 由于 Transformer 没有像 RNN 那样的递归机制，因此使用位置编码来提供有关序列中每个 token 的位置的信息。这使他们能够理解句子中每个单词的位置。Transformer 使用 sin 以及 cos 结合的方式，让 position encoding 可以编码任意长的句子。\n Attention Vector 以及 Positional Encoding 相加后，得到了输入向量，接下来我们就要进行 Attention 了。这里将 2048 * 12288 的输入简化成 3*512。\n     Image from https://dugas.ch/artificial_curiosity/GPT_architecture.html\n  输入向量，通过权重 Wq, Wk, Wv 得到 Query, Key, Value。 Query 与 Key 进行点积，然后除以根号 d 进行 softmax 得到 attention score attention score 与 Value 进行点乘  Sparse Attention GPT3 并没有使用 Attention, 而是使用 Sparse Attention。sparse attention 除了相对距离不超过 k 以及相对距离为 k，2k，3k，\u0026hellip; 的 token，其他所有 token 的注意力都设为 0，如下图所示：\n 好处有两点：\n 减少注意力层的计算复杂度，节约显存和耗时，从而能够处理更长的输入序列； 具有“局部紧密相关和远程稀疏相关”的特性，对于距离较近的上下文关注更多，对于距离较远的上下文关注较少；  Multi-Head Attention  GPT3 将上述过程重复执行 96 次，有96个头,最后将输出在列行进行 concat。\nFeed Forward Feed Forward 层是一个全连接层，输入是 Multi-Head Attention 的输出，输出是 2048 * 12288。\n Add \u0026amp; Norm 将进入 Multi-Head Attention 的前的输入和 Feed Forward 的输出相加，然后进行 Layer Normalization。\n Decoding 我们需要将 Add \u0026amp; Norm 的输出 2048 * 12288 转换回 2048 * 50257，这样才能知道 2048 个 token 下一个 token 的概率分布。我们复用了 Embedding 层的权重，将其转置后与 Add \u0026amp; Norm 的输出相乘，得到 2048 * 50257 的输出。随后经过 Softmax 得到概率分布, GPT3 取 topk 个 token 作为输出。\n  了解了 GPT3 的架构，我们就可以更好的理解 LLM Infra 的设计。下面我们进入如何训练大模型的话题。这里我们使用目前 SOTA 的开源模型 DeepSeek-V3 来进行讲解。\nInfra GPU 集大成者 GB200 NVL72:\n Blackwell Architecture NVIDIA Grace CPU Fifth-Generation NVIDIA NVLink NVIDIA Networking   NVIDIA Quantum-X800 InfiniBand NVIDIA Spectrum™-X800 以太网 NVIDIA BlueField®-3 DPU  CUDA Network NVLink 机器内通信可以通过：\n PCIe 内存 NVLink  简单来说就是目前的 PCIe 5.0 的带宽无法满足 AI 分布式训练中参数同步的时间需求，需要带宽更大的机器内 device 通信的方式。NVLink 是给单机多 GPU 之间提供高速互联的技术，替换 PCIe。\n 随着 NVLink 的代次的发展，速度也在不断地提升，到目前第五代，单个 NVIDIA Blackwell Tensor Core GPU 支持多达 18 个 NVLink 100 GB/s 连接，总带宽可达 1.8 TB/s，比上一代产品提高了两倍，是 PCIe 5.0 带宽的 14 倍之多。\n 注意只有 SXM 口才能真正的使用 NVLink P2P 通信，而不是 NVLink Bridge 连接。 NVLink 最新的技术 NVLink C2C interconnect 技术：将两个高性能 NVIDIA Blackwell Tensor Core GPU 和一个 NVIDIA Grace CPU 连接到两个 Blackwell GPU，最终的目的是使 NVIDIA GPU、DPU 和 CPU 能够与定制芯片实现一致互连, 速度达到 900 GB/s，下图是 NVIDIA GH200 Grace Hopper Superchip 的架构图：\n NVSwitch  当 GPU 通过 NVLink 进行 P2P 连接时，最高总带宽可以达到 1.8TB/s, 但是每个 GPU 必须将相同的 1.8TB/s 连接分成七个专用的 256 GB/s 点对点连接，也就是 GPU 和 GPU之间的通信最大只能达到 1.8TB/s。如果想突破这个限制就需要 NVLink Switch, NVSwitch 其实是个 ASIC 芯片。 NVLink Switch 可以保证 GPU 之间的通信带宽达到 1.8TB/s 的完整带宽，大幅压缩通信时间，在推理过程中实时响应 tok/s/user 相对于没有 NVSwitch 的情况提升 1.4倍，而且batch 越大，提升越明显。\nInfiniBand 参考 知乎-AI 集群基础设施 InfiniBand 详解。\n机器间通信可以通过：\n TCP/IP 协议 RDMA(Remote Direct Memory Access) 协议   InfiniBand RoCE (RDMA over Converged Ethernet) iWARP (Internet Wide Area RDMA Protocol)  RDMA 协议可以不像 TCP/IP 协议需要 CPU 参与，数据需要经过数据链路层，网络层，传输层，应用层，CPU 参与数据包的接受，解包等操作。RDMA的内核旁路机制，允许应用与网卡之间的直接数据读写，同时RDMA的内存零拷贝机制，允许接收端直接从发送端的内存读取数据，极大的减少了CPU的负担，提升CPU的效率。\n RDMA网络，分为三类分别是Infiniband、RoCE、iWARP。其中，Infiniband 是一种专为 RDMA 设计的网络，从硬件级别保证可靠传输，而 RoCE 和 iWARP 都是基于以太网的 RDMA 技术，支持相应的 verbs 接口。\n IB（InfiniBand）： 搭建基于 IB 技术的 RDMA 网络需要专用的 IB 网卡和 IB 交换机。\niWARP（Internet Wide Area RDMA Protocal）： 基于 TCP/IP 协议的 RDMA 技术，由 IETF 标 准定义。iWARP 支持在标准以太网基础设施上使用 RDMA 技术，但服务器需要使用支持 iWARP 的网卡。\nRoCE（RDMA over Converged Ethernet）： 基于以太网的 RDMA 技术，也是由 IBTA 提出。RoCE 支持在标准以太网基础设施上使用RDMA技术，但是需要交换机支持无损以太网传输，需要服务器使用 RoCE 网卡。\nIB 链路速率随着网络带宽的升级越来越快，如下图：\n 这里的 x4, x12 是线缆中 Link 数量，x4 代表 4 个 Link，x12 代表 12 个 Link，一个 Link 一来一回两条线。\nInfiniBand 的线缆有别于以太网线缆以及光纤线缆，InfiniBand网络互连产品包括：DAC高速铜缆、AOC有源线缆以及光模块。\nMellanox 在全球 InfiniBand 市场的占有率基本上无敌的存在，在英伟达收购 Mellanox 之后，也于2021年推出了自己的第七代 NVIDIA InfiniBand 架构：NVIDIA Quantum-2。\nNVIDIA Quantum-2 平台包括：NVIDIA Quantum-2 系列交换机、NVIDIA ConnectX-7 InfiniBand 适配器、BlueField-3 InfiniBand DPU以及电缆。 目前最新的是：\n NVIDIA Quantum-X800 InfiniBand  NVIDIA® ConnectX® InfiniBand HCAs(Host Channel Adapters),ConnectX-8 InfiniBand SuperNIC 可提供每秒 800 Gb/s 的数据吞吐量 InfiniBand Switches 最高 144 个口，每个口 800 Gb/s 一共 115Tb/s   NVIDIA Spectrum™-X800 以太网  通过 SuperNICs 以及 NVIDIA Spectrum-X800 SN5600 Ethernet switch 提供 800 Gb/s 的吞吐量 Switch 最高 64 个口，每个口 800 Gb/s 一共 51.2Tb/s   NVIDIA BlueField®-3 DPU  支持以太网以及 InfiniBand 连接， 400Gb/s 速度 NVIDIA DOCA 软件框架进行 SDN 网络、安全、存储、管理、AI 加速等功能   BlueField SuperNICs: Provides best-in-class remote direct-memory access over converged Ethernet (RoCE) network connectivity between GPU servers at up to 400Gb/s。  GPUDirect 是 NVIDIA 开发的一项技术，可实现 GPU 与其他设备（例如网络接口卡 (NIC) 和存储设备）之间的直接通信和数据传输，而不涉及 CPU。 GPU Direct 技术包括 GPUDirect Storage、GPUDirect RDMA、GPUDirect P2P 和 GPUDirect Video。\n GPUDirect P2P 主要是单机多卡场景下，让 GPU可以通过PCI Express直接访问目标GPU的显存 GPUDirect RDMA 主要是在多机多卡场景下，计算机1的GPU可以直接访问计算机2的GPU内存，而不需要 CPU -\u0026gt; CPU 再由计算机 2 的 CPU -\u0026gt; GPU 多次拷贝  Communication MPI (Message Passing Interface) 是一种并行计算中常用的通信协议，MPI 有多种实现，比如 OpenMPI, Gloo, NCCL 等。\nNCCL（Nvidia Collective multi-GPU Communication Library）实现了针对 NVIDIA GPU 性能优化的多 GPU 和多节点集合通信原语。NCCL 提供了诸如 All Gather，All Reduce，Broadcast，Reduce，Reduce-Scatter 等实现，这些实现优化后可以通过 PCIe、 NVLink、InfiniBand 等高速互联，从而实现高带宽和低延迟。\n介绍几种通信原语：\n Reduce：从多个sender那里接收数据，最终combine到一个节点上。   AllReduce：Reduce 的变种，将所有节点的数据都reduce到所有节点上。   Broadcast：一个节点将数据发送到所有节点。  不同的网络拓扑对 NCCL 通信的速度有很大影响，如下图所示：\n  第一种是两个GPU通过CPU然后通过QPI和另一个CPU上的两块卡相连，因此速度最慢，但也能达到\u0026gt;5GB/s。 第二种是两个GPU通过PCIe switch相连后再经过CPU连接，速度会稍微低一点。 第三种是四张卡都在一个PCIe switch上，所以带宽较高，能达到\u0026gt;10GB/s PCIe的带宽大小。 第四种是DGX-1架构，通过 NVLink 直接连接，速度最快，能达到 60GB/s。    左图2机8卡，机内PCIe，机间InfiniBand能达到\u0026gt;10GB/s的速度，InfiniBand基本上能达到机内的通信速度； 右图4机32卡，机内NVLink，机间InfiniBand，带宽能达到\u0026gt;40GB/s。  NCCL P2P 的几种 Level:\n LOC：从不使用P2P（始终禁用） NVL：当 GPU 通过 NVLink 连接时使用 P2P PIX：当 GPU 位于同一 PCI 交换机上时使用 P2P。 PXB：当 GPU 通过 PCI 交换机（可能是多跳）连接时使用 P2P。 PHB：当 GPU 位于同一 NUMA 节点上时使用 P2P。 流量将通过 CPU。 SYS：在 NUMA 节点之间使用 P2P，可能跨越 SMP 互连（例如：QPI/UPI）。  Storage Operation and Maintenance Failure Hardware Exception：\n GPU ECC Error：通常需要重启机器或 Reset GPU。128 个节点每 1-2 天就有一个。 IB（Infiniband）/NCCL问题：通常是硬件问题，比如网卡坏了，或者网络抖动等。   由于网络问题导致训练降速，比如降速 20% 左右，需要通过二分法来判断异常的节点进行相应替换。 也可能导致任务直接异常，比如 ：p2p_plugin.c:141 NCCL WARN NET/IB : Got async event : port error。  任务 Hang 住：通常和 IB/NCCL 问题相关，也需要人工检测。OPT-175B 训练中多次出现任务 Hang 住的情况。 GPU 掉卡：此时一般会触发 CUDA Error 或程序异常退出，比如：“RuntimeError: Caught RuntimeError in pin memory thread for device 4.”。 机器异常：GPU 之外的硬件异常，比如硬盘、CPU 等异常，甚至机器直接挂掉。 机器配置异常：比如发现某个机器开启了 MIG。 集群维护：通常一个集群里不仅仅是支持一个任务，偶尔会需要维护升级，可能需要停止任务。 存储挂掉：比如 Lustre 存储挂掉  Monitoring \u0026amp; Fault Tolerance 需要自动故障检测和快速恢复机制：\n 当用户提交训练任务后，除了在每个 GPU 上创建一个训练 Executor 外，还会创建一个训练守护进程，定期向 Driver 发送心跳。心跳信号包含各种信息，以实现实时的异常检测和发送告警信息。Driver 在特定时间内未收到心跳信号时，也会触发故障恢复流程，包括：   暂停所有训练的 Executor，并执行一系列的自检诊断。 一旦识别到异常机器，将其驱逐，并将通过测试的同等数量的健康机器加入集群。除此之外，还提供了用户接口，允许用户手动识别并驱逐异常机器。 机器恢复完成之后，将从最近的 Checkpoint 恢复训练。对 Checkpoint 的保存和恢复过程也需要进行了优化，以最大限度减少对训练进度的影响。引入了两阶段方案，第一阶段每个 GPU 训练进程将显存中的状态信息写入主机内存中，由于高速 PCIe 带宽，这个过程通常只要几秒钟，这样训练进程可以马上继续训练。第二阶段中，一个后台进程异步的将状态信息同步到分布式文件系统，MegaScale 使用的是 HDFS。 其心跳信号包括 IP 地址，Pod 名字，硬件信息等，还包含当前的训练进度信息。除此之外，训练进程的 stdout/stderr 日志也会被收集，以便进行实时的聚合、过滤和分析。如果识别到 warning 或 error 等关键字，Driver 将实时的上报这些诊断信息。最后，RAMA 的流量指标也会包含在内，以便更好的识别网络利用率和通信效率  日常巡检（需要精确到毫秒级的监控系统，避免 Spike）：\n 启动大型矩阵乘法任务，捕获这些故障 还会启动一些简单的通信测试，以保证网络的正常 机内测试：回环测试（Loopback test）：测量机器内所有的 RAMA 网卡到各种机内端点（包括内存、GPU）的回环带宽。会进行 full-mesh 测试，覆盖所有可能的链路组合，以便推断 PCIe 配置中潜在的链路问题。 RNIC-to-RNIC 测试：主要测试机内不同 RNIC 之间的连通性和带宽性能。以便发现 RNIC 是否符合速度规格或存在路由问题。 NCCL 测试：在机内进行 All-to-All 测试，也会在同一个 TOR 交换机的机器之间进行 All Reduce 测试，以发现潜在的硬件故障和性能问题  Training 大模型训练，这里我们主要参考 Andrej Karpathy 在 2023 年 MSBuild 的分享，分为以下四个阶段：\n Pre-training Supervised Fine-Tuning Reward Modeling Reforcement Learning  在介绍这四个阶段之前，首先普及下 Hardware 的只是，因为大模型训练需要大量的计算资源，比如 GPU，此外数据的存储和网络通信都是需要考虑的。\nHardware GPU 一提到 GPU （Graphics Processing Unit）我们自然而然将其和 NVIDIA 挂钩，这个是有一定的道理的，因为 NVIDIA 的 GPU 在深度学习领域占有很大的市场份额。不过这个市场巨大，有 AMD 以及 Brodcom 的 Application-specific Integrated Circuits (ASICs) 芯片也在不断地吞噬市场份额。\n虽然 GPU 带了个 Graph，但是在 AI 中我们更多的是使用 GPGPU (General-purpose computing on graphics processing units)。如果你不了解 GPU 的术语，推荐你看看 Modal 维护的 GPU 术语表。GPU 相对于 CPU\nNetwork Storage System Pre-training Pretaining model 是大模型训练最消耗算力和数据的阶段，这个阶段的目标是让模型学习到语言的基本规律，比如语法，语义等。这个阶段的数据量很大，通常是 TB 级别的数据，通常是几周到数月。这个阶段的训练通常是无监督的，也就是说我们不需要标注数据。这个阶段的训练通常是在大规模的 GPU 集群上进行的，比如 OpenAI 的 GPT3 数千张 V100 上训练一个月天。\n作为最消耗计算资源的预训练，我们在这个章节来分享 LLM 训练的一些技术，比如说分布式训练并行技术，训练优化技术。\nDistributed Training 参考 OpenAI 的文章 Techniques for training large neural networks , 主要分为以下四个部分：\n Data Parallelism Pipeline Parallelism Tensor Parallelism Expert Parallelism    这里不同颜色的块代表模型不同的层 虚线代表分割到不同的 GPU 箭头表示模型的前向传播以及反向传播  Data Parallelism Data Parallelism 将数据分成不同的 subset, 然后分发到不同的 GPU 上。对于每一个 GPU, 模型是完整的，所以每一个 GPU 都需要存储模型完整的参数。所以当 GPU 显存无法存储模型参数时，Data Parallelism 就不适用了，不过现在有些 Offload Memory 的技术或者 KV Cache 可以缓解这个问题。\n正是因为每个 GPU 需要完整的参数，所以当更新的时候，需要保证工作线程（一般和 GPU 保持一致）的参数保持一致。这样在工作线程上需要引入阻塞通信：\n 每个 worker 计算梯度 平均每个 worker 的梯度 每个 worker 更新参数  当参数量越大，线程数越多，这个阻塞通信的开销就越大，当然可以使用异步的方式进行通信，但是这样可能会损害学习率，得不偿失，尤其对于 LLM 模型，投入资源大，不容有失。GPU 之间的通信通过 NCCL (NVIDIA Collective Communication Library) 这个库来实现。上述通信过程通常的叫法是 AllReduce 如下图所示：\n 我们注意到如果 AllReduce 在一个单独的进程进行处理，那么这个进程需要 Gather All 以及 Scatter All, 这样单进程的通信开销和节点数以及参数量成正比。为了消除节点数以及参数量带来的阻塞等待浪费，我们可以使用 Ring AllReduce，如下图所示：\n 首先将 N 个 worker 构建一个 ring, 每个进程将数据划分成 N 个 chunk。\n 第 N 个 worker 将第 N 个 chunk 发送给第 worker[N+1]，同时从 worker[N-1] 接受 chunk。\n 第 N 个 worker 将接受到的 N-1 chunk 和自己的 N-1 chunk 进行 reduce，然后发送到 worker[N+1]。\n 重复接受，reduce，发送的过程，重复 N-1 次，这样每个 worker 获得结果的一部分。最后再进行一轮的 Send 操作而不做 reduce 操作，这样每个 worker 就获得了最终的结果。\nPipeline Parallelism (Model Parallelism) Pipeline Parallelism 将模型分成不同的 stage, 然后分发到不同的 GPU 上。对于大模型来说，比如 LLaMA 65B, 如果参数是 FP32 那么总显存需要 260GB 但是一般训练都采用半精度也就是 FP16，那么也需要 130GB 显存，目前最前进的 H200 显存也就是 80GB。目前来自 DeepSeek 团队训练的 DeepSeek V3 已经首次在超大规模模型上验证了FP8训练的可行性和有效性。但是 DeepSeek V3 是 MOE (Mixtures of Experts) 模型，参数达到 671B, 这个我们后面再聊。PP 的通信量相对比较小，因此常常会放在不同的机器上。\n  模型每一层的输入和输出都有顺序，F 代表 Forward, B 代表 Backward, U 代表 Update 每个 worker (GPU) 在同一时间只负责处理一个 stage 时间序列上，资源利用出现大量的气泡  为了优化这个问题，我们可以将刚才 ring allreduce 的思想拿过来，也就是将一个 batch 数据进行切分，将计算时间和等待时间 overlap，如下图所示：\n 上图为 GPipe 的示意图，GPipe 将模型 layer 的 chunk 数据的激活值连续传递给下一个 worker，也连续进行向后传递。然后同步聚合梯度，更新参数。  而 PipeDream 则让 worker 交替处理向前和向后传递。相对 GPiPe，PipeDream 有更好的资源利用率，但是需要更多的通信开销。\nTensor Parallelism Tensor Parallelism 将模型的 operator 分成不同的 subset, 然后分发到不同的 GPU 上, 比如说矩阵乘法。Pipeline parallelism 是将模型的层分到不同的 GPU, 而 Tensor Parallelism 是将模型层内的 operator 分到不同的 GPU。对于现代模型比如 Transformer, 将激活值和大的权重进行点积计算是计算的瓶颈。比如 MegatronLM 在 Transformer 的Self-Attention 和 MLP 层进行了并行化矩阵乘法。PTD-P 使用tensor，pipeline，以及data parallelism， pipeline scheduler 为每个设备分配多个非连续层，以网络通信为代价减少气泡开销。其通信量更大，因此往往会将 TP 放在单个机器内部，可以充分利用机内的 NVLink 高速带宽。\n下图是采用的是 8DP 12PP 4TP 方案，因此需要的 GPU 为 8x12x4=384：\n Expert Parallelism Mixture-of-Experts (MoE) 是一种将模型每一层的某些部分放在同一个 GPU 上执行，这意味着你可以通过门控制器控制模型中哪些部分会应用到本次输入和输出。每一组门控制的一组参数都是一个 Expert，不同的 Expert 在不同的 GPU 上。\n 很明显 MoE 架构可以显著的提升模型的大小，这来源于一个经验，在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。MoE 是基于 Transformer 架构的，由下面两个部分组成：\n 稀疏 MoE 层：MoE 层有若干个专家，如下图为 8 个专家，这些专家可以是 FFN 层，也可以是 MoE 层，像俄罗斯套娃。 门控网络或路由：这个路由决定 token 会被发送到哪个专家，路由器的参数会在训练中进行训练。   上面讲到 MoE 的优点，但是 MoE 也有一些缺点：\n 微调过程中泛化能力不足，容易过拟合 推理过程中需要加载所有的参数到内存，即使只有一个专家被激活，比如 Mixtral 8x7B ，VRAM 需要放的下 47B 的参数，这里不是 56B 是因为 MoE 只有 FFN 是独立的，其他层会共享参数。 Token 被路由到不同的专家，导致批量大小分布不均，导致计算资源浪费。  对于门控网络，除了最典型的带有 Softmax 的网络，还有其他网络，比如包括带噪声的 TopK 门控 (Noisy Top-K Gating)，引入可调节的随机噪声并保留前 k 值。\n 随机噪声是为了让专家间的负载更加均衡，在训练过程中才避免只有受欢迎的专家得到训练，在 transformers 库中，可以通过 aux_loss 参数来控制辅助损失。 topk 可以选择性的保留前 k 个值，加速训练和推理  列举两个典型的 MoE 模型或者说方法：\n Gshard   为了保证负载平衡和训练效率，Gshard 除了使用上述的辅助损失外，还进行了以下的优化：  随机路由: 在 Top-2 设置中，我们始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。 定义一个专家能处理多少令牌。如果两个专家的容量都达到上限，令牌就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。  Switch Transformer  Switch Transformer 使用接受两个输入，每个输入4个专家，而并非单输入，至少两个专家的设定。这样做的优势是：\n 减少门控网络 (路由) 计算负担 每个专家的输入 batch 至少可以减半 降低通信成本，保持模型质量  于此同时 Switch Transformer 也对专家容量进行了研究，通过大于 1 的容量因子为 token分配不均匀时提供缓冲。提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。 在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。 Switch Transformer 还探索了混合精度，也就是专家使用原始精度，其他部分使用 Bfloat16 精度，在保证训练稳定性的同时，增加了训练速度。\nST-MoE 的研究者们发现，编码器中不同的专家倾向于专注于特定类型的令牌或浅层概念。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。\n训练 MoE 模型有哪些 tricks:\n 稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能； MoE 模型在小任务比如 SuperGLUE 容易过拟合，不如在大任务比如 TriviaQA 效果好； 冻结 MoE 参数进行微调，可以显著加速并且对模型效果影响很小； MoE 适合更小的 batch 和更高的学习率； 在稀疏模型上进行微调效果好用稠密模型。  对于稀疏模型和稠密模型如何选择呢？\n 稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。   我们刚才在讲分布式训练的优化手段，讲到了 Expert Parallelism，从而引申到 MoE 模型，那么为什么 MoE 模型需要 Expert Parallelism 呢？\n 在 Expert Parallelism 中，专家被放置在不同的设备上，每个设备处理不同批次的训练样本； 当单个专家参数可以在一个设备上时候，采用数据并行和专家并行的方式训练 如果单个专家模型不能再一个设备上，需要加入模型并行，将单个专家模型分布到不同的设备上，同时配合数据进行并行训练加速  部署专家模型优化方式：\n 对专家模型进行蒸馏 路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。 合并各个专家的权重，在推理时减少了所需的参数数量  Zero-DP Sequence Parallel Context Parallelism Prepare Data LLaMA 做 Pre-training 时的训练数据如下所示：\n 收集完原始数据后，需要进行数据处理，比如 Tokenization。\n Model 我们看下 2020 年 GPT3 以及 2023 年 LLaMA 模型的一些参数对比：\n 解释下上图中的参数：\n Vocabulary Size: 50257。这是指的是 Tokenizer 中词汇表的数量，也就是输入向量的维度。 Context Length: 2048。这是指的是生成模型在生成下一个 token 之前会看到的窗口的大小。 虽然 GPT3 训练的参数量是 175B，而 LLaMA 只有65B 但是 LLaMA 训练的 token 1.5T \u0026gt; GPT3 300B。 一些训练的超参数，比如 batch size, learning rate, 头数量以及模型层数等。  训练的输入如果是以下文本, 这里会由特殊 token \u0026lt;|endoftext|\u0026gt; 进行标记\n Row 1: Here is an example document 1 showing some tokens. Row 2: Example document 2\u0026lt;|endoftext|\u0026gt; Example document 3\u0026lt;|endoftext|\u0026gt; Example document Row 3: This is some random text just for example\u0026lt;|endoftext|\u0026gt; This Row 4: 1,2,3,4,5  Tokenization 后的结果如下：\n 这里 Batch 取 4，Context Length 取 10。其中每个单元格只能看见同行的单元格，绿色的是当前 highlight 的 token 所能看到的 context, 红色是它的目标。\nTraining Supervised Fine-Tuning 在这个阶段需要小的高质量的数据，一般是人类标注的数据，比如 prompt 以及相应理想的回复， 一般需要 10~100k。\n这一阶段会将 Pre-training 的模型加载进来，然后在这个数据集上进行训练，得到的数据就是 SFT(Supervised Fine-Tuning) 模型。目前进行 SFT 的模型有很多，比如 Instruct, Coder, Math 以及 Reasoning 分别针对指令性的 QA、编程、数学以及推理任务场景。\nReward Modeling RLHF(Reward Learning from Human Feedback) 阶段分为两个部分，一个是 Reward Model，一个是 Reinforcement Learning。\nReward Model 会将数据收集变成比较的形式，举个例子\n 这里，人类需要输入相同的指令，在不同的输出中进行 Rank，得到 pair 数据集，大概 100k~1M。\n训练中:\n  每一行的蓝色 prompt 是一样的 黄色是 SFT 模型输出 绿色是 reward token，也就是 SFT 评价模型输出的质量，和人类评价的质量进行比较 损失函数衡量 reward token 与人类标记的 ground truth 的一致性  Reinforcement Learning 准备 10k~100k 的 Prompt 数据, Reward Model 对这些模型进行训练, 100 GPUs天级别。\n  每一行的蓝色 prompt 是一样的 黄色是 SFT 模型输出, 作为初始化值，作为 Reward Model 的训练数据 绿色是 reward token，这个 token 会将 sampled token, 也就是黄色部分进行评价，如果高，则黄色部分 token 在后续的采样中会被采样的概率会增加。  PPO 算法就是 RLHF 模型, 为什么要使用 RLHF, 参见下图，RLHF 可以显著减少 predict 的熵，也就是预测的更加稳定。\n 参考知乎姜富春​博士总结的 聊聊Reasoning Model的精巧实现（ReFT, Kimi K1.5, DeepSeek R1） 介绍了 ReFT, Kimi K1.5, DeepSeek R1 这三个 Reasoning Model 的实现,其中 RL 是关键。\nRLHF 可以参考 OpenRLHF.\nServing Model Optimization 模型这块本来想放在 Model 里面，但是这里的模型优化主要是让推理更快，所以放在 Serving 里面。\nMHA vs MQA vs GQA vs MLA 这里首先解释下这几个名词：\n MHA: Multi-Head Attention MQA: Multi-Query Attention GQA: Global Query Attention MLA: Multi-Latent Attention  参看苏大的博客 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA。\nMHA 在 Transformer 解码器中，由于 token 的注意力依赖于前面的 token，因此不会重新计算前面的上下文，而是缓存其 Key 和 Value, 为什么没有缓存 Query 呢？\n 如下面这个 不带 Causal Mask 的 Attention 公式，从上到下，在 T 阶段，只有 Q 参与了每一个地方的计算，但是 K 和 V 是已经算好的，不需要再次计算。但是引入 KV Cache 后虽然计算量减少了，但是常驻内存占用增加了，而且随着 Context 长度的增长，对显存的占用也越大，单卡可能无法放得下。如果拓展到多卡或者多机，那么将会引入通信带宽的瓶颈限制。所以这一章节就是为了优化这个 KV Cache 的问题。\nMQA MQA（Multi-Query Attention） 思想比较简单，就是所有 header 的 Q 共用一个 K V，所以 KV Cache 就减少到原来的 1/h。其缺点就是对模型性能会带来一定的影响。\nGQA GQA（Grouped-Query Attention）为了缓解 MQA 对模型性能带来的影响，讲 h 个 Head 划分为 g 个 Group, 这样 KV Cache 就压缩原先的 h/g 倍。llama2/3-70B中，GQA的g=8，h=32。这里的 g=8 是考虑到单机 8 卡，这样在做计算时尽可能保证K、V多样性的同时最大程度上减少卡间通信。\nMLA MLA（Multi-Latent Attention）很多的数学概念，对于我有点复杂了，可以参看上面苏大的博客。我总结下思路吧：\n 在 GQA 的基础上，将 K, V 拼在一起看做由输入和一个参数矩阵的乘积结果[K,V] = x.W 通过不同的投影矩阵来增强了GQA的能力，并且推理时可以保持同样大小的KV Cache 那么反过来，如果我们只需要跟GQA相近的能力，那么 dimension W 取更小的值（DeepSeek-V2取了512），从而进一步压缩KV Cache。  Serving Pattern 随着 Chat，Code Assistant 等 LLM 应用的蓬勃发展，LLM Serving 由单卡逐步拓展到多卡，甚至是多个实例。由此衍生出 Orchestration Pattern, 例如 prefill-decode disaggregation, context cache migration, traffic-adaptive request routing 等。\nPrefill-Decode Disaggregation 为什么要将 P(Prefill) 以及 D(Decode) 分开呢？参考prefill 和 decode 该分离到不同的卡上么？ - Chayenne Zhao的文章 - 知乎\nP 和 D 分离的本质还是 P 和 D 阶段的计算模型不同，P 阶段是 Compute bound，D 阶段是 Memory bound。\n Compute bound: 输入所有 tokens Attention 的计算，增大 batch 会被这个所限制，没有更多算力可算。 Memory bound: 每一个 token 生成都需要 KV Cache 从显存中频繁的读取，增大 batch 会被这个所限制。  优势：\n 显著的降低了显存的；P 阶段计算时中间激活值参数保留在显存中，而 D 阶段只需要 P 阶段的 KV Cache 即可，如果 P 和 D 不分离，在 D 阶段激活值占据的显存就是浪费，在模型较大以及 context 较长的情况下，显存的浪费会更加明显。 提高吞吐量：多卡系统上分离 prefill 卡和 decode 卡可以实现更高效的 pineline 并行，提高吞吐量。  缺点：\n 通讯开销：跨卡传输 KV cache 无疑会带来新的通讯压力。 隐藏的开销：为了加强通讯，networking 硬件的成本不可忽略。此外，大规模的 data center 部署很多不同机型造成的 fragmentation 也有不小代价。  P/D 分离带来了架构上的创新，例如：\n  MoonCake   在满足 SLO(TTFT（time to first token），TBT(token between token)) 情况下最大化 KV Cache 的缓存命中以及最大化 MFU（Model FLOPs Utilization）。因为复用 KV Cache 涉及到远程通信，例如从其他介质（CPU,DRAM,SSD）中读取到显存，所以会增大 TTFT；而更大的batch 增加 MFU 的同时也会增加 TBT 的时间。 Mooncake 设计了 global scheduler（conductor）。对于每个 request，Conductor 需要为其选择一组用于 prefiil 和 decode 的设备，然后进行调度。首先迁移尽可能多的 KV cache 到 prefill 设备上；接着在 prefill 设备上通过 chunked and layer-wise prefill 的方法连续地 stream prefill 所得的 KV cache 到 decode 设备上；最后在 decode 设备上加载 KV cache，将此 request 加入到 continuous batching 中，完成 decode。 Mooncake 将 GPU 集群内的 CPU DRAM SSD RDMA 资源分别建立了资源池。 在 GPU 和 GPU 之间转移 KV cache 由单独的 GPU direct RDMA 设备完成，被称为 messager。原文中基于哈希的前缀存储能够为上层用户提供 context caching API。 如果某个请求中未被 cache 的 tokens 数目超过了一个特定的 threshold（prefill_chunk）。这个请求将会被拆为多个 chunks，然后流水执行。通常而言，prefill_chunk 的大小大于 1k。 在 prefill 阶段进行预估而没有被拒绝的请求可能在 decode 阶段出于 SLO 的缘故而被拒绝，这也是一个显著的 staleness 问题。  chunked prefill 可以参考 基于 chunked prefill 理解 prefill 和 decode 的计算特性 - Chayenne Zhao的文章 - 知乎.\nMLC 的 MicroServing API 通过细粒度的 API编排推理引擎实现：\n Data Parallel Prefill-Decode Disaggregation Balanced prefill-decode disaggregation： prefill 和 decode 工作负载可能会不平衡。当处理长 prompts 时，prefill引擎可能会被过度利用，而decode引擎则以低利用率运行，甚至保持空闲状态。将部分prefill计算动态卸载到decode引擎中。为了实现这一点，路由器需要决定“decode_start”（decode引擎开始prefill的位置）并将其传递给所有 API.  Context Cache Migration 在为 QA 工作负载提供服务时，开发人员倾向于将不同类别的上下文缓存放入不同的引擎中，并根据其匹配的上下文类别来调度传入流量。考虑有多个引擎，其中一些专门用于历史背景，另一些则专门用于科学背景。如果科学请求多于历史请求，我们可能希望通过上下文迁移将一些历史引擎切换到科学引擎，反之亦然。上面的 MicroServing 细粒度控制推理引擎 API,在引擎之间实现高效的 KV 传输，而无需中断服务。\nTraffic-Adaptive Request Routing 根据工作负载特征动态重新配置编排模式，而无需更改底层引擎。可编程路由器允许不同编排策略之间的无缝切换：\n 当prefill：decode比率（prefill输入token的时间/decode所有输出token的总时间）增加时，可以通过balanced prefill-decode disaggregation 将更多prefill计算转移到 decode引擎。 如果大部分 prompt 数据在 decode 引擎的Context Cache中找到，系统可以完全绕过prefill引擎，直接调用start_generate来处理 prompt 的非 cache 部分。  Serving Optimization  KV Cache Paged Attention Quantization Speculate Decode Constrainted Decoding Chunked prefill Prompt Cache KV Compression  KV Cache 参看大模型推理加速：看图学KV Cache - 看图学的文章 - 知乎。\n大模型推理性能优化的一个常用技术是KV Cache，该技术可以在不影响任何计算精度的前提下，通过空间换时间思想，提高推理性能。具体可以参考👆的参考，总结下：\n Transformer Encode 阶段是自回归的 自回归下 Attention 的第 k 次计算只依赖于第 k 个 Q,其他部分是重复计算，可以通过 KV Cache 来缓存  Paged Attention 参看 图解大模型计算加速系列之：vLLM核心技术PagedAttention原理 - 猛猿的文章 - 知乎。\nPaged Attention 主要解决直接给输入请求预分配显存导致的显存利用率低的问题；通过借鉴现代操作系统的内存分页技术，提高推理的显存利用率。\n 如上图所示：\n Request A 是模型请求，相当于操作系统中的进程 Logical KV Blocks 相当于操作系统中的虚拟内存页，每个 Block 有固定大小，在vLLM中默认大小为16，即可装16个token的K/V值 Block Table 相当于操作系统中的页表，记录了每个 Logical Block 在物理内存中的位置 Physical KV blocks 相当于操作系统中的物理内存页，存储真实的K/V值，这里是存在 vRAM 中的.  Paged Attention 在不同场景是如何工作的：\n 场景  Parallel Sampling: 我给模型发送一个请求，希望它对prompt做续写，并给出三种不同的回答。我们管这个场景叫parallel sampling。 Beam Search: 即在每个decode阶段，我不是只产生1个token，而是产生top k个token（这里k也被称为束宽）。top k个token必然对应着此刻的top k个序列。我把这top k个序列喂给模型，假设词表的大小为|V|，那么在下一时刻，我就要在k*|V|个候选者中再选出top k，以此类推。不难想象每一时刻我把top k序列喂给模型时，它们的前置token中有大量的KV cache是重复的。   vLLM 如何调度：  Parallel Sampling: 每个请求都会被分配一个Logical Block，这个Block会被映射到vRAM中的一个Physical Block。对于 Parallel Sampling，每个请求的Logical Block都是独立的，但是 Physical Block 是共享的，通过 ref count 来判断是否需要释放,而不是再独立分配 Physical Block。在 Decode 阶段，通过 Copy-on-Write 的方式，将新生成 token 的 KV Block 复制到新的 Physical Block 中，同时新生成 token 前面 token 所在的 Block ref count -1。 Beam Search: 根据最新时刻的beam search decoding结果，释放掉不再被需要的逻辑块和对应的物理内存空间，达到节省显存目的，具体过程可参考上面知乎的文章。    Speculate Decoding 参看佳瑞大佬的这篇文章 大模型推理妙招—投机采样（Speculative Decoding） - 方佳瑞的文章 - 知乎。\n投机采样这个专业名称比较拗口，应该叫推理采样；这个技术的核心是解码过程中，某些token的解码相对容易，某些token的解码则很困难。因此，简单的token生成可以交给小型模型处理，而困难的token则交给大型模型处理。这里的小型模型可以采用与原始模型相同的结构，但参数更少，或者干脆使用n-gram模型。小型模型不仅计算量较小，更重要的是小模型生成，大模型校正可以减少了内存访问的需求。\nConstrainted Decoding Chunked prefill Prompt Cache KV Compression Quantization DeepSeek V3 在最后，我们来分析下国产的 LLM 模型 DeepSeek V3，这个模型是由 DeepSeek 团队开发的，通过分析和我们上面的技术做对应。 目前 DeepSeek-V3 是在 14.8T token 上进行训练的 MoE 架构，总参数为 671B，每个 token 激活 38B。 完整训练在 H800 上用时 2.788M hours。\nArchitecture  如上图所示，DeepSeek-V3 采用了 MoE 架构，其中：\n 原始 Transformer 中的 Decode 部分的 FFN 层被替换成了 MoE 层，这里使用的是 DeepSeekMoE； 原始 Transformer 架构中的 Multi-Head Attention 层被替换成了 Multi-Latent Attention 层。  DeepSeekMoE 参看 DeepSeekMoE论文阅读，这个架构主要是让专家是真正的专家（具有非重叠且集中的知识），避免专家间的知识杂交和知识冗余,在更少的资源占用情况下获得更好的性能。\n DeepSeekMoE引入了两项核心策略：\n 细分专家为更小单位，并从中激活更多组合，允许更灵活地选择激活的专家。  这个策略的直觉是如果 token 可分配的专家越少，那么这些专家就很难学到有差异化的知识 为了在增加 token 可分配专家的情况下不增加计算量，将激活专家拓展 N 倍的同时，将 FFN 的隐层维度缩减为 1/N   隔离部分专家作为共享专家，以捕捉通用知识并减少路由专家间的冗余  有专门的共享专家来捕捉和整合不同上下文中的通用知识，则可以减轻其他路由专家之间的参数冗余。这种冗余的减少将有助于构建一个参数效率更高、专家更加专业的模型。 这里路由器将对共享专家不起作用，而是直接将 token 分配给共享专家。 共享专家从总专家数剥离，也就是细分专家=总专家-共享专家    Multi-Latent Attention 该方法来自于 DeepSeek-V2，参见 MHA vs MQA vs GQA vs MLA。\nAuxiliary-loss-free strategy for load balancing 辅助损失通过引入随机噪声让专家避免在训练过程中出现强者恒强的问题，但是也会损害模型的性能。而 Auxiliary-loss-free strategy 就是为了在负载平衡和模型性能之间实现更好的权衡。 具体做法：\n 引入 bias 在训练中发现哪些专家过载，则减少 𝛾（bias 更新速度的超参数） ,如果专家负载不足，增加 𝛾  但为了防止任何单个序列内的极端不平衡，还采用 Complementary Sequence-Wise Auxiliary Loss， 鼓励每个序列上的专家负载达到平衡，具体算法公式可以参看论文。\nNode-Limited Routing 使用限制路由机制来限制训练期间的通信成本。确保每个代币将被发送到最多 𝑀 个节点，这些节点是根据分布在每个节点上的专家的最高 affinity score 之和来选择的。在此约束下，MoE 训练框架几乎可以实现 full computation-communication overlap, 也就是计算和通信可以同时进行不互相阻塞。\nNo Token-Dropping DeepSeek-V3 在训练期间不会丢弃任何标记。此外，使用特定的部署策略来保证推理负载平衡，因此DeepSeek-V3在推理过程中也不会丢失令牌。\nMulti-Token Prediction  参考 知乎。\nInfrastructure 集群状态  256节点，一个节点8张卡，一共 2048 个 NVIDIA H800 GPU 卡和卡之间通过 NVLink 和 NVSwitch 进行连接，不同的节点间通过 InfiniBand 进行连接  训练框架 HAI-LLM：\n 8 个节点的 16 路 Pipeline Parallelism   DualPipe 重叠了前向和后向过程的计算和通信阶段，从而解决了跨节点专家并行带来的繁重通信开销的挑战  64 路 Expert Parallelism   cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths.  ZeRO-1 Data Parallelism   精心优化了内存占用，从而使我们能够在不使用昂贵的张量并行（TP）的情况下训练   根据上图：\n DualPipe 显著减小了气泡 DualPipe 将峰值激活内存增加了数倍，PP \u0026lt; 1 尽管 DualPipe 需要保留模型参数的两个副本，但这并不会显着增加内存消耗，因为在训练期间使用了较大的 EP 大小 气泡和激活内存都不会因为 mini-batch 增加而增加  通信 Cross-Node All-to-All Communication:\n cross-node GPUs are fully interconnected with IB (50GB/s) intra-node communications are handled via NVLink (160GB/s) 如何有效利用 NVLink 和 IB 的不同带宽   限制每个 token 最多调度到 4 个节点，从而减少 IB 流量 每个 token 当做出路由决策时，它将首先通过IB传输到其目标节点上具有相同节点内索引的GPU。一旦它到达目标节点，我们将努力确保它通过 NVLink 立即转发到托管目标专家的特定 GPU，而不会被随后到达的令牌 block 通过 IB 和 NVLink 的通信完全重叠，每个 token 可以有效地为每个节点平均选择 3.2 名专家，而不会产生 NVLink 的额外开销。这意味着，虽然 DeepSeek-V3 目前选择8个专家，但是可以拓展到 3.2x4 个。 使用 warp specialization technique 将 Streaming Multiprocessor (SM) 分拆到 10 个 communication channels。  Dispatching 阶段：(1) IB 发送、(2) IB 到 NVLink 转发和 (3) NVLink 接收由各自的 warp 处理。分配给每个通信任务的warp数量根据所有SM的实际工作负载动态调整。 Combining 阶段：(1) NVLink 发送、(2) NVLink 到 IB forwarding和accumulation、以及 (3) IB receiving和accumulation也由动态调整的 warp 处理。 dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs。    Memory Optimization Extremely Memory Saving with Minimal Overhead：\n Recomputation of RMSNorm and MLA Up-Projection 在反向传播的时候，消除了持续存储它们输出激活的需求。通过轻微的额外开销，这种策略显著降低了存储激活所需的内存需求。 Exponential Moving Average in CPU：在训练过程中，我们保留模型参数的指数移动平均（EMA）以在学习率衰减后早期估计模型性能。EMA参数存储在CPU内存中，并在每次训练步骤后异步更新。这种方法使我们能够保持EMA参数，而不会产生额外的内存或时间开销。 Shared Embedding and Output Head for Multi-Token Prediction：采用DualPipe策略，将模型的最浅层（包括嵌入层）和最深层（包括输出头）部署在同一个PP rank上。这种安排使得MTP模块和主模型之间可以物理共享参数和梯度，共享嵌入和输出头。这种物理共享机制进一步提高了我们的内存效率。  FP8 Mixed Precision Training：\n  使用 1 × 𝑁元素进行分块分组或使用 𝑁× 𝑁元素进行块分组。在提高精度的累加过程中，相关的反量化开销在很大程度上得到了缓解 为了进一步减少 MoE 训练中的内存和通信开销，在 FP8 中缓存和调度激活，同时在 BF16 中存储低精度优化器状态。 大部分核心计算内核，即GEMM运算，都是以FP8精度实现的。这些 GEMM 运算接受 FP8 张量作为输入并产生 BF16 或 FP32 格式的输出。如上图所示，与线性算子相关的所有三个 GEMM，即 Fprop（前向传递）、Dgrad（激活后向传递）和 Wgrad（权重后向传递）均在 FP8 中执行。该设计与原始BF16方法相比，理论上计算速度提高了一倍。此外，FP8 Wgrad GEMM 允许将激活存储在 FP8 中，以便在向后传递中使用。这显着减少了内存消耗 保持以下组件的原始精度（例如 BF16 或 FP32）：嵌入模块、输出头、MoE 门控模块、归一化算子和注意力算子。    细粒度的量化方法，该方法在更细粒度的级别上应用缩放。如上图所示，(1) 对于激活，我们在 1x128 图块基础上对元素进行分组和缩放（即每 128 个通道的每个标记）； (2) 对于权重，我们在 128x128 块的基础上对元素进行分组和缩放（即每 128 个输入通道每 128 个输出通道）。这种方法确保量化过程可以通过根据较小的元素组调整比例来更好地适应异常值。 沿着 GEMM 操作内部维度引入缩放因子 microscaling formats with smaller quantization granularity will be supported in Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) Underflow issues：在 Tensor Core 上执行 MMA（矩阵乘法累加）期间，使用有限的位宽来累加中间结果。一旦达到间隔 𝑁，这些部分结果将被复制到 CUDA 内核上的 FP32 寄存器，在其中执行全精度 FP32 累加。如前所述，我们的细粒度量化沿内部维度 K 应用每组缩放因子。这些缩放因子可以在 CUDA 核心上有效地相乘，作为反量化过程，且附加计算成本最小。 值得注意的是，这一修改降低了单个warpgroup的WGMMA（Warpgroup级矩阵乘法累加）指令发出率。然而，在 H800 架构上，两个 WGMMA 并发持续是很典型的：当一个 warpgroup 执行提升操作时，另一个 Warpgroup 能够执行 MMA 操作。这种设计可以实现两个操作的重叠，从而保持 Tensor Core 的高利用率。根据实验，设置 𝑁= 128 个元素，相当于 4 个 WGMMA，代表最小累积间隔，可以显着提高精度，而不会引入大量开销。  Low-Precision Storage and Communication:\n 将缓存的激活和优化器状态压缩为较低精度的格式，进一步减少了内存消耗和通信开销。低精度优化器状态。我们采用 BF16 数据格式而不是 FP32 来跟踪 AdamW（Loshchilov 和 Hutter，2017）优化器中的第一和第二时刻，而不会导致可观察到的性能下降。然而，主权重（由优化器存储）和梯度（用于批量大小累积）仍保留在 FP32 中，以确保整个训练过程中的数值稳定性。 低精度激活。Wgrad 操作在 FP8 中执行。为了减少内存消耗，自然选择以 FP8 格式缓存激活以用于线性算子的向后传递。但为了低成本高精度训练，对几个算子进行了特殊考虑：   注意力算子之后的线性输入。这些激活也用于注意算子的向后传递，这使得它对精度敏感。专门为这些激活采用定制的 E5M6 数据格式。此外，这些激活将在后向传递中从 1x128 量化图块转换为 128x1 图块。为了避免引入额外的量化误差，所有缩放因子均进行舍入缩放，即积分幂为 2。 MoE 中 SwiGLU 算子的输入。为了进一步降低内存成本，我们缓存 SwiGLU 运算符的输入，并在向后传递中重新计算其输出。这些激活也通过我们的细粒度量化方法存储在 FP8 中，在内存效率和计算精度之间取得平衡。  低精度通信   在 MoE 上投影到 FP8 之前对激活进行量化，然后应用调度组件，该组件与 MoE 上投影中的 FP8 Fprop 兼容。与注意力算子之后的 Linear 输入一样，此激活的缩放因子是 2 的整数幂。类似的策略应用于 MoE 下投影之前的激活梯度。对于前向和后向组合组件，我们将它们保留在 BF16 中，以保持训练管道关键部分的训练精度。  Inference and Deployment 在H800集群上部署DeepSeek-V3，每个节点内的GPU通过NVLink互连，集群中的所有GPU通过IB完全互连。为了同时确保在线服务的服务级别目标（SLO）和高吞吐量，采用以下部署策略将 prefilling 和 decoding 阶段分开：\n Prefilling 阶段：   预填充阶段的最小部署单元由4个节点、32个GPU组成。注意力部分采用 4 路张量并行（TP4）和序列并行（SP），并结合 8 路数据并行（DP8）。其较小的 TP 大小为 4 限制了 TP 通信的开销。对于MoE部分，我们使用32路专家并行（EP32），这确保每个专家处理足够大的批量大小，从而提高计算效率。对于 MoE 的全对全通信，我们使用与训练中相同的方法：首先通过 IB 跨节点传输令牌，然后通过 NVLink 在节点内 GPU 之间转发。特别是，我们对浅层的密集 MLP 使用 1 路张量并行，以节省 TP 通信。 为了实现MoE部分不同专家之间的负载平衡，需要确保每个GPU处理大致相同数量的 token。为此，引入了冗余专家的部署策略，即复制高负载专家并进行冗余部署。高负载专家根据在线部署过程中收集的统计数据进行检测，并定期（例如每10分钟）进行调整。确定冗余专家集后，我们根据观察到的负载仔细地重新安排节点内 GPU 之间的专家，力求尽可能平衡 GPU 之间的负载，而不增加跨节点的 all-to-all 通信开销。对于DeepSeek-V3的部署，我们为预填充阶段设置了32个冗余专家。对于每个GPU，除了其原来托管的8个专家外，还将托管一个额外的冗余专家。 此外，在预填充阶段，为了提高吞吐量并隐藏 all-to-all 和 TP 通信的开销，同时处理具有相似计算工作量的两个微批次，将一个 mini batch 的注意力和MoE与另一个微批次的调度和组合重叠。 最后，专家的动态冗余策略，其中每个 GPU 托管更多专家（例如 16 个专家），但在每个推理步骤中只会激活 9 个专家。在每一层的全对全操作开始之前，我们动态计算全局最优路由方案。考虑到预填充阶段涉及的大量计算，计算该路由方案的开销几乎可以忽略不计。  Decoding 阶段：   将共享专家视为路由专家。从这个角度来看，每个 token 在路由过程中都会选择9个专家，其中共享专家被认为是一个重负载的专家，总是会被选择。解码阶段的最小部署单元由40个节点、320个GPU组成。 Attention部分采用TP4+SP，结合DP80，MoE部分采用EP320。对于MoE部分，每个GPU仅托管一名专家，64个GPU负责托管冗余专家和共享专家。调度和组合部分的全对全通信是通过 IB 上的直接点对点传输来执行的，以实现低延迟。此外，我们利用IBGDA（nfiniBand GPUDirect Accelerator）（NVIDIA，2022）技术进一步减少延迟并提高通信效率。 与预填充类似，根据在线服务的统计专家负载，定期确定一定时间间隔内的冗余专家集。但是，不需要重新排列专家，因为每个 GPU 仅托管一名专家。 为了提高吞吐量并隐藏所有通信的开销，在探索在解码阶段同时处理具有相似计算工作负载的两个微批次。与prefilling不同，注意力在解码阶段消耗了大部分时间。因此，我们将一个微批次的注意力与另一个微批次的调度+MoE+组合重叠。在解码阶段，每个专家的批量大小相对较小（通常在 256 个token以内），瓶颈是内存访问而不是计算。由于MoE部分只需要加载一名专家的参数，因此内存访问开销很小，因此使用较少的SM不会显着影响整体性能。因此，为了避免影响注意力部分的计算速度，我们可以只分配一小部分SM来dispatch+MoE+combine。  硬件建议 Communication Hardware：\n 未来的供应商开发出能够从计算单元 SM 中卸载这些通信任务的硬件，充当 GPU 协处理器或网络协处理器。 为了降低应用程序编程的复杂性，从计算单元的角度统一IB（横向扩展）和NVLink（纵向扩展）网络。有了这个统一的接口，计算单元可以通过提交基于简单原语的通信请求，轻松完成整个 IB-NVLink 统一域的read, write, multicast, and reduce 等操作。  Compute Hardware：\n 建议未来的芯片设计提高Tensor Core中的累加精度以支持全精度累加，或者根据训练和推理算法的精度要求选择合适的累加位宽。这种方法确保错误保持在可接受的范围内，同时保持计算效率。 当前 GPU 仅支持 per-tensor 量化，缺乏对细粒度量化（如分块量化和块量化）的本机支持。在当前实现中，当达到𝑁间隔时，部分结果将从 Tensor Core 复制到 CUDA 核心，乘以缩放因子，然后添加到 CUDA 核心上的 FP32 寄存器。尽管结合我们精确的 FP32 累积策略，显着减轻了反量化开销，但 Tensor Core 和 CUDA 核心之间频繁的数据移动仍然限制了计算效率。因此，建议未来的芯片通过使 Tensor Core 能够接收缩放因子并通过组缩放实现 MMA 来支持细粒度量化。这样，整个部分和累加和反量化可以直接在Tensor Core内部完成，直到产生最终结果，避免了频繁的数据移动。 支持在线量化。尽管我们的研究证明了在线量化的有效性，但当前的实现仍难以有效支持在线量化。在现有流程中，我们需要从HBM（高带宽存储器）中读取128个BF16激活值（之前计算的输出）进行量化，然后将量化后的FP8值写回HBM，仅在MMA时再次读取。为了解决这种低效率问题，我们建议未来的芯片将 FP8 转换和 TMA（张量内存加速器）访问集成到单个融合操作中，以便可以在激活从全局内存转移到共享内存的过程中完成量化，从而避免频繁的内存读写。我们还建议支持扭曲级转换指令以加速，这进一步促进层归一化和 FP8 转换更好的融合。或者，可以采用近内存计算方法，其中计算逻辑放置在 HBM 附近。在这种情况下，BF16 元素在从 HBM 读入 GPU 时可以直接转换为 FP8，从而减少大约 50% 的片外内存访问。 支持转置 GEMM 操作。当前的架构使得将矩阵转置与 GEMM 运算融合起来很麻烦。在工作流程中，前向传递过程中的激活被量化为 1x128 FP8 块并存储。在向后传递期间，需要读出矩阵、去量化、转置、重新量化为 128x1 块，并存储在 HBM 中。为了减少内存操作，建议未来的芯片在 MMA 操作之前启用共享内存中矩阵的直接转置读取，以满足训练和推理中所需的精度。结合 FP8 格式转换和 TMA 访问的融合，这一增强功能将显着简化量化工作流程。  Pre-Training 引用  GPT3 Architecture Microserving LLM engines State of GPT Andrej Karpathy LLM Action MOE Architecture 万卡 GPU 集群实战：探索 LLM 预训练的挑战  ","date":"2025-01-09","permalink":"https://blog.xieydd.top/llm-infra/","tags":["llm","infra","model training","vllm"],"title":"LLM Infra 你需要知道的一切"},{"categories":["tech","2025","serverless","serving"],"contents":"之前在做 Serverless 模型推理 Modelz，虽然现在已经 pivot 了，但是还是想分享一下如何优化模型推理的冷启动问题。由于我们的服务是基于容器调度，所以这里也涉及到了容器的冷启动问题。\n优化模型推理的冷启动 问题 首先我们看下 Serverless 模型推理，从用户请求到模型推理的过程：\n Click me 1 2 3 4 5 6 7 8 9 10 11 12 13  sequenceDiagram participant User participant Cloudflare participant Ingress participant AutoScaler participant Node participant containerd User-\u0026gt;\u0026gt;Cloudflare: Model Call Cloudflare-\u0026gt;\u0026gt;Ingress: Request Ingress-\u0026gt;\u0026gt;AutoScaler: Request AutoScaler-\u0026gt;\u0026gt;Node: Scale Up Node-\u0026gt;\u0026gt;containerd: Container Note right of containerd: 1. Pull Image \u0026lt;br\u0026gt;2. Start Container\u0026lt;br\u0026gt;3. Download model     整个流程的链路很长，但是真正耗时的地方在最后 Containerd 拉取镜像和启动容器的过程。我们将这个部分进一步细化,这里的每个阶段的时间大致来自于引用1：\n Click me 1 2 3 4 5 6 7 8 9 10 11  flowchart TD subgraph Pod Create 3A[Pull Image 3.5GB 140s] --\u0026gt; 3B[Download Model] end subgraph GPU Node Provision 2A[VM Create 40s] --\u0026gt; 2B[Node Initialize 45s] 2B --\u0026gt; 2C[GPU Driver Install 25s] end subgraph AutoScaler 1A[HPA reaction 10s] --\u0026gt; 1B[Auto Provisioning reaction 30s] --\u0026gt; 1C[Node auto-scaling 35s] end     如果是 30G（在 AI 推理场景并不稀有） 的镜像，那么拉取时间将超过 15min, 这个时间对于用户来说是不可接受的。\n而模型下载取决于模型的大小以及模型是否已经存在于 Pod 中，这个时间也是不可控的，但是后文我们也会针对性的提出优化方案。\nDeep Dive Why image is so large?   由上面两张图可以看到\n 除了 NVIDIA Kernel Driver 以及 CUDA Lib 放在 Host 上，AI 应用程序以及框架所依赖的库都放在镜像中。 NVIDIA 的策略导致你无法大幅缩减你的镜像，你不知道哪些库会被使用，所以你只能把所有的库都放在镜像中。  我们尝试过的解决方案 1. 预热 首先我们会使用 cluster-proportional-autoscaler 根据既定规则，比如总节点为 8 的时候该类型 GPU 资源扩容到 2 个节点即使没有请求，也有预留 bubble. 然后根据 image 使用频率，使用 kube-fledged 在这些节点上创建 ImageCache，这样在真正请求的时候，镜像已经在节点上了。\n2. Cache 模型 我们开发了一个 HuggingFace 的模型缓存服务，这个服务会在模型被调用的时候，通过比对模型的 hash 值，如果模型已经存在缓存服务中，那么直接返回缓存的模型，否则下载模型到缓存服务中。\n3. GCP Image Streaming 利用 GCP Image Streaming 将自己管理的镜像或者用户自定义的镜像转换到 GCP 的 Artifact Registry 中，在节点拉取镜像时，通过网络 mount container layers 到节点上，让 containerd 误以为镜像已经在节点上。但是这个方案有几个缺点：\n 需要 GCP 的支持, vendor lock-in 用户镜像需要 proxy 转换到 GCP，这个过程会有一定的延迟 虽然 pod running 但是不是真正的完全可运行，可能会导致运行时缓慢  4. 更换镜像格式 将 OCI 镜像格式转换成 nydus 格式，并结合 lazy pulling 技术 zran, 测试下来有着数倍的提升。就是需要修改 containerd 配置，支持 nydus。 配合 Dragonfly 的 P2P 技术，可以进一步提升镜像拉取速度。\n5. 使用 JuiceFS 构建模型缓存集群 通过构建独立缓存池，将模型缓存到 JuiceFS 中。通过 JuiceFS CSI 将缓存目录挂载到容器中，如果模型已经存在 JuiceFS 中，那么直接使用，不存在则下载并直接缓存到 JuiceFS 中。这套架构主要是利用 JuiceFS Posix 以及使用对象存储的优势，无需关注缓存大小。这里需要对 JuiceFS 的参数进行调优，比如 prefetch block, buffer size 等。\n后续可能的优化   使用 GCP 的 image preloading 功能，通过secondary boot disks preload 镜像到 node 上。\n  In-class registry cache spegel。\n  Parallel Downloading in Kubelet KEP 3673。\n  Parallel Container Layer Unpacking, 这里在引用1中提到 containerd 需要实现 high IO queue depth 才能充分利用 EBS 的 throughput。\n  yetone 的方案:\n parse 了 Dockerfile，然后获得了 base image 和一系列 args、env 和 commands，并将其顺序合并起来 hash 了一下作为 s3 object key，然后在我们的 image builder job 的 pod 的 container 里起了 dind，然后在里面用 base image 起了新的 container 然后在里面执行上一步 parse 出来的 commands，执行完毕后把这个 container 的 rootfs 打成 tar 包并用 zstd 压缩然后上传到 s3\n  OCI image builder 和 containerd remote snapshotter，在 builder 侧自己构建镜像把所有 layer 只分成两个 layer ：环境（极大）和代码（极小），然后用 pzstd 和 s5cmd 流式压缩和流式上传到 s3，然后在 snapshotter 侧用 s5cmd 和 pzstd 流式下载和流式解压，直接打满了 GKE 的 disk IO，把 image 的拉取速度提升到了以前的 4 倍左右\n   Modal lazying container loading\n    Do some research on ServerlessLLM OSDI24\n  引用  https://www.youtube.com/watch?v=e6Oo2aoZPnA https://www.youtube.com/watch?v=SlkEW4C2kd4  ","date":"2025-01-08","permalink":"https://blog.xieydd.top/improve-model-serving-cold-start/","tags":["serving","cold start","inference"],"title":"优化模型推理的冷启动"},{"categories":["tech","2024","Cloud","Storage","System Design"],"contents":"All you need know about S3 AP 场景下 S3 的最佳实践 来源论文 Key Points:\n price and durability tradeoff: Cloud object storage provides the best durability guarantees while being the cheapest storage option latency: first byte latency 与 total latency 看取舍，1MB 以下对象，两者基本一致，对象越大，两者差距越大 throughput: medium in 80 ~ 90 Gbit/s 最佳请求大小：OLAP 中 8~16MiB 最佳 requests=throughput*(baseLatency+size*dataLatency)/size 对于 S3，饱和 100 Gibt/s 实例的最佳请求并发度为 ∼200–250, base latency 30 ms, data latency 20 ms/MiB  S3 Express One Zone 如何利用好 S3 Express One Zone 结合HackNews，博客 以及文章S3 Express is All You Need， 我们得出以下结论：\n S3 Express One Zone 比 S3 Standard 的存储价格高7倍，不适合数据湖场景下的“主存储”。 对于 512KiB 的对象进行 PUT， GET 操作是最省成本的 超过 50MB 的 512KB 块存储文件没有成本优势，因为延迟达到 S3 Standard 且价格无优势 One Zone 意味着如果希望数据可以跨 zone 高可用，需要跨 zone 复制，传输有一定的成本 Express 也不适合压缩层，因为压缩前对象大概率不会是 512KB 以下 作为 Standard 上的缓存层，当对象小于 645KB 是有性能收益的，但是如果超过那你就需要考虑成本和性能的权衡了 完全可以作为像 dynamodb 这类 key value NOSql 数据库的缓存层 不支持 s3 lifecycle policy 20ms 的延迟，可以作为 replication layer  ","date":"2024-09-13","permalink":"https://blog.xieydd.top/s3/","tags":["Cloud","AWS","S3","Storage","System Design"],"title":"S3"},{"categories":["tech","2024","serverless","database"],"contents":"DuckDB (In-Process OLAP)\n MotherDuck  simple data lakerhouse: DuckDB + Dagster + S3 Modern Data Stack: Meltano + DuckDB + dbt + Superset kafka streaming pipeline from Postgres to DuckDB Iceberg API + S3 + DuckDB (Parquet)\nPostgres\n Neon EDB  StarRocks (MPP OLAP)\nClickHouse (MPP OLAP)\n ClickHouse-Local  引用  https://supabase.com/blog/supabase-acquires-oriole https://neon.tech/blog/architecture-decisions-in-neon#fn-2  ","date":"2024-08-12","permalink":"https://blog.xieydd.top/serverless/","tags":["serverless","database"],"title":"Serverless Database"},{"categories":["tech","2024","Postgres"],"contents":"最近在研究 Postgres 高可用的方案，这里记录一下。\nPostgres 高可用 高可用目标 Postgres 高可用一般有两个目标：\n RPO（Recovery Point Objective）即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量。 RTO（Recovery Time Objective）即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能所需要的最短时间周期。 简单来说就是，在多长时间内恢复数据库恢复到什么状态，比如在 5min 内恢复到丢失数据不超过 30min 的状态。当然最好的情况就是 RTO \u0026lt; 30s, RPO~=0。  场景 为了达到上面的这个最好的情况，需要覆盖以下场景：\n 当 Primary 节点发生故障时，自动切换到 Standby 并在 RTO 的要求下恢复到 RPO 目标。 当数据库发生数据的意外删除、升级变更错误、或者遇到硬件故障等，可以恢复到指定时间点。  概念 为了满足以上场景需要有以下技术或者概念的支持：\n Continuous Archiving：Continuous Archiving 一般是是对 WAL(writer ahead log) 进行归档；如果遇到 db system crash 可以通过 replay WAL 来进行恢复。 Point-in-Time Recovery (PITR) ：对于硬件故障来说，基于物理复制的高可用故障切换可能会是最佳选择。而对于数据损坏（无论是机器还是人为错误），时间点恢复（PITR）则更为合适：它提供了对最坏情况的兜底。 Physical Replication：数据文件和事务日志文件全部复制 （PGData, pg_wals） Logical Replication：根据复制的标记（例如主键）在发布和订阅之间进行复制，一般不用于容灾，FDW 场景居多。 Streaming Replication ：基于 WAL 日志的流复制，主要用于容灾场景。将 WAL XLOG 记录连续从 primary 传送到 standby， 有同步以及异步两种方式。  工具 Backup and Restore 这里首先列举下常用的备份和恢复方式以及优劣：\n1. Pg_dump (Logical Backup) 逻辑备份是通过 SQL 命令 pg_dump 将数据库中的数据导出到一个文件中，然后通过 SQL 命令将数据导入到数据库中。 优势：\n 根据需要，逻辑备份可以是表级到数据库级 备份不会阻止数据库上的读/写活动 可以恢复到 PostgresSQL 的不同主要版本，甚至不同的操作系统架构中 劣势： 逻辑备份在恢复时，需要 replay ,如果数据量大，需要很长时间，而且可能会降低整体性能 不支持全局变量的 dump, 只能用 pg_dumpall  2. Physical Backup 物理备份是停止 PostgreSQL 集群后进行的 PostgreSQL 离线备份,这些备份包含整个集群数据。 优势：\n 备份和恢复速度快 适合大型数据库 适合高可用场景 劣势： 不能跨版本恢复 不能跨操作系统恢复  3. Continuous Archiving and Point-in-Time Recovery (PITR) Online Backup 或者叫 Hot Backup, 先进行完整的备份,可以在不停止 PostgreSQL 集群的情况下在线进行。增量备份生成的 WAL 日志，然后可以通过恢复 WAL 来恢复存档/WAL。 优势：\n 可以恢复到任何时间点 不会导致应用程序出现任何停机 劣势： 可能需要很长时间才能从存档中恢复数据，这些主要用于容量巨大、无法进行频繁备份的数据库。  4. Snapshots and Cloud Backups 快照需要操作系统或者 cloud provider 的支持，有rsync等工具可以用来拍摄快照。 劣势：\n 不适用于数据库将表空间存储在多个驱动器卷中的情况。  备份需要考虑很多情况，比如备份的频率、备份的存储位置、备份的恢复时间、备份保留策略等等，所以需要一些工具辅助我们来进行备份，下面列举一些常用的开源工具如下：\n pgbackrest EDB barman WAL-G 从这个讨论中，可以看到 barman 相对于 pgbackrest 还是有些功能的缺失：   Zstd 压缩 Delta restore Encryption at rest Native postgres page checksum validation Multi repo  High Availability  Patroni  Patroni 默认使用的是异步的 Streaming Replication，这意味着主节点上的事务提交后，可能会有一段时间才会被复制到备节点上。这段时间内，如果主节点发生故障，可能会丢失这段时间内的数据。为了减少这种数据丢失，可以使用同步复制，但是同步复制会影响主节点的性能，因为主节点必须等待所有备节点都已经接收到并写入 WAL 日志后才能提交事务。所以需要在可用性和性能之间做平衡。\nPatroni 的 maximum_lag_on_failover 和 pg 的 wal_segsize 的大小，需在可用性和持久性之间做平衡。\n maximum_lag_on_failover 默认 1MB(1048576 bytes) 意思是如果有个节点滞后超过这个值，就不会被选为新的主节点。一般配合 loop_wait 和 ttl 参数 一起使用。例如 ttl 是 30 的话，如果 Patroni 节点在 30 秒内未能与 Etcd 或 Consul 续约，则该节点将被认为失去了领导权。loop_wait 设置为 10 秒，Patroni 每隔 10 秒执行一次其主要操作循环，包括状态检查和必要的操作。最坏的情况下的丟失量：maximum_lag_on_failover 字节+最后的 TTL秒时间内写入的日志量。减小该值可以降低故障切换时的数据损失上限，但也会增加故障时因为从库不够健康（落后太久）而拒绝自动切换的概率。 wal_segsize 参数定义了每个 WAL 日志段文件的大小，默认是 16MB  架构 目前 Postgres 高可用架构繁多，这里列举两种常见的架构，分别对应自建 Postgres 以及云上托管 Postgres 的典型架构：\n Pigsty Cloudnative-PG HA  Pigsty HA 架构 下图来自于 pigsty: 自上而下：\n 应用层通过 DNS 解析到 vip-manager 的 VIP，vip-manager 通过 etcd 获取当前主库的 IP 地址，然后将 L2 VIP 绑定到主库所在节点；通过 HAProxy 进行 L5 层端口转发。  Patroni：同步主节点信息给到 etcd。 vip-manager：虚拟 ip 和状态由 etcd 进行同步管理。 HAProxy：根据端口分别进行路由  5433：连接 PGBouncer 连接池，连接 primary 进行 read/write 5434：连接 PGBouncer 连接池，连接 replica 进行 read-only 5436：直连 primary，管理使用 5438：直连 replica，管理使用，连接不处理在线读取流量的专用副本，用于ETL和分析查询。     primary 和 replica 通过 Streaming Replication 进行 WAL 日志的同步，primary 通过 pg_receivexlog 将 WAL 日志发送到 replica，replica 通过 pg_replay 进行 WAL 日志的重放。 Patroni 通过 pgBackRest 进行备份，备份数据可存储在本地，远程 s3 或者 minio 存储中, 可参考文档。    PostgreSQL 使⽤标准流复制搭建物理从库，主库故障时由从库接管。 Patroni 负责管理 PostgreSQL 服务器进程，处理高可用相关事宜。 Etcd 提供分布式配置存储（DCS）能力，并用于故障后的领导者选举 Patroni 依赖 Etcd 达成集群领导者共识，并对外提供健康检查接口。 HAProxy 对外暴露集群服务，并利⽤ Patroni 健康检查接口，自动分发流量至健康节点。 vip-manager 提供一个可选的二层 VIP，从 Etcd 中获取领导者信息，并将 VIP 绑定在集群主库所在节点上。 在主从架构+故障自动切换+同步 streaming replication +pgBackRest 备份的情况下 RTO 在 1min 内且 RPO 为 0，即在不丢失数据的情况下 1min 恢复。   Cloudnative-PG HA 架构 根据 Kubernetes 容器编排的特点，Cloudnative-PG HA 架构采用了更加现代化的架构：\n 多 region 部署 Kubernetes 多可用区（大于等于3）部署 PostgreSQL 节点 Primary-Standby 采用同步或者异步 Streaming Replication PostgreSQL 实例不共享资源,独占节点资源，在不同的 Kubernetes 工作节点，使用本地卷 应用层提供 rw、ro、r 三种服务，分别服务连接主节点、只读工作负载的热备用副本、任意只读工作负载、在发生故障转移时，它会自动更新服务以指向升级的服务，确保来自应用程序的流量无缝重定向。 提供 Pooler 对象，创建 PGBouncer 连接池，用于连接主节点和只读节点 通过 Replica Cluster 跨多个 Kubernetes 集群部署 PostgreSQL  通过将 PostgreSQL 备份数据存储在多个 location、region 并可能使用不同的提供商（灾难恢复）来减少全局恢复点目标 (RPO) 通过利用主 Kubernetes 集群之外的 PostgreSQL 复制来减少全局恢复时间目标 (RTO)（高可用性） 指定的主集群可以随时升级，使副本集群成为能够接受写连接的主集群。   WAL 通过 s3 进行归档 通过 barman 进行备份，可以备份到云对象存储例如 s3 或者使用 Volume Snapshot 进行备份  在上述架构下可为跨区域灾难恢复提供最多大约 5 分钟的 RPO，如果使用同步 Streaming Replication 可以达到 0 RPO, 且具备极低的 RTO。\nSupabase Backup  Click me ```mermaid graph TD; A(Supabase Backup)---\u0026gt;B(Pro); B(Pro)---\u0026gt;E(Database Size 0-40GB); B(Pro)---\u0026gt;F(Database Size 40GB+); B(Pro)---\u0026gt;G(PITR); B(Pro)---\u0026gt;H(Read Replica); E(Database Size 0-40GB)---\u0026gt;I(Logical Backup); F(Database Size 40GB+)---\u0026gt;J(Physical Backup); G(PITR)---\u0026gt;J(Physical Backup); H(Read Replica)---\u0026gt;J(Physical Backup); A(Supabase Backup)---\u0026gt;C(Team); C(Team)---\u0026gt;K(Database Size 0-40GB); C(Team)---\u0026gt;L(Database Size 40GB+); C(Team)---\u0026gt;M(PITR); C(Team)---\u0026gt;N(Read Replica); K(Database Size 0-40GB)---\u0026gt;I(Logical Backup); L(Database Size 40GB+)---\u0026gt;J(Physical Backup); M(PITR)---\u0026gt;J(Physical Backup); N(Read Replica)---\u0026gt;J(Physical Backup); A(Supabase Backup)---\u0026gt;D(Enterprise); D(Enterprise)---\u0026gt;O(Database Size 0-40GB); D(Enterprise)---\u0026gt;P(Database Size 40GB+); D(Enterprise)---\u0026gt;Q(PITR); D(Enterprise)---\u0026gt;R(Read Replica); O(Database Size 0-40GB)---\u0026gt;J(Physical Backup); P(Database Size 40GB+)---\u0026gt;J(Physical Backup); Q(PITR)---\u0026gt;J(Physical Backup); R(Read Replica)---\u0026gt;J(Physical Backup); ```     Click me ```mermaid graph TD; A(Supabase Backup)--\u0026gt;B(Pro); A(Supabase Backup)--\u0026gt;C(Team); A(Supabase Backup)--\u0026gt;D(Enterprise); B(Pro)--\u0026gt;E(Daily Backup, Retain 7 days); E--\u0026gt;H(pg_dumpall logical backup， when database size \u0026gt; 40GB will use physical backup); C(Team)--\u0026gt;F(Daily Backup, Retain 2 weeks); F--\u0026gt;H(pg_dumpall logical backup， when database size \u0026gt; 40GB will use physical backup); D(Enterprise)--\u0026gt;G(Daily Backup, Retain 1 month); D--\u0026gt;J(physical backup); ```     用户可以访问每一天生成的 logical backup 的 sql 文件进行 restore。  Click me ```mermaid graph LR; A(Supabase PITR)--\u0026gt;B(WAL-G, archiving Write Ahead Log files, default 2 min or certain file size threshold and physical backups); B--\u0026gt;C(2 minutes RPO); C--\u0026gt;D(show database restore available from and latest restore available at); ```      Click me ```mermaid graph LR; A(PGVecto.rs Cloud PITR)--\u0026gt;B(barman-cloud-wal-archive archiving Write Ahead Log files, default 5 min or certain file size threshold and barman-cloud-backup for physical backups); B--\u0026gt;C(5 minutes RPO); C--\u0026gt;D(show database restore available from and latest restore available at); D--\u0026gt;E(delete cluster will delete all wal and physical backups); ```     引用  https://pigsty.io/ https://cloudnative-pg.io/ https://www.cnblogs.com/xianghuaqiang/p/14792001.html https://docs.pgbarman.org/release/3.10.1/ https://github.com/cloudnative-pg/cloudnative-pg/discussions/3145 https://supabase.com/blog/postgresql-physical-logical-backups  ","date":"2024-07-26","permalink":"https://blog.xieydd.top/postgres-ha/","tags":["High Availability","Postgres"],"title":"Postgres 高可用"},{"categories":["tech","2023","kubernetes","documentation"],"contents":"最近在进行晋级答辩(已失败 :\u0026lt;)，主题还是去年做了一些调度方面的工作，借此机会对调度系统进行一些高维度的思考，在此记录下，如有不足之处，还望看官大人指出。\n调度的目的(什么是调度系统) 调度的目的其实是在满足资源需求的前提下，提升资源利用率、稳定性、以及性能（吞吐、延迟）等。不同层次都有调度的问题需要解决，微架构下的指令调度，OS 层面的线程调度，集群层面的任务调度等。 例如 PL(编程语言)中的内存管理系统其实也是个调度系统，C 需要手动的 malloc 申请内存，free 释放内存，手动分配带来了很多问题。比如内存分配到栈还是堆，什么时候需要释放，悬挂指针、内存泄漏等问题。为了更加有效的管理内存分配，现代编程语言都垃圾回收期以及内存分类器来自动的管理内存，比如 rust 通过 borrow checker、ownership、borrow 来管理并确保跨堆栈和堆的内存管理。\n调度器分类以及常见调度器 按照调度时机可以分为首次调度和运行时调度（也叫重调度），我们见到的大部分调度器都是首次调度。 针对调度器的架构，可分为单体(Monolithic)调度器、两层调度器、状态共享调度器这三类。\n  调度器分类 (figure 1)   单体式调度器 首先单体式调度器的资源请求、任务调度、任务状态、资源状态都是通过该实例进行管理和同步的。OS 进程、线程调度，Hadoop YARN 调度器以及 Kubernetes 原生调度器都属于此类调度器。\n两层调度器 状态共享调度器 工作中使用的调度器 一些思考🤔 ","date":"2023-02-02","permalink":"https://blog.xieydd.top/scheduler/","tags":["scheduler","kubernetes"],"title":"调度系统"},{"categories":["tech","2022","kubernetes","documentation"],"contents":"最近在搞在内部自研平台上做一些 NUMA 感知调度的工作，涉及到 kubernetes 节点资源拓扑的发现以及调度方面的内容。但是无奈才疏学浅，遇到问题查问题，一知半解的始终抓不到头绪，谨以此篇文章来梳理总结。\n为啥需要感知拓扑 这里 kubernetes 官方说了：目前越来越多的系统利用 CPU 和硬件加速器，比如 GPU,DPU 来支持低延迟的任务以及高吞吐的并行计算任务。\n但是好像又说的不清楚，其实本质原因是冯诺依曼架构带来的问题。还是那句老话，没有银弹，冯诺依曼架构将存储器和运算器分开，指令和数据均放在存储器，为现代计算的通用性奠定了基础。但是也埋下了隐患，那就是内存容量指数级提升后，CPU 和内存之前的数据传输成为了瓶颈。目前服务器中的设备基本都是通过 PCIe 总线进行高速连接，而不同的用途的服务器可能其总线布局也不相同，如下图所示（网上找到的，非本人所画）,左图 GPU 驻留在不同的 PCIe 域上，GPU 内存之间的直接 P2P 复制是不可能的，从 GPU 0 的内存复制到 GPU 2 的内存需要首先通过 PCIe 复制到连接到 CPU 0 的内存，然后通过 QPI 链接传输到 CPU 1 并再次通过 PCIe 传输到 GPU 2。可以想象这个过程在延迟和带宽方面增加了大量的开销，而右图可以通过 GPU P2P 连接实现超高速通信。简单总结下，拓扑会影响设备间的通信，通信对业务造成稳定性以及效率，所以需要通过一些技术手段让业务感知拓扑。\n  PCIe Topo (figure 1)   拓扑类型 目前有哪些拓扑需要感知:\n GPU Topology Awareness NUMA Topology Awareness  GPU Topology Manager 业界目前有几种实现方案：\n Volcano GPU Topology Awareness 百度智能云 GPU 拓扑感知调度  Volcano 目前未完全实现，智能云闭源只能通过一些分享的信息，管中窥豹。\nWhy 为什么需要 GPU 的拓扑感知，首先上个图，这张图来自 NVIDIA 官方，描述了现在主流的 GPU 显卡 V100 在服务器中的拓扑关系。\n  GPU Topo (figure 2)   每块 V100 GPU有6个 NVLink 通道，8块 GPU 间无法做到全连接，2块 GPU 间最多只能有2条 NVLink 连接。其中 GPU0 和 GPU3，GPU0 和 GPU4 之间有2条NVLink 连接，GPU0 和 GPU1 之间有一条 NVLink 连接，GPU0 和6之间没有 NVLink 连接，故 GPU0 与 GPU6 之间仍然需要通过 PCIe 进行通信。NVlink 连接的单向通信带宽为 25 GB/s，双向通信带宽为 50 GB/s，PCIe 连接的通信带宽为16 GB/s。所以在 GPU 训练过程中如果错误的分配了 GPU， 比如某训练任务 Pod 申请了两张卡 GPU0 与 GPU6，在跨 GPU 通信可能就成为了训练任务的瓶颈。\n拓扑信息可以在节点执行命令查看：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # nvidia-smi topo -m GPU0\tGPU1 GPU2 GPU3 GPU4\tGPU5 GPU6 GPU7 GPU0\tX PIX\tPHB PHB SYS\tSYS\tSYS SYS GPU1\tPIX\tX PHB PHB SYS\tSYS\tSYS SYS GPU2\tPHB\tPHB\tX PIX SYS\tSYS\tSYS SYS GPU3\tPHB\tPHB\tPIX X SYS\tSYS\tSYS SYS GPU4\tSYS\tSYS\tSYS SYS X PIX\tPHB PHB GPU5\tSYS\tSYS\tSYS SYS PIX\tX PHB PHB GPU6\tSYS\tSYS\tSYS SYS PHB\tPHB\tX PIX GPU7\tSYS\tSYS\tSYS SYS PHB\tPHB\tPIX X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge) PIX = Connection traversing a single PCIe switch NV# = Connection traversing a bonded set of # NVLinks   How 这里就不一一解析，本身也没完整的实现来看，就以我自己的理解来梳理出一些大致思路。\n第一步其实是感知，即通过 daemon 组件来进行 nvidia gpu 、网络拓扑、NVLink、PCIe 的信息。第二步则是调度器，定义策略。策略1：优先将同一个 NUMA node 下 NVLink 数最多的 GPU 调度到一个 Pod 上；策略2：优先将处于同一个 PCI switch 的 GPU 和网卡分配给同一个 Pod。大致整体思路如下：\n GPU-device-plugin 或者其他 daemon 进程，构造节点的 GPU 拓扑信息 CRD； Pod 定义 topo 策略，比如策略1或者策略2； 新定义调度器会根据 pod 调度的策略 fliter 、priority阶段过滤不满足策略节点，给满足策略节点打高分； 关于节点 gpu device 的发现更新交给 device-plugin 和 kubelet 来做, 参见文章。  目前 GPU 拓扑信息可以通过官方的 nvml (NVIDIA Management Library) 通过接口查询。\nNUMA Topology Awareness Why 谈到 NUMA 拓扑感知，一定要先解释 NUMA 是干啥呢，为啥要感知它呢？\n  NUMA Topo (figure 3)     CPU Cache Latency (figure 4)   上面两幅图给你答案，现代 CPU 多采用 NUMA 架构， NUMA 全称 \u0026ldquo;Non-Uniform Memory Access\u0026rdquo; 即非一致性内存访问。为啥搞个非一致性，一致性不好吗？答案肯定是不好，因为如果使用 UMA 即一致性内存访问，随着北桥上的物理核心越来越多，CPU的频率也越来越高，总线带宽扛不住，访问同一块内存的冲突问题也会越来越严重。我们回到 NUMA架构，每个 NUMA node 上会有自己的物理CPU内核，以及每个 NUMA node 上核心之间之间也共享 L3 Cache。同时，内存也分布在每个NUMA node上的。某些开启了超线程的CPU，一个物理CPU内核在操作系统上会呈现两个逻辑的核。\n回到业务侧，对于业务侧，如果程序都跑在同一个NUMA node上，可以更好地去共享一些L3 Cache，L3 Cache的访问速度会很快。如果L3 Cache没有命中，可以到内存中读取数据，访存速度会大大降低。\n在容器大行其道的今天，由于 CPU 错误分配的问题尤为严重。因为现在节点出现了超卖，节点上有大量的容器在同时运行，如果同一个进行分配了不同的 NUMA 会发生什么问题：\n CPU 争抢带来频繁的上下文切换时间； 频繁的进程切换导致 CPU 高速缓存失败； 跨 NUMA 访存会带来更严重的性能瓶颈。  总结下：在现代 CPU 架构下，如果不感知 NUMA 拓扑关系，错误的进行 CPU 的分配，会导致性能问题，影响业务的 SLA。\nHow 上一章节阐述了为什么需要 NUMA 感知调度，那目前怎么感知 NUMA 拓扑呢，有什么现成的方案呢？这边我简单列下在 Kubernetes 生态的项目，各位看官如果有补充，可在评论区评论：\n Kubernetes Topology Manager Offical Crane NUMA 拓扑感知 Koordinator Fine-grained cpu orchestration  Kubernetes Topology Manager 拓扑管理器（Topology Manager） 是一个 kubelet 组件，旨在协调负责这些优化的一组组件。Topology Manager 其实是解决一个历史问题，CPU Manager 和 Device Manager 是独立工作的，互相不感知。 首先来看下 Kubernetes Topology Manager 的实现，这里我也不想造轮子了，可以参看阿里的同学总结的一篇好文。这里做一个总结：\n 找到不同资源的 topology hints 即拓扑信息， cpu 的选择标准是在满足资源申请的情况下，涉及的 NUMA 节点个数最少前提下涉及到 socket 个数最小的优先选择。 device manager 则在满足资源申请情况下，涉及 NUMA 节点最小优先选择。 不同 topology 类型的 hints 做 merge 操作，也就是 union，选出最优策略  如果选到还好，如果没选出来怎么办？kubernetes 提供了 kubelet 配置策略:\n best-effort: kubernetes 节点也会接纳这个 Pod，就是效果不达预期。 restricted：节点会拒绝接纳这个 Pod，如果 Pod 遭到节点拒绝，其状态将变为 Terminated。 single-NUMA-node：节点会拒绝接纳这个Pod，如果 Pod 遭到节点拒绝，其状态将变为Terminated。这里比 restricted 还多一个条件是选出来的 NUMA 节点个数需要是1个。  所以我们看到 Kubernetes Topology Manager 还是在以 NUMA 为中心来进行不同资源（NIC, GPU, CPU）来进行 complete fair 最短路径的选择。而且是在 pod 被调度到某个节点上后 kubelet 执行上述的过程，这样会带来几个问题：\n pod 有很大概率会 Terminated, 生产上不可用。 节点的 topology-manager-policy 配置不方便， kubelet 每次配置参数都需要重启，如果遇到特殊的版本可能会重启所有节点 pod，详见文章  所以我们会想到几个优化的方案：\n 让一个类似 kubelet 的 daemon 进程，可以发现 topo 关系，并向外暴露； 可以让拓扑感知放在 kube-scheduler 给 pod 分配 node 的时候就感知，而且可以指导调度； 提供声明式的灵活的 topology manager policy。  下面介绍的几个拓扑感知方案其实就是基于上面的 idea 应运而生的。\nCrane NUMA 拓扑感知 首先看下 Crane NUMA 感知调度的架构图。\n  Crane NUMA Topology Aware (figure 5)   大致流程如下：\n Crane-Agent 从节点采集资源拓扑，包括NUMA、Socket、设备等信息，汇总到NodeResourceTopology这个自定义资源对象中。 Crane-Scheduler在调度时会参考节点的NodeResourceTopology对象获取到节点详细的资源拓扑结构，在调度到节点的同时还会为Pod分配拓扑资源，并将结果写到Pod的annotations中。 Crane-Agent在节点上Watch到Pod被调度后，从Pod的annotations中获取到拓扑分配结果，并按照用户给定的CPU绑定策略进行CPUSet的细粒度分配。  这里其实就已经解决上述 Kubernetes Topology Manager 的缺陷。不过我们发现策略怎么配置呢，这里提供两种策略配置方案\n 业务侧，在 pod 上打不同的标签来指定策略：  none：该策略不进行特别的CPUSet分配，Pod会使用节点CPU共享池。 exclusive：该策略对应kubelet的static策略，Pod会独占CPU核心，其他任何Pod都无法使用。 NUMA：该策略会指定NUMA Node，Pod会使用该NUMA Node上的CPU共享池。 immovable：该策略会将Pod固定在某些CPU核心上，但这些核心属于共享池，其他Pod仍可使用。   节点侧  目前默认是 cpu manager policy 是 static 即允许为节点上具有某些资源特征的 Pod 赋予增强的 CPU 亲和性和独占性； topology manager policy 是 SingleNUMANodePodLevel。 如果节点无 topology.crane.io/topology-awareness 标签，则 topology manager policy 为 none    这里有个比较特别的功能，默认 kubelet 的 static 的 cpu manager 策略，只对 pod qos 为 guranteed 且资源申请为整数的 pod 生效，且分配指定的 cpu 其他进程无法占用。但是配合 crane-agent 和 Crane NUMA 感知调度后, 可以实现 pod 和 绑定核心的 pod 共享资源，可以在利用绑核更少的上下文切换和更高的缓存亲和性的优点的前提下，还能让其他 workload 部署共用，提升资源利用率。而且放松了 pod 的要求，只需要任意container的CPU limit大于或等于1且等于CPU request即可为该container设置绑核。实验下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $ cat nginx apiVersion: v1 kind: Pod metadata: name: nginx annotations: topology.crane.io/topology-awareness: \u0026#39;true\u0026#39; topology.crane.io/cpu-policy: \u0026#39;immovable\u0026#39; spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: requests: cpu: 2 memory: 1Gi limits: cpu: 2 memory: 1Gi $ k exec -it nginx /bin/bash $ taskset -cp 1 # 查看绑核 pid 1\u0026#39;s current affinity list: 0,1 # 查看 burstable pod 的 cpuset 信息 $ cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2260198d_db73_41f0_8ae3_387e09d3b9ec.slice/cri-containerd-6a5dfa37f9ce9102e1f781160d1fecb11b17dc835e5d72b9d7f573b515af86b3.scope/cpuset.cpus 0-9 # change to exclusive annotations: topology.crane.io/topology-awareness: \u0026#39;true\u0026#39; topology.crane.io/cpu-policy: \u0026#39;exclusive\u0026#39; $ cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2260198d_db73_41f0_8ae3_387e09d3b9ec.slice/cri-containerd-6a5dfa37f9ce9102e1f781160d1fecb11b17dc835e5d72b9d7f573b515af86b3.scope/cpuset.cpus 2-9   符合预期, 这里 cpuset 看官不了解可以参看 Linux Cgroup 入门教程：cpuset。\nKoordinator Fine-grained cpu orchestration Koordinator 和 Crane 在 NUMA 感知这块的架构是类似的，koordlet 替代 crane-agent, koord-scheduler 替代 crane-scheduler, 甚至描述节点 topo 的 CRD 都是一样的名字 NRT。这里梳理几个不同的点：\n Koordinator 支持的 cpu manager 的策略更多，除了 static 还支持申请完整物理核心的 full-pcpus-only 策略，以及需要多个 NUMA 满足分配情况下的均匀分配策略 distribute-cpus-across-NUMA。 Koordinator 支持基于 NUMA 拓扑更多的调度策略， 比如 bin-packing 优先调慢一个节点或者分配到最空闲的节点。 此外 Koordinator 相对于 crane 在 PodQos 以及 CPU manager 上的粒度更细，这也是为啥叫 Fine-grained cpu orchestration 的原因吧，回头单独整一篇文章来详细解读下。  引用 这里站在巨人的肩膀上，再次感谢。\n https://kubernetes.io/zh-cn/ https://github.com/volcano-sh/volcano/ https://mp.weixin.qq.com/s/uje27_MHBh8fMzWATusVwQ https://www.infoq.cn/article/tdfgiikxh9bcgknywl6s https://github.com/NVIDIA/go-nvml https://gocrane.io/zh-cn/docs/ https://koordinator.sh/docs/user-manuals https://www.likakuli.com/posts/kubernetes-kubelet-restart/ https://zhuanlan.zhihu.com/p/121588317  ","date":"2022-12-29","permalink":"https://blog.xieydd.top/kubernetes-topo-aware-all-you-need-know/","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切"},{"categories":["Opinion","AI","DevTools"],"contents":"最近我用 Code Agent 高强度地构建了一款安全产品——基于 eBPF + PostgreSQL 的 LLM Security Insight 平台。技术栈很杂：eBPF、React、Go、Docker、Kubernetes、PostgreSQL、DuckDB。React 和 eBPF 我完全是新手，但一个月后项目竟然跑通了。这篇聊聊我的使用感受。\nMy Agent Journey 从 VS Code Copilot 到 Cursor，再到 Gemini CLI、VS Code Agent、Claude Code、OpenCode、OhMyOpenCode —— 我基本把主流工具挨个试了个遍。\n最早接触的是 VS Code Copilot。指定文件，说明改动，生成 Patch。叫 Code Assistant 更准确，就是个结对编程工具。在熟悉的领域用起来确实舒服，代码提示 + 结对编程，编程就应该是这样。\n后来切换到 Cursor，它的 Agent 模式和多文件编辑让我眼前一亮。当时很多人吐槽 —— 习惯了结对编程，多文件编辑让 Review 时间变长，模型生成的代码也不敢直接 Approve。但 Cursor 敢做第一个吃螃蟹的人。随着 Claude Sonnet 模型的能力提升，Cursor 彻底癫狂。哦对，2025 年初 Cursor 联合创始人还给我们团队发邮件，聊可复现开发环境 infra（envd）。那时候他们就预见 Agent 大规模落地会遇到一致性的问题。Cursor 用向量数据库 + Turbopuffer 解决多租户场景的代码检索，这思路和我们做 VectorChord 如出一辙。不过 Cursor 免费额度锐减后，我没信用卡续费，就换赛道了。\nGemini CLI 来了。Google 的资源和 Infra 确实给力，慷慨的免费 Quota 吸引了大量用户。当时我还在负责 VectorChord Cloud，只能用它修修小 Bug、寫寫 Test。人肉上下文（Context）工作得很好，但依然没有 Agent 的感觉 —— 还是结对编程。\nVS Code 的 Agent 模式让我惊喜。支持多厂商模型（OpenAI、Claude、Google），Enterprise 订阅的 Quota 足够我用。但我更习惯多文件编辑，每一行代码还是心里有数。\n直到 Claude Code 出现，我才真正体会到 Agent 的感觉。模型能力飞跃带来的代码质量提升 + Plan/Auto/Edit 模式的丝滑切换，让我可以规划、有节奏地实现目标。\nHow I Work 我的工作流大概是这样：\n1. Global Context\n1 2 3 4 5 6 7 8 9 10  ~/llm-observability/context/ ├── agent-protocol-research.md ├── analyze.sql ├── llm_prompt_injection_analyzer_pg.py ├── pencil-designs/ ├── security-product-research.md └── designs/ ├── architecture.md ├── frontend.md └── mcp.md   所有调研、设计、工具代码都丢进这个文件夹。Agent 能读取，Xuanwo 的实践很有道理。\n2. Design First 和 Agent 一起打磨设计文档，确定技术选型和架构。设计文档就是 Agent 的说明书，写得越清楚，产出越靠谱。\n3. Validate Before Build 先让 Agent 写验证性代码，确认方案可行。之前有个 PostgreSQL Schema 设计，Agent 生成的 DDL 我不确认能否工作，于是先跑了个 Demo 验证。\n4. Full Implementation 方案确认后，Agent 完整实现。中间会遇到 Agent 自己搞不定的 Case，比如 eBPF 的某些内核兼容性问题和 Go Runtime 的并发 Bug。这种 Case 我会切换到 Claude Sonnet 4.5，让它带着上下文直接 Debug，效率很高。\nGit Worktree 是我的救星。Agent 在独立分支折腾，出了篓子不影响主分支。但数据库 Schema 变更是个痛点 —— Docker 跑的单机 PG 无法并发修改 Schema，只能串行。\nTool Selection 为什么我最终回到了 VS Code？因为我一直用 VS Code Remote 连接开发机。Claude Code 和 Codex 都在 VS Code 上有官方 Extension，还能输入图片（这个场景我经常遇到）。VS Code Remote + Claude Code Extension 是目前我用着最顺手的组合。\n至于模型选择，我的经验是：\n   维度 推荐模型     审美 GPT-5.2-Codex \u0026gt; Gemini-3 Pro \u0026gt; Claude Opus 4.5 \u0026raquo; GLM 4.7   Plan Claude Opus 4.5 断档领先   Code Claude Sonnet 4.5、GLM 4.7   Debug Claude Opus/Sonnet 4.5、GLM 4.7    多模型协同是趋势。不同模型擅长不同领域，组合使用效果最好。但订阅多个模型、跟踪每个模型的更新迭代、记住每个插件的用法 —— 这也是真够累的。\nThoughts Fork 是刚需\n 代码/文件级别：Git 解决了。 运行环境级别：Docker/envd 解决了个人用户需求，但大规模 Agent 运行时的调度还没有完美方案。Unikernel 可能是个方向，但 Firecracker 这类技术还不够成熟。 数据库级别：ACID 特性让 Fork 很难。分布式数据库一旦 Fork 必然有数据冗余和一致性问题。PostgreSQL 这种单体数据库反而有优势。通过 PITR + JuiceFS Snapshot 可以实现 PG 的 Fork，工具如 piglet。  多模型协同是未来\nSkill Best Practice + MCP 是生产力放大器\nContext/Memory 的演进和共享是核心战场\nAll in AI\nNext Step  升级开发机配置 全自动化：DNS 配置、域名注册、Agent Notification 更好的 Global Context + Memory 管理方案 大规模 Agent 运行时 Infra 研究  ","date":"2026-02-02","permalink":"https://blog.xieydd.top/code-agent-2026-02-02/","tags":["Code Agent","VS Code","Claude Code","OpenCode","Workflow"],"title":"Coding at Agent-Speed"},{"categories":["tech","2024","vector search","Postgres"],"contents":"加入 Tensorchord 已经一年有余，一直也没有时间静下心来写一些文章。主要是有了彤彤女儿后，事情多了很多。中间也经历过业务从 Serverless 模型推理 Modelz pivot 到向量搜索领域 VectorChord 的过程。Pivot 的经历或许可以在之后的文章中和大家分享，感兴趣的也可以直接联系我。最近半年一直在开发 VectorChord Cloud, 所以在这里边学边总结向量数据库中的门门道道。\n1. 什么是向量 向量在物理，数学，以及计算机科学等领域的含义都有所不同。这里的向量主要指的是计算机科学中的向量，也就是一组有序的数值。在计算机科学中，向量通常用来表示数据，比如在机器学习中，我们通常会将一张图片转换成一个向量，或者将一段文字 tokenizer 之后转换成一个向量，然后再进行训练。在向量数据库中，我们通常会将一张图片，一段文本，或者一段音频通过 embedding 模型转换成一个向量，然后再进行存储和检索。下面是一个简单的例子，我们通过 all-MiniLM-L6-v2 模型将一段文本转换成一个向量。all-MiniLM-L6-v2 将句子和段落映射到 384 维 dense vector，并可用于聚类或语义搜索等任务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from sentence_transformers import SentenceTransformer # 初始化模型 model = SentenceTransformer(\u0026#39;all-MiniLM-L6-v2\u0026#39;) # 要嵌入的文本示例 sentences = [ \u0026#34;Hugging Face is creating a tool that democratizes AI.\u0026#34;, \u0026#34;I love natural language processing.\u0026#34;, \u0026#34;Transformers are state-of-the-art models for NLP tasks.\u0026#34; ] # 生成嵌入 embeddings = model.encode(sentences) # 打印嵌入 for sentence, embedding in zip(sentences, embeddings): print(f\u0026#34;Sentence: {sentence}\u0026#34;) print(f\u0026#34;Embedding: {embedding}\\n\u0026#34;)   总结一下,向量其实是真实世界的实体和计算机世界的桥梁, 计算机通过向量来理解和处理真实世界的数据。\n2. 什么是向量数据库 世界本没有向量数据库，只是向量多了，就成了向量数据库，开个玩笑hh。这里我给个简单的定义：能够索引并存储 vector，以实现快速检索和相似性搜索功能的数据库。网络上很多人将向量数据库定义为专注于处理向量数据的数据库，这个定义是不准确的。准确的说向量与向量搜索是一种新的数据类型和查询处理方法，和传统数据库的类似和索引方法并无本质区别。\n3. 什么是向量搜索 向量搜索也叫向量检索，是一种 Information Retrieval 的技术，用于在高维向量空间中查找与给定查询向量最相似的向量。为了衡量两个向量之间的相似性，我们通常会使用余弦相似度，欧氏距离，曼哈顿距离等。为了加速向量搜索，我们通常会使用索引结构，比如 KD-Tree，IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。向量搜索在很多领域都有应用，比如在推荐系统中，我们可以使用向量搜索来查找与用户历史行为最相似的商品，然后推荐给用户；在图像检索中，我们可以使用向量搜索来查找与给定图片最相似的图片；在 RAG（Retrieval Augmented Generation）中，我们可以使用向量搜索来查找与给定问题最相似的文本，增强大模型的 Context 从而提高生成答案的质量。\n3.1 向量搜索应用场景 3.1.1 推荐系统 如 Qdrant 关于 Video Content-based Recommendation 的 On-premise 案例，通过 multilingual universal sentence encoder 来对上传视频时候的脚本进行嵌入。这里不是简单的对视频进行抽帧，更多的信息来自于上传时候的视频标题，描述，自动检测标签以及通过 whisper 语音识别的内容。所以目前遇到的问题是如果视频是无音频，被迫使用标题以及描述进行推荐，这样对于审核团队来说是一个很大的挑战。这里提到了推荐领域的 call start issues, 也就是用户在刚开始使用的时候，推荐系统的推荐质量不高，这个时候用户体验会很差。在非即时更新的协作推荐器以及元数据推荐器的基础上，增加基于内容的推荐器，可以大大优化 call start issues。\n3.1.2 图像检索 immich 是一个高性能的开源 self-hosted 图像以及视频管理解决方案。试想当你把你所有的视频和图片都上传到 immich 之后，你很难在很短的时间内找到你想要的图片或者视频。这个时候就需要一个高效的图像检索系统 smart search，通过向量搜索技术，你可以通过文本描述以及额外的过滤器（标签，日期等）来快速精准的找到你想要的图片或者视频。\n   图片来自于 immich\n 3.1.3 RAG RAG（Retrieval Augmented Generation）主要解决在 LLM 应用中的几个问题：\n LLM 训练模型的数据不是即时的，换句话说是静态的数据，获取最新数据重新进行训练的成本太大。 LLM 缺乏特定领域的知识，因为 LLM 的训练语料大都是网络上通用的数据集。而在比如金融，医疗，法律等领域，私域中的数据或许是最重要的，缺乏领域内数据会让 LLM 出现幻觉问题。 LLM 的黑匣子问题，我们无法知道 LLM 是如何生成答案的，其答案的来源来自何处。  这里借用 Paul lusztin 和 Aurimas Griciunas 的两张图来解释 RAG 的工作原理：\n 获取金融新闻的流式即时数据，以及历史数据 将数据进行 chunking 转换成 embedding 模型的输入，然后将 embedding 存储到向量数据库中 用户提问 通过向量搜索找到最相似的新闻 chunks，然后将用户历史的 chat 信息和新闻 chunks 进行 Prompt composition 输入到 LLM 中生成答案。 将答案返回给用户 将新的 chat 信息存储到用户历史数据中   私域中的数据，例如 Notion, Jira,本地 pdf 文件等等进场 chunking 转换成 embedding 模型的输入 将 chunk 输入到 embedding 模型中，然后将 embedding 存储到向量数据库中 Vector Database 构建 Index 用户提问, 输入到 embedding 模型 embedding 输出 query 的 embedding vector 将 5 中的 vector 作为 Query vector 输入到向量数据库中 向量数据库通过 ANNs（Approximate Nearest Neighbors Search）找到最相似的 chunks 将搜索到的 chunks 和 query 构建 Prompt 输入到 LLM 中生成答案  3.2 相似度指标 3.2.1 余弦相似度 余弦相似度是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的夹角来衡量的。余弦相似度的取值范围是[-1, 1]，其中1表示两个向量之间的夹角为0度，表示两个向量完全相同；-1表示两个向量之间的夹角为180度，表示两个向量完全相反；0表示两个向量之间的夹角为90度，表示两个向量之间没有相似性。计算公式如下：\n 这个公式计算了向量 𝐴 和 𝐵 之间的夹角余弦值。\n3.2.2 欧氏距离 欧氏距离是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的距离来衡量的。欧氏距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下：\n 这个公式计算了向量 𝐴 和 𝐵 之间的欧氏距离, 有些直接不开根号其只是数值不同，并无本质区别。\n3.2.3 负内积 负内积（Negative inner product），它是通过计算两个向量之间的内积来衡量的。数值越大则表示两个向量之间的相似性越高。计算公式如下：\n 3.2.4 曼哈顿距离 曼哈顿距离（taxicab distance），它是通过计算两个向量之间的距离来衡量的。曼哈顿距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下：\n 3.3 向量搜索算法 直觉上，我们可以通过遍历所有的向量来找到与给定查询向量最相似的向量，但是这种方法的时间复杂度是 O(n)。当向量的数量很大时，这种方法是不可行的，对于你的应用延迟不可接受。为了加速向量搜索，我们通常会使用索引结构，比如 IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。通过 ANNs (Approximate Nearest Neighbors Search) 算法，我们可以在更低的时间复杂度，比如 O(log(n))，找到与给定查询向量最相似的向量。\n3.3.1 LSH (Locality Sensitive Hashing) 局部敏感哈希 (LSH) 的工作原理是通过哈希函数处理每个向量，将向量分组到存储桶中，从而最大化哈希冲突，而不是像通常的哈希函数那样最小化冲突。\n这里引用 Pinecone 的一张图：\nLSH 的具体细节如下图所示：\n Shingling：使用 k-shingling 以及 one-hot encoding 将文本转换成稀疏向量  k-shingling 的意思是以窗口大小为 k 的滑动窗口，在文本中提取 k 个连续的字符 one-shot encoding 的意思是，将 k-shingling 的结果和词汇表进行比较，如果存在则在词汇表表示为1，不存在则为0    后使用 MinHash 创建“签名”  创建 [1\u0026hellip;len(voc)+1] 的随机排列 随机排列中从上到下的值作为 index ，如果原始稀疏 vector 的 index-1 位置为1则取随机排列的 index-1 位置数为签名值 重复 n 次得到 n 维度稠密向量    Band and Hash  将 n 维度的签名向量分成 b 组，每组 r 个 对每组进行 hash，得到 b 个 hash 值 如果两个向量的 hash 值相同，则将这两个向量放到同一个桶中 如果在同一个桶中，则认为其为候选对     这里随着 b 的增大返回更多的候选对，这自然会导致更多的误报\n 这意味着随着纬度的增加，误报的可能性越大，而且维数增大后需要维护更多的 hash 桶，存储的开销也会增大。所以 LSH 更适合低维度的向量搜索，不是目前的主流向量搜索算法。\n3.3.2 IVF（Inverted File Index） 倒排索引算法是一个简单、易懂而且非常容易实现的算法，而且有着不错的搜索速度，但是搜索的精度较 HNSW 较差些，但是内存消耗相对 HNSW 更少。\n构建 IVF 索引的核心分为两个步骤：\n 通过聚类算法将向量分成 nlist 个簇 将向量分配到对应的簇中  搜索时候，设定需要搜索的聚类个数 nprobe\n 这里参数的影响是：\n 增大 nlist 会降低构建索引的速度，因为在聚类过程中向量需要跟更多的中心点进行计算；同时会降低搜索时间，因为对应中心点的向量更少了，做 knn 的时候更快。 增大 nprobe 会提高召回率但是会降低搜索速度，因为需要搜索更多的单元格。   3.3.3 HNSW (Hierarchical Navigable Small World) HNSW 结合了 NSW 以及 Skip List 的优点，是一种高效的向量搜索算法。HNSW 的核心思想是通过构建一个多层的图，每一层都是一个小世界，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。\nNSW 是建立在一个理论的基础上，NSW 上的点到任意点的距离都是有限的，而且是通过很少的几次跳跃就能找到的。\nNSW 的构造过程：\n 随机选择一个点作为插入点 查找与插入点最近的 m 个点 将插入点与 m 个点相连  这里的随机性会让前期的图中长连接线增多，加速搜索，可以理解成“高速公路”，下图中的红色线就是长连接线： NSW 的搜索过程如下，这里借用知乎网友“工牌厂程序猿”的一张图：\n  初始化三个集合，分别是 visited，candidate，result（定长）；随机选择初始点进入，并加入 visited 以及 candidate 集合, candidate 保存和 query 点的距离 寻找初始点的 n 最近邻点，加入到 visited 集合，注意如果友点在 visited 集合中则废弃，，对 n 个近邻点并行计算和 query 的距离，进行升序排序（由近到远）加入 candidate 集合 寻找 candidate 集合中的 n 个近邻点，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 C 点，只有 D 点没访问过，因为 D 点距离 query 点距离小于 C 到 query 点距离，所以 result 中将 C 换成 D 点，candidate 中将 C 换成 D 点 重复 3 步骤，寻找 D 的 n 哥最近邻，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 E 点和 G 点，因为 E 点距离 query 点距离小于 result 集合中的最大距离，所以 result 中将 H 换成 E 点，candidate 中将 E 点剔除 重复 3 candidate 集合中距离 query 最小距离的点 H 的距离比 result 集合中距离 query 最大的点 E 的距离还大，则停止查询  Skip List 是一种高效的数据结构，可以在 O(log(n)) 的时间复杂度内找到与给定查询向量最相似的向量。Skip List 的核心思想是通过构建一个多层的链表，每一层都是一个有序的链表，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。\n 这里需要注意 HNSW 的几个点：\n 需要控制 HNSW 每一层的点最大连接数 Max, 在随机（越底层概率越大）插入节点时，如果有邻居节点 N 的连接数大于 Max，则对 N 进行 KNN 搜索重新与新的邻居建立连接。 启发式选边策略：在每一层搜索与插入点最邻近的 M 个节点的时候，它是先召回了 efConstruction 个，然后再选择出 M 个(efConstruction \u0026gt;= M)，选择 M 的过程可以直接选择 Top-M 但是可能会降低整体的连通性，“工牌厂程序猿” 的文章具体列举了这个 case:   这里的 efConstruction 是 4，M 是 2，如果直接选择 Top-M 的话，一定会选择 A 和 B, 这样 ABQ 和 CD 的连通性就降低了，这里在选择 A 后寻找第二个最近邻的时候检测 QA 和 AB 距离，如果 QA \u0026gt; AB 则再寻找下一个最近邻，知道大于 QA 为止,这里找到 C 点时 AC \u0026gt; AQ。\nHigh degree vertex 越靠近最上层，这样可以减少搜索路径，提高搜索效率  构建参数：\n efConstruction: 图构建过程中的一个参数，用来控制在为每个节点建立连接时，考虑的最近邻候选节点的数量。该参数具体影响的是图在构建过程中节点之间连接的质量。较高的 efConstruction 值意味着在为一个节点选择邻居时会考虑更多的候选节点，从而生成更优质的图结构。但是，较高的 efConstruction 值会增加构建图的时间和空间复杂度，而且在搜索时也会增加搜索时间。 m: 每个顶点添加的最大邻居数，分为 m_0=2m 以及 m_max=m, 参看代码.  搜索参数：\n efSearch: 在搜索时，用来控制搜索的质量。较高的 efSearch 值意味着搜索时会考虑更多的候选节点，从而提高搜索的质量。但是，较高的 efSearch 值会增加搜索时间。  HNSW 由于检索过程中涉及平均单条查询会产生数百个读磁盘操作，需要不停的寻找下一个随机点，放到 SSD 会导致极高的时延，所以是全内存的。\nNSG (Navigating Spreading-out Graph) 虽然 HNSW 的查询速度快，精度高，但是并非完美，它主要有以下几个痛点：\n 索引复杂且占用内存大：HNSW 的核心是“多层高速公路”网络。这个多层结构虽然效果好，但实现起来相对复杂，而且为了存储每一层的图结构，需要占用不小的内存（RAM）。当数据量达到十亿、百亿级别时，内存成本会非常高。 构建过程复杂：在构建 HNSW 索引时，需要为每个新加入的节点随机选择一个层级，然后在该层级及以下的所有层级中寻找邻居并连接，这个过程也比较耗时。  NSG 论文要解决的核心问题是：能不能设计一种**结构更简单、索引更小（更省内存）**的图，但搜索性能却能和复杂、庞大的 HNSW 相媲美，甚至超越它？ NSG 的创新可以总结为“一个理论，一个枢纽，一个技巧”。\n 核心理论：从“任意两点”到“一个中心”的思想转变 (MRNG)   理想的图 (MSNET)：学术界早就提出过一种理想的图，叫做“单调搜索网络”（Monotonic Search Network, MSNET）。在这种图上搜索，你从任何一个点出发，每走一步都必然离你的目标更近。就像在一个山谷里找最低点，你只要一直往下走，保证能找到。这种图上搜索不会“走回头路”，效率极高。但问题是，构建一个完美的 MSNET 非常非常耗时，不切实际。 NSG的理论基础 (MRNG)：论文作者基于 MSNET 的思想，提出了一种新的、更稀疏的图叫 MRNG (Monotonic Relative Neighborhood Graph)。它也是一种能保证“单调性”的图，但构建规则更巧妙。然而，构建一个完整的 MRNG 仍然太慢了。 思想的飞跃：作者们意识到，我们真的需要保证任意两个点之间都有单调路径吗？在实际搜索中，我们总是从一个（或少数几个）入口点开始搜索。  HNSW 的思路：从顶层图的入口点开始，层层下沉。 NSG 的革命性思路：我们干脆只设置一个全局的“入口点”，我们称之为“导航节点”（Navigating Node）。我们不再追求构建一个“任意两点都能高效互通”的复杂网络，而是构建一个“从这个导航节点出发，到任何其他节点都有一条高效路径”的中心化网络。 这就好比城市交通规划：   HNSW：建立了一个“地铁 + 公交 + 快速路”的立体交通网，你从任何地方到任何地方都很方便，但系统复杂，建设成本高。 NSG：在城市正中心建立一个巨大的“宇宙中心交通枢纽”。所有远距离出行，都先到这个枢纽，再从枢纽出发去目的地。整个网络结构变成了以枢纽为中心的放射状，大大简化了。  核心设计：导航节点 (Navigating Node)  这个“宇宙中心交通枢纽”就是 NSG 的灵魂——导航节点。\n 如何选择？ 通常选择整个数据集的“质心”（几何中心）附近的一个点。这个点具有最好的全局视野。 它有什么用？  搜索的唯一入口：所有的查询，无论目标在哪里，都统一从这个导航节点开始。 建图的绝对核心：在为每个节点 p 选择邻居时，NSG 不再是盲目地在全局找，而是利用一个非常聪明的技巧。    核心技巧：高效的邻居选择策略  这是 NSG 能够兼顾“图的稀疏性”（省内存）和“搜索的高效性”的关键。当给节点 p 构建边时，如何为它选择邻居？\n 传统方法：在 p 周围一定范围内找最近的几个点。范围选多大？邻居选多少个？很难权衡。 NSG 的天才技巧：  我们先在图上（一个预先构建的粗糙 k-NN 图）模拟一次从“导航节点”到 p 的搜索。 这次搜索会经过一条路径，比如 导航节点 → a → b → \u0026hellip; → p。 NSG 认为，这条路径上经过的所有节点 (a, b, \u0026hellip;)，都是 p 的“绝佳邻居候选人”。为什么？因为这些点是连接全局中心和局部点 p 的关键桥梁。 最后，NSG 只需要在这些少量的“候选人”中，再使用 MRNG 的规则精选出最终的邻居。 这个技巧的妙处在于，它把一个全局的、开放的邻居选择问题，变成了一个局部的、数量有限的优化问题，极大地降低了建图的计算复杂度。    结合上面的创新点，我们来梳理一下 NSG 的完整流程：\n 构建 (Build)   步骤一：找枢纽。计算数据质心，找到离质心最近的点作为“导航节点”。 步骤二：打基础。构建一个基础的、粗糙的 k-近邻图（k-NN Graph）。这一步是为了后续的路径搜索。 步骤三：连主路 (核心)。遍历图中除导航节点外的每一个节点 p：  以 p 为目标，从“导航节点”在上一步的 k-NN 图上进行一次搜索。 把搜索路径上访问过的所有节点都收集起来，形成 p 的“候选邻居池”。 从这个池子里，为 p 精心挑选出固定数量（比如 m 个）的邻居并连接。同时，为了保证图的稀疏性，严格控制每个节点出度的上限。   步骤四：查漏补缺。由于严格限制了出度，可能会导致图的某些部分断开连接。最后，从导航节点开始做一次深度优先搜索（DFS），检查是否所有点都可达，如果发现有“孤岛”，就强行给孤岛上的点连一条边到主网络，确保图的连通性。  搜索 (Search) NSG 的搜索过程异常简洁：   从“导航节点”出发。 不断地、贪心地跳到当前节点的所有邻居中，离查询目标最近的那个邻居。 重复上一步，直到找不到比当前节点更近的邻居为止，此时就到达了局部最优，也就是搜索结果。  NSG 选边跟 HNSW 选择最小边策略不同。以点 r 为例，当 r 与 p 建立连接时，以 r 和 p 为圆心，r 和 p 的距离为半径，分别做圆，如果两个圆的交集内没有其他与 p 相连接的点，则 r 与 p 相连。在连接点 s 时，由于以 s 和 p 距离为半径的交集圆内，已有点 r 与 p 相连，所以 s 和 p 不相连。t点因为s点已经排除，所以保留,下图中最终与点 p 相连的点只有r, t 和 q，这样减少了冗余边，减少了平均出度。\n NSG 相对 HNSW 的优势：\n 索引大小 (Index Size)：NSG 完胜。NSG 的索引（图占用的内存）通常只有 HNSW 的 1/2 到 1/3。这是 NSG 最大的优势，尤其是在处理超大规模数据集时，能节省巨大的硬件成本。 搜索性能 (Performance)：两者旗鼓相当，NSG 略有优势。在很多数据集上，NSG 的搜索速度（QPS, Queries Per Second）都超过了 HNSW，尤其是在要求高精度（High Recall）的场景下。 简洁性 (Simplicity)：NSG 完胜。NSG 是单层图结构，没有 HNSW 复杂的层级概念，无论是理解还是实现都更加简单。  但是业界普遍还是采用 HNSW, 主要原因有以下几点：\n 增量索引 (Incremental Indexing) 的能力至关重要   工业场景：在实际应用中，数据是动态变化的。比如，电商网站每秒都有新商品上架，社交网络每秒都有新图片发布。我们不可能每次都为了新增的几个数据点，就花几小时甚至几天去重建整个索引。我们需要一个能高效“添加”和“删除”单个数据点的索引。 HNSW 的优势：HNSW 的多层结构天然支持增量索引。当一个新节点加入时，算法会给它随机分配一个层级，然后从顶层开始，像普通查询一样找到它应该插入的位置，并连接附近的邻居。这个过程是局部化的，非常高效，不会影响整个图的结构。删除节点也相对容易处理。 NSG 的劣势：NSG 的设计哲学——依赖一个固定的全局“导航节点”——让增量索引变得非常困难。  添加节点：新节点 p 的邻居是通过从“导航节点”到 p 的路径来决定的。但新节点 p 还没有加入图，怎么找路径？即便加入，它的存在也可能会改变其他节点的最优路径，理论上可能需要更新大量节点的边，这违背了增量索引的初衷。 删除节点：如果删除的节点恰好是某个关键路径上的一环，可能会导致图的连通性被破坏，或者搜索效率下降。 这是 HNSW 在工业界占据主导地位的最核心、最关键的原因。 对于静态数据集（比如一次性构建好，不再改变的图像库），NSG 是个好选择。但对于绝大多数需要持续更新的在线服务，HNSW 的灵活性是不可替代的。    生态系统和成熟度 (Ecosystem and Maturity)   先发优势：HNSW 提出得更早，并且其作者很快将其集成到了当时非常流行的 nmslib 库中。随后，像 Faiss (Facebook AI)、Lucene (Apache) 等业界顶级的开源库都迅速跟进，提供了稳定、高效的 HNSW 实现。 社区和文档：有了这些顶级库的支持，HNSW 迅速积累了庞大的用户社区、丰富的文档、教程和生产环境下的实践经验。当一个工程师遇到问题时，他能很容易地找到解决方案。 信任和稳定性：HNSW 已经在无数公司的生产环境中经受了多年的考验，其鲁棒性（robustness）和稳定性得到了充分验证。对于追求稳定压倒一切的工业界来说，选择一个“久经沙场的老将”远比选择一个“初出茅庐的天才”要稳妥。  鲁棒性和参数敏感度 (Robustness \u0026amp; Parameter Sensitivity)   HNSW：其多层结构提供了很好的容错性。即使在高层走错了一步，下沉到更稠密的低层后，仍然有很大的机会修正路线，最终找到正确的目标。它的性能对参数（如 M, efConstruction）虽然敏感，但调整起来相对直观。 NSG：其性能高度依赖于“导航节点”的选择和图的连通质量。如果导航节点选得不好（比如选在了数据分布的边缘），或者在建图过程中为了稀疏性而剪枝过多，导致关键路径丢失，那么搜索性能可能会急剧下降。它的“天花板”可能很高，但“地板”也可能更低。  3.3.4 DiskANN DiskANN 是建立在这样一个背景下：\n ANNS 算法（如 HNSW、NSG）为了保证速度，都依赖于将索引存储在内存中。这导致存储成本非常高，限制了可以处理的数据集大小。 SSD 的潜力： 固态硬盘（Solid-State Drives, SSDs）相比传统的硬盘更快，但仍然比内存慢很多。大家普遍认为，将索引放在 SSD 上会导致搜索速度大幅下降，无法满足实际应用的需求。  DiskANN 要解决的问题：如何设计一种新的 ANNS 索引，突破内存限制，使得我们能够在单台机器上，利用廉价的 SSD 存储十亿级别的数据，并保持高精度和低延迟？\nDiskANN 系列有三篇文章，DiskANN，FreshDiskANN，FilterDiskANN。\nDiskANN 的核心创新在于：算法 (Vamana 图) + 系统设计（SSD 优化）。\n 核心算法：Vamana 图  Vamana 图是 DiskANN 论文提出的新型图索引，它主要解决了以下两个问题：\n 优化 SSD 访问模式： 普通的图索引（如 NSG、HNSW）在 SSD 上进行搜索时，会产生大量的随机读取，导致延迟很高。Vamana 图旨在减少 SSD 的随机读取次数，使其更倾向于顺序读取。 更好的搜索性能： 相比于 NSG 和 HNSW，Vamana 图在特定场景下可以提供更好的搜索精度和速度。 为了实现这些目标，Vamana 图的设计有以下几个关键点： Robust Prune: 在构建图的过程中，Vamana 图使用了一种名为 Robust Prune 的策略来选择每个节点的邻居。这个策略的目标是确保搜索路径上的节点能够以一定的比例因子 alpha 更快地接近查询目标，从而减少搜索的跳数和 SSD 的读取次数。 Iterative Construction: Vamana 图的构建是一个迭代的过程。首先，图被初始化为随机连接的图，然后通过迭代地优化每个节点的邻居来改进图的结构，使其更适合于近似最近邻搜索。 Graph Merging: 为了处理大规模数据集，Vamana 图支持将多个小的图索引合并成一个大的图索引。这个特性使得我们可以并行地构建索引，并将其合并成一个完整的索引，从而提高构建效率。  系统设计：SSD 优化策略  DiskANN 的另一大创新在于针对 SSD 的特性进行了一系列优化，从而最大限度地发挥 SSD 的性能。这些优化策略包括：\n Beam Search: 为了减少 SSD 的随机读取次数，DiskANN 使用了一种名为 Beam Search 的策略。在搜索过程中，算法维护一个包含多个候选节点的集合（beam），并并行地从 SSD 中读取这些节点的邻居信息。这样可以减少搜索过程中的 round-trip 次数，从而降低延迟。 Caching: DiskANN 利用内存来缓存频繁访问的节点信息。通过将热点数据缓存在内存中，可以减少对 SSD 的访问次数，从而提高搜索速度。 Implicit Re-Ranking: DiskANN 使用 Product Quantization (PQ) 等向量压缩技术来减少内存占用。但是，压缩后的向量会损失精度。为了弥补这个损失，DiskANN 在从 SSD 中读取邻居信息时，同时读取原始的、未压缩的向量，并使用这些向量来重新排序候选结果，从而提高搜索精度。 Full-Precision with Neighborhoods: 将全精度向量和邻居信息一起存储在 SSD 上，读取邻居信息时也可以一起读取全精度信息，无需额外开销，便于使用全精度信息rerank。  构图过程:\n 首先构建随机图，这里和 NSG 的K最近邻图不一样，对每一个节点随机选择 R 个节点相连接 计算起点，找全局质心最近的点，目的是尽量减少平均搜索半径 搜索起点对每个点做 ANN，将搜索路径上所有的点作为候选邻居集，执行 alpha = 1 的裁边策略 （参考 NSG） 整 alpha \u0026gt; 1(论文推荐 1.2）重复步骤 3。因为 3 是基于随机近邻图做的，第一次迭代后图的质量不高，所以需要再迭代一次来提升图的质量，这个对召回率很重要。这里拿上图举例，如果 alpha为1.2，当 ps 的距离大于 1.2 * pr 的距离，才会裁撤 ps 边。  下图可以直观感受下，alpha=1 以及 alpha=1.2 最后图的区别,第一行是 alpha=1, 第二行是 alpha=1.2，alpha=1.2 的图更加稠密，明显多了长边，减少了搜索半径。\n 这时候你可能有疑问，如果按照这样建图，根本不可能在 64GB 的机器上存放超过 1B 的数据，这里就要有一些优化手段：\n 先做全局 kmeans，将数据分成 k 个簇，然后将每个点分到距离最近的 I 个簇中，一般 I 取 2 就够了。对每个簇建基于内存的 Vamana 索引，最后将 k 个 Vamana 索引合并成一个索引。 使用量化的方法，建索引时用原始向量，查询的时候用压缩向量。因为建索引使用原始向量保证图的质量，搜索的时候使用内存可以 hold 住的压缩向量进行粗粒度搜索，这时的压缩向量虽然有精度损失，但是只要图的质量足够高，大方向上是对的就可以了，最后的距离结果还是用原始向量做计算的。 每个点的邻居集和原始向量数据存在一起。这样做的好处是可以利用数据的局部性。  如果索引文件放在 SSD 上，为了保证搜索时延，尽可能减少磁盘访问次数和减少磁盘读写请求。因此 DiskANN 提出两种优化策略：\n 缓存热点：将起点开始 C 跳内的点常驻内存，C 取 3~4 就比较好。 beam search： 简单的说就是预加载，搜索 p 点时，如果 p 的邻居点不在缓存中，需要从磁盘加载 p 点的邻居点。由于一次少量的 SSD 随机访问操作和一次 SSD 单扇区访问操作耗时差不多，所以我们可以一次加载 W 个未访问点的邻居信息，W 不能过大也不能过小，过大会浪费计算和 SSD 带宽，太小了也不行，会增加搜索时延。  此外 DiskANN 相对于 HNSW 或者 NSW 还有一个优势就是，其删除一个节点是真删除，而且删除后不会影响图的连通性。HNSW 删除一个节点后，可能会导致图的连通性被破坏，需要重新构建索引。而 DiskANN 删除一个节点后，策略是这样的：假设我们要删除的节点是**“郑州”**这个交通枢纽。郑州在中国的交通网络中是一个至关重要的“十字路口”。\n 有高速公路从“北京”、“西安”开往郑州（这些是郑州的in-neighbors）。 也有高速公路从郑州通往“武汉”、“南京”（这些是郑州的out-neighbors）。 当我们要删除“郑州”时：  确定“受影响的节点”：谁最受伤？是那些原本有路通往郑州的城市，比如“北京”和“西安”。它们的出行选择变少了。 提供“修复候选方案”：规划师对北京说：“你原来可以去郑州，现在郑州没了。但是郑州原来可以通往‘武汉’和‘南京’。现在我把‘武汉’和‘南京’作为新的高速公路修建提案交给你。” 运行 RobustPrune 进行智能决策：现在，北京的工程师（RobustPrune 算法）开始工作了。他会评估：   输入：我（北京）现有的所有高速公路 + 新的提案（通往武汉、南京）。 决策过程：在所有这些选项中，根据 RobustPrune 的核心原则（即选择那些能最大化导航效率、提供“捷径”且不冗余的连接），重新选出不超过 R 条最优的高速公路。 可能的结果：  北京的工程师发现，修一条到“武汉”的高速确实是个好捷径，于是决定修建。 他发现到“南京”的提案并不比现有路线好，于是放弃。 他甚至可能为了给到武汉的新路腾出名额（保持总数不超过R），拆掉了一条原来通往某个小城市的、不太重要的旧路。    对所有受影响节点重复此过程：“西安”的工程师也会做同样的事情。    最终结果： 通过在每个受影响的节点上运行 RobustPrune，我们不是盲目地添加所有可能的连接，而是让每个节点根据自己的局部情况和全局导航目标，自主地、智能地选择最优的连接来“修复”网络破损的地方。 这样，我们既成功删除了目标节点“郑州”，又没有破坏网络的稀疏性（度限制 R），同时还最大程度地保持了整个高速公路网的导航质量。这就是 Vamana 删除操作的精髓所在。\nDiskANN 的优势在于可以在很小的内存占用下配合 SSD 达到不错的搜索性能，但是规格小的机器在构建索引的时候会比较慢，这里需要做好权衡。\nFreshDiskANN 在 DiskANN 基础上做了进一步的优化，主要是针对数据更新的场景。FreshDiskANN 通过引入一个新的索引结构，称为 FreshVamana，并引入删除列表，来支持并加速对数据的的插入和删除。于此同时采用两阶段 StreamingMerge 算法，将索引分为长期索引，临时索引，通过分层支持大规模数据。\nFilterDiskANN 从满足查询标签条件（如日期、价格范围、语言）的索引点中找到查询嵌入的最近邻，支持两种过滤算法：\n FilteredVamana：构建单一索引，其中每个顶点的邻居基于几何结构和共同标签来决定。该算法在图构建过程中同时考虑向量距离和标签信息，形成既考虑几何邻近性又考虑标签相关性的连接 StitchedVamana：为每个标签构建单独的子图，然后将这些子图叠加并剪枝形成最终图。这种方法在实际数据集上consistently提供比FilteredVamana高2倍的QPS，但构建时间更长  3.3.5 Summary 这里总结下，内存占用上 HNSW 明显大于 IVF，LSH 以及 Flat(KNN)，召回率以及搜索速度上 HNSW 优于 IVF，LSH。DiskANN 在内存占用上优于 HNSW，但是在构建索引的时候会比较慢，搜索速度上优于 HNSW，但是召回率上不如 HNSW。所以在选择向量搜索算法的时候需要根据自己的需求来选择。\n3.4 向量搜索算法优化 通过减少 Vector 大小，或者通过降维让搜索更快，这里列举了一些常见的向量搜索算法优化方法。\n3.4.1 PQ（Product Quantization） 这里借用知乎网友的一张图，没办法，网友的图画的太好了： 构建阶段：\n 首先将N个原始向量，各自切分为多个子向量。比如256维向量，切分为8个32维子向量 然后在每个子向量空间内进行聚类，可以采用KMeans等聚类算法。假设每个子空间有1024个聚类，对每个聚类中心编码，得到1024个ID 将原始向量编码成最近的聚类中心ID，最后做拼接  检索阶段：\n 检索向量进行切分 切分的子空间和计算每个聚类中心的距离，做成距离表 利用距离表计算query和候选样本在每个子空间的距离，累加后取 top-k  其中涉及到切分都可以使用并行求解，这里一般不直接使用 PQ 因为依旧需要很多的距离计算，这里一般先进行 IVF 找到最有希望的top-k cluster 然后再进行 PQ。\n3.4.2 SQ（Scalar Quantization） SQ 比较简单 编码： scalar = (max-min)/255, floor(value-min/scaler) 如果小于0 则取 0，大于 255 则取 255，这样就将向量压缩到 0-255 之间，这样可以减少向量的大小，但是会损失一些信息。 解码：value = min + (code + 0.5)*(max-min)/255\n3.4.3 RabitQ RabitQ 来源于论文 RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search, 具体理论细节可以参考 RabitQ 的作者 Jianyang Gao 的 blog。\nRabitQ 指出现阶段 PQ 算法的两个问题：\n 用 kmeans 的质心作为 codebook， 构建时启发式的近似估计，没有理论保证 距离估计，用量化后的向量和 query向量的距离估计原始向量和 query 向量的距离，缺乏近似误差范围 上述的两个问题就导致 PQ 可能在一些数据集或者真实场景下的误差非常大。  如何解决上述问题：\n codebook 构建阶段  首先对数据向量进行归一化，以便将它们对齐在单位超球面上D维空间 构建一组 $2^{D}$ 坐标为的双值向量 $−1/\\sqrt{D}$或者 $+1/\\sqrt{D}$（即，该集合由超立方体的顶点组成，这些顶点均匀地分布在单位超球面上） 通过将每个双值向量乘以随机正交矩阵来随机旋转双值向量（即执行一种 Johnson-Lindenstrauss 变换)，为了消除确定性码本对特定向量的偏好 对于每个向量，将其与码本最接近的向量作为量化向量， 最小化内积。 由于每个量化向量都是旋转的 D 维双值向量，我们将其量化码表示为长度的位串D，其中 0 和 1 表示两个不同的值。 码本构造的基本原理是它具有清晰的几何解释（即码本中的向量是单位超球面上的一组随机旋转向量） 使得可以明确地分析数据向量、它们的量化向量和查询向量之间的几何关系。   距离估计  根据上述几何关系仔细设计了数据向量和查询向量之间距离的估计器，并证明这个估计器是无偏的，而且提供了误差范围。 于此同时在估计距离时，即使使用较短的量化代码，也能以较小的经验误差估计出大约一半的优势    RaBitQ’s distance estimator：\n 单个 data vector 使用 bitwise 按位操作 批量数据使用 SIMD 加速  使用随机 codebook 避免双值 codebook 在一些特定向量表现不佳，比如 $1/\\sqrt{D}$\u0026hellip; $−1/\\sqrt{D}$ 和 (1, 0, 0, 0) 的量化，我们使用随机正交矩阵去乘这个 codebook, 让 codebook 单位向量有相同的概率旋转到单位超球面上的任意位置\n这里使用通俗的话讲解下 RabitQ 的核心思想：\n终极解说：高维空间中的闪电搜图术 (RaBiT-Q 算法)\n想象一下，我们正在为世界上最大的图片分享网站（比如Instagram或Pinterest）构建一个“以图搜图”的功能。这意味着，当用户上传一张“沙滩上的柯基犬”的照片时，我们要能从包含数十亿张图片的数据库里，瞬间找出所有其他“沙舍上的柯基犬”或者类似的图片。\n这是一个巨大的挑战。计算机是如何“看懂”并比较图片的呢？\n 第一步：把图片变成“数字指纹”——向量化  计算机无法直接理解图片。它首先需要通过一个复杂的神经网络（比如 ResNet），将每一张图片都转换成一个由很多数字组成的“列表”，我们称之为向量 (Vector)。这个向量就是这张图片的“数字指纹”，它捕捉了图片的核心特征。\n 小白理解： 就像我们给人办身份证，除了照片，还会记录身高、体重、血型等信息。这里的“向量”就是一张图片的“数字身份证”，包含了成百上千个描述它的“特征值”。 专业说明： 我们使用预训练的深度学习模型（如CNN）提取图像的高维特征嵌入（Embedding）。例如，一张图片可以被表示为一个128维或更高维度的浮点数向量 o_r。我们的目标是，对于一个查询向量 q_r，找到数据库中与它欧几里得距离 ||o_r - q_r|| 最小的那些 o_r。  第二步：数据大扫除——归一化  直接比较这些原始的“数字指纹”既慢又困难。数据可能存在整体的偏移，而且每个向量的“长度”（数值大小的综合体现）也各不相同，会干扰我们对“方向”（内容相似度）的判断。\n操作：\n 中心化 (Centering): 找到所有图片向量的“平均位置”（即质心 c），然后让每个向量都减去这个中心。这相当于把整个坐标系的原点“搬”到了数据中心。 归一化 (Normalization): 将每个中心化后的向量，都除以它自身的长度。  结果： 经过这一步，所有的“数字指纹”都发生了两个变化：\n 它们不再是散乱的，而是都围绕着同一个中心。 它们的“长度”都变成了 1。它们现在都“趴”在一个巨大的、高维的“单位球”的球面上。这个操作保留了向量的方向（代表图片内容），而舍弃了其原始的长度。   小白理解： 想象一下，我们把所有人的身高都统一拉伸或压缩到1米8，但保持他们高矮胖瘦的“比例”不变。这样一来，大家就都站在了同一个起跑线上，比较起来更公平。 专业说明： 我们将欧氏距离的计算问题，通过 ||o_r - q_r||² = ||o_r - c||² + ||q_r - c||² - 2 · ||o_r - c|| · ||q_r - c|| · \u0026lt;q, o\u0026gt; 这个公式，转化为了一个求解归一化后的单位向量 q 和 o 之间内积 \u0026lt;q, o\u0026gt; 的问题。||o_r - c|| 可以在索引时预计算，||q_r - c|| 在查询时计算一次即可。问题的核心变成了如何快速估算 \u0026lt;q, o\u0026gt;。  第三步：建立“宇宙坐标系”——码本构建与随机化  即使所有向量都在单位球面上，它们的数量依然是海量的。逐一比较内积还是太慢。我们需要一套“参考坐标系”来快速给向量定位。\n操作：\n 码本 (Codebook): 我们在球面上建立一套“参考点”。一个绝妙的方法是，在球内部嵌入一个超立方体 (Hypercube)，它的所有顶点正好落在球面上。这些顶点非常均匀、对称地分布，是理想的参考点。这些顶点的集合，就是我们的“码本”。 随机化 (Randomization): 一个固定的码本有其“盲区”。为了解决这个问题，我们不使用一个固定的立方体，而是将它进行随机旋转。我们生成M个（比如16个）不同的随机旋转，得到M个姿态各异的“码本”。   小白理解： 为了快速找到地球上的某个城市，我们不会去拿尺子量。我们会先看它属于哪个大洲（亚洲、欧洲\u0026hellip;）。这里的“码本”就是“七大洲”的划分。但固定的七大洲划分可能对某些边界城市不友好。于是我们干脆设计了M套不同的、随机的“大洲划分法”，从多个角度来给城市定位。 专业说明： 码本由超立方体的顶点 {-1, 1}^D 构成。我们生成M个随机正交旋转矩阵 R_1, ..., R_M。这样我们就拥有了M个码本 C_1, ..., C_M，其中 C_i 是由 R_i 旋转后的超立方体顶点组成。  第四步：给每张图片打上“压缩编码”——索引构建  这是离线完成的工作，为整个数据库建立一个可以被闪电搜索的索引。\n操作： 对于数据库中的每一张图片（即每一个向量 o），我们：\n 用之前生成的那M个旋转矩阵，去旋转 o。 将M个旋转后的 o，分别在M个码本中找到离它最近的那个顶点。 因为超立方体的顶点坐标只有+1和-1，我们可以用1个比特（0或1）来表示。比如+1记为1，-1记为0。 这样，每个 o 就被转换成了 M个D维的二进制码。  结果： 一个原本需要数千比特存储的浮点数向量，现在被压缩成了M个D比特的二进制码。存储空间大大减少，而且为后续的快速位运算比较铺平了道路。\n 小白理解： 我们为公司里的每个员工，都从M个不同的角度拍了一张照片，并根据照片生成了一个极简的“素描码”（二进制码），存入他的档案。 专业说明： 对于每个 o，我们计算 b_i = Quantize(R_i * o)，其中 i=1...M。Quantize 函数找到超立方体中最近的顶点，并将其表示为D比特的二进制码 b_i。同时，我们也预计算并存储 \u0026lt;ō, o\u0026gt;，即量化误差项，为后续的精确估算做准备。  第五步：闪电搜索——查询与估算  当用户上传新图片（查询向量 q）时，实时搜索开始。\n 查询编码： q 必须经历和数据库图片完全相同的处理：归一化、用M个相同的旋转矩阵旋转、生成M个二进制码。 粗筛 (Candidate Search): 我们拿着 q 的M个二进制码，去数据库中进行M次搜索。搜索方式不是慢速的浮点数计算，而是超高速的位运算。我们计算查询码和数据库码的汉明距离（有多少位不同），快速捞出几百个最相似的“候选图片”。 精排 (Re-ranking \u0026amp; Estimation): 对这几百个候选图片，我们不能再用粗糙的二进制码了。我们需要一个更精确的估算。这时，那个神奇的估算公式登场了： \u0026lt;o, q\u0026gt; ≈ \u0026lt;ō, q\u0026gt; / \u0026lt;ō, o\u0026gt;  这个公式告诉我们，我们想知道的真实内积 \u0026lt;o, q\u0026gt;，可以用两个可以被高效计算或提前算好的值来近似！ 为什么可以这么近似？ 因为我们天才地利用了高维空间的集中现象。我们证明了，在随机旋转下，那个导致公式无法简化的“麻烦项”\u0026lt;ō, e₁\u0026gt; 的值会高度集中于0，可以忽略不计。这是整个算法的理论基石和点睛之笔。   最终排序： 我们用这个估算公式，为几百个候选图片计算出近似的内积，再结合之前的第一步公式，得到最终的距离排序。将最靠前的结果返回给用户。   小白理解：  新来的访客也被从M个角度拍了照，生成M个“素描码”。 安保拿着这些素描码去档案室飞快地比对（位运算），找出几百个长得最像的员工。 对于这几百个员工，安保不再看素描了，而是拿出一个“神奇估算器”，通过一个简单的除法，就估算出了访客和每个候选员工的真实“亲近度”，然后排出名次。    总结\n这个算法的精髓在于，它完美地融合了多个强大的思想：\n 降维思想： 将复杂的欧氏距离问题，转化为更简单的单位向量内积问题。 量化压缩： 用超立方体码本将浮点向量压缩成二进制码，极大节省了存储和计算。 随机化思想： 通过随机旋转码本，避免了算法的“盲区”，并激活了高维空间的奇特性质。 高维几何： 深刻利用了“集中现象”这一反直觉的数学事实，推导出了一个极为高效的近似估算公式，完成了从“粗筛”到“精排”的最后一跃。  最终，一个原本需要在数十亿数据上进行慢速、精确计算的“不可能完成的任务”，被巧妙地分解为离线编码和在线闪搜两部分，实现了在保证高精度的前提下的数量级加速。\n4. 常见的向量数据库极其优劣 以下列举了一些常见的向量数据库，以及他们的优劣势, 有些是专用向量数据库，有些是对现有关系型数据库的扩展。\n4.1 Milvus Milvus 是一款非常优秀的开源向量数据库，支持多种向量搜索算法，包括 HNSW，DiskANN，IVF 等。 除了向量检索的基本功能，还提供 sharding, streaming data ingestion 以及 hybrid search 等功能。\nMilvus 采用的是云原生的存算分离，shared-everthing 的架构，且控制面和数据面分离。各个组件都是独立且可横向拓展的，分别为：\n 接入层(Access Layer)：由一组无状态 proxy 组成。它对外提供用户连接的 endpoint，负责验证客户端请求并合并返回结果。  它使用Nginx、Kubernetes Ingress、NodePort、LVS 等负载均衡组件提供统一的服务地址。 由于 Milvus 采用大规模并行处理 (MPP) 架构，代理会聚合并后处理中间结果，然后将最终结果返回给客户端   协调服务(Coordinator Service)：负责分配任务给执行节点，分别有 root coord、data coord、query coord。  root coord: 处理数据定义语言 (DDL) 和数据控制语言 (DCL) 请求，例如创建或删除集合、分区或索引，以及管理 TSO（时间戳 Oracle）和 time ticker。 data coord: 管理 data 以及 index 节点拓扑，维护元数据，触发flush、compact、索引构建等后台数据操作。 query coord: 管理查询节点 topology ，load balancing 以及 growing segments 到 sealed segments 转换.   执行节点(Worker Node)：执行从协调服务分配的任务以及 proxy DML 命令。  Query Node：检索增量日志数据，并通过订阅 log broker 将其转换为不断增长的段，从对象存储加载历史数据，并在向量和标量数据之间运行混合搜索。 Data Node：通过订阅 log broker 来获取增量日志数据，处理mutation 请求，并将日志数据打包成日志快照并存储在对象存储中 Index Node：构建索引。索引节点不需要常驻内存，可以通过Serverless 框架来实现。   存储层(Storage)：对象存储负责存储数据，存储 Data files 和 Index files。  Meta Storage: 元存储存储元数据的快照，例如 collection schema 和 message consumption checkpoints。存储元数据需要极高的可用性、强一致性和事务支持，因此 Milvus 选择了 etcd 进行元存储。 Milvus 还使用 etcd 进行服务注册和健康检查。 Object Storage: 存储日志的快照文件、标量和向量数据的索引文件以及中间查询结果。 Milvus 使用 MinIO 作为对象存储，可以轻松部署在 AWS S3 和 Azure Blob。但对象存储的访问延迟较高，且按查询次数收费。为了提高性能并降低成本，Milvus 计划在基于内存或 SSD 的缓存池上实现冷热数据分离。 Log Broker: 发布-订阅系统，它负责流数据持久化和事件通知。当工作节点从系统故障中恢复时，它还能确保增量数据的完整性。 Milvus cluster 使用 Pulsar 作为 log broker； Milvus standalone 使用 RocksDB 作为 log broker。此外，log broker 可以很容易地替换为Kafka等流数据存储平台。    Milvus 的云原生架构是其优点，同时也给开发者带来了不小的挑战，例如新概念的学习，以及相关组件 Pulsar 或者 etcd 带来的运维管理上的挑战。\n4.4 Pinecone 4.5 Qdrant 4.6 Pgvector 4.7 Pgvecto.rs 4.8 VectorChord 如何选型参考 大模型时代如何选向量数据库？Milvus（Zilliz）、LanceDB、Chroma、Pinecone四大热门技术全解析！。\n5. 优秀的向量搜索库以及向量数据库开源项目 6. 向量数据库商业化你需要知道什么 7. 总结 在这里我也是走马观花的介绍了一些向量搜索的基础知识，以及一些常见的向量搜索算法，向量搜索应用场景，向量搜索算法优化，常见的向量数据库极其优劣势，优秀的向量搜索库以及向量数据库开源项目，后面还是希望可以应用到实际的场景中。希望这篇文章能够帮助你更好的了解向量搜索。\n8. 引用 非常感谢 Pinecone 的文章，让我对向量数据库有了更深的了解。\n https://www.pinecone.io/learn/series/faiss/vector-indexes/ https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/ https://zhuanlan.zhihu.com/p/379372268 https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/ https://www.xiemingzhao.com/posts/hnswAlgo.html https://whenever5225.github.io/2020/05/11/hnsw-heuristic/ Search Engine For AI：高维数据检索工业级解决方案 https://zhuanlan.zhihu.com/p/50143204 https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw https://blog.wilsonl.in/graph-vector-search/  ","date":"2024-07-13","permalink":"https://blog.xieydd.top/vector-search/","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道"},{"categories":null,"contents":"📫 您如果想联系我，可以直接发送邮件，邮箱地址 xieydd@gmail.com,或者您可以加我的微信 echo -n 'eGlleWRkX2hhaGEK' | base64 -d.\n💻\n截止到 2026 年，有着超过 8 年的 AI Infra 的经验：\n2018-2021.2（含实习）Unisound  在 AI 算法公司 Unisound 负责 Atlas 超算平台的研发和运维，支持 NLP 以及 CV 模型训练。主要工作如下：   开发大规模智能调度系统以优化多租户资源分配 优化高性能分布式文件系统 Lustre 的性能 构建多层缓存的云原生架构以加速 AI 模型训练  在 Unisound 从事 8 Bit 训练及推理优化工作，落地模型在 NPU 以及 NVIDIA Edge Device 的优化工作  2021.2-2023.5 Tencent Cloud  构建公有云大规模 AI 平台：   通过 EKS （Elastic Kubernetes Service） 构建高性能、可伸缩的弹性离线训练平台 结合公有云对象存储以及加速器 GooseFS, 构建云上高性能缓存调度系统  构建 FinOps 基础设施帮助公共云中的客户更轻松地管理优化云成本，提升云资源利用率：   通过调度时以及重调度优化，通过高低优任务识别，智能弹性扩缩容 结合腾讯如意内核调度器优化以及可观测性，在保证服务质量的同时进行成本优化 在内部云中大规模推出降低成本计划，通过资源的合理分配，提升资源利用率  2023.5-至今 Tensorchord  负责在 GCP 上构建 Serverless Inference 平台 ModelZ， 提供极致的冷启动优化模型服务推理服务：   通过构建缓存模型服务、镜像预热等手段优化模型服务的冷启动时间 引入 JuiceFS 构建高性能缓存调度系统，提升模型服务的性能  Cloud Team Leader: 构建向量数据库 VectorChord 的云服务以及客户支持 VectorChord Cloud   在 AWS 上构建基于 PostgreSQL 的向量数据库，实现控制面、数据面分离，BYOC(Bring Your Own Cloud)、BYOD(Bring Your Own Data) 等功能 引入云原生架构，实现 PostgreSQL 存算分离、高可用、Backup、PITR(Point-In-Time Recovery)、In-Place Upgrade 等功能 构建基于 PostgreSQL 企业级知识库 RAG 系统, 作为 agent 记忆模块  技能栈：Kubernetes, GCP, AWS, Kubeflow, FinOps, RAG, Vector Database, Storage Accelerate, Tensorflow , Pytorch, Cloud Native, MLOps, AI Infra, PostgreSQL etc.\n开源项目 🌱 目前专注在 MLOps 以及 FinOps 领域,贡献了一些开源项目：\n fluid Fluid, elastic data abstraction and acceleration for BigData/AI applications in cloud. (Project under CNCF) crane Crane is a FinOps Platform for Cloud Resource Analytics and Economics in Kubernetes clusters. The goal is not only to help users to manage cloud cost easier but also ensure the quality of applications. crane-scheduler Crane scheduler is a Kubernetes scheduler which can schedule pod based on actual node load. creator Creator is the brain of crane project, contains crane core algorithm module and evaluation module. openmodelz One-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine). clusternet [CNCF Sandbox Project] Managing your Kubernetes clusters (including public, private, edge, etc.) as easily as visiting the Internet vectorchord Scalable, fast, and disk-friendly vector search in Postgres, the successor of pgvecto.rs.  推荐博客    Type Author/Company Blog URL     Infra Chris Riccomini https://materializedview.io/   Infra Jack Vanlightly https://jack-vanlightly.com/   Math and Science 苏剑林 https://kexue.fm/   AI Infra Colfax https://research.colfax-intl.com/blog/   Postgres Gabriele Bartolini https://www.gabrielebartolini.it/articles/   AI Sebastian Raschka https://magazine.sebastianraschka.com/archive?sort=new   AI Algorithm Tom Yeh https://www.byhand.ai/   AI Infra Chip Huyen https://huyenchip.com/blog/    ","date":"2022-12-29","permalink":"https://blog.xieydd.top/about/","tags":null,"title":"关于远东"}]