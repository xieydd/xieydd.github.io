[{"categories":["2024","Postgres"],"content":"最近在研究 Postgres 高可用的方案，这里记录一下。 Postgres 高可用 ","date":"2024-07-26","objectID":"/postgres-ha/:0:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"高可用目标 Postgres 高可用一般有两个目标： RPO（Recovery Point Objective）即数据恢复点目标，主要指的是业务系统所能容忍的数据丢失量。 RTO（Recovery Time Objective）即恢复时间目标，主要指的是所能容忍的业务停止服务的最长时间，也就是从灾难发生到业务系统恢复服务功能所需要的最短时间周期。 简单来说就是，在多长时间内恢复数据库恢复到什么状态，比如在 5min 内恢复到丢失数据不超过 30min 的状态。当然最好的情况就是 RTO \u003c 30s, RPO~=0。 ","date":"2024-07-26","objectID":"/postgres-ha/:1:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"场景 为了达到上面的这个最好的情况，需要覆盖以下场景： 当 Primary 节点发生故障时，自动切换到 Standby 并在 RTO 的要求下恢复到 RPO 目标。 当数据库发生数据的意外删除、升级变更错误、或者遇到硬件故障等，可以恢复到指定时间点。 ","date":"2024-07-26","objectID":"/postgres-ha/:2:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"概念 为了满足以上场景需要有以下技术或者概念的支持： Continuous Archiving：Continuous Archiving 一般是是对 WAL(writer ahead log) 进行归档；如果遇到 db system crash 可以通过 replay WAL 来进行恢复。 Point-in-Time Recovery (PITR) ：对于硬件故障来说，基于物理复制的高可用故障切换可能会是最佳选择。而对于数据损坏（无论是机器还是人为错误），时间点恢复（PITR）则更为合适：它提供了对最坏情况的兜底。 Physical Replication：数据文件和事务日志文件全部复制 （PGData, pg_wals） Logical Replication：根据复制的标记（例如主键）在发布和订阅之间进行复制，一般不用于容灾，FDW 场景居多。 Streaming Replication ：基于 WAL 日志的流复制，主要用于容灾场景。将 WAL XLOG 记录连续从 primary 传送到 standby， 有同步以及异步两种方式。 ","date":"2024-07-26","objectID":"/postgres-ha/:3:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"工具 ","date":"2024-07-26","objectID":"/postgres-ha/:4:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"Backup and Restore 这里首先列举下常用的备份和恢复方式以及优劣： 1. Pg_dump (Logical Backup) 逻辑备份是通过 SQL 命令 pg_dump 将数据库中的数据导出到一个文件中，然后通过 SQL 命令将数据导入到数据库中。 优势： 根据需要，逻辑备份可以是表级到数据库级 备份不会阻止数据库上的读/写活动 可以恢复到 PostgresSQL 的不同主要版本，甚至不同的操作系统架构中 劣势： 逻辑备份在恢复时，需要 replay ,如果数据量大，需要很长时间，而且可能会降低整体性能 不支持全局变量的 dump, 只能用 pg_dumpall 2. Physical Backup 物理备份是停止 PostgreSQL 集群后进行的 PostgreSQL 离线备份,这些备份包含整个集群数据。 优势： 备份和恢复速度快 适合大型数据库 适合高可用场景 劣势： 不能跨版本恢复 不能跨操作系统恢复 3. Continuous Archiving and Point-in-Time Recovery (PITR) Online Backup 或者叫 Hot Backup, 先进行完整的备份,可以在不停止 PostgreSQL 集群的情况下在线进行。增量备份生成的 WAL 日志，然后可以通过恢复 WAL 来恢复存档/WAL。 优势： 可以恢复到任何时间点 不会导致应用程序出现任何停机 劣势： 可能需要很长时间才能从存档中恢复数据，这些主要用于容量巨大、无法进行频繁备份的数据库。 4. Snapshots and Cloud Backups 快照需要操作系统或者 cloud provider 的支持，有rsync等工具可以用来拍摄快照。 劣势： 不适用于数据库将表空间存储在多个驱动器卷中的情况。 备份需要考虑很多情况，比如备份的频率、备份的存储位置、备份的恢复时间、备份保留策略等等，所以需要一些工具辅助我们来进行备份，下面列举一些常用的开源工具如下： pgbackrest EDB barman WAL-G 从这个讨论中，可以看到 barman 相对于 pgbackrest 还是有些功能的缺失： Zstd 压缩 Delta restore Encryption at rest Native postgres page checksum validation Multi repo ","date":"2024-07-26","objectID":"/postgres-ha/:4:1","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"High Availability Patroni Patroni 默认使用的是异步的 Streaming Replication，这意味着主节点上的事务提交后，可能会有一段时间才会被复制到备节点上。这段时间内，如果主节点发生故障，可能会丢失这段时间内的数据。为了减少这种数据丢失，可以使用同步复制，但是同步复制会影响主节点的性能，因为主节点必须等待所有备节点都已经接收到并写入 WAL 日志后才能提交事务。所以需要在可用性和性能之间做平衡。 Patroni 的 maximum_lag_on_failover 和 pg 的 wal_segsize 的大小，需在可用性和持久性之间做平衡。 maximum_lag_on_failover 默认 1MB(1048576 bytes) 意思是如果有个节点滞后超过这个值，就不会被选为新的主节点。一般配合 loop_wait 和 ttl 参数 一起使用。例如 ttl 是 30 的话，如果 Patroni 节点在 30 秒内未能与 Etcd 或 Consul 续约，则该节点将被认为失去了领导权。loop_wait 设置为 10 秒，Patroni 每隔 10 秒执行一次其主要操作循环，包括状态检查和必要的操作。最坏的情况下的丟失量：maximum_lag_on_failover 字节+最后的 TTL秒时间内写入的日志量。减小该值可以降低故障切换时的数据损失上限，但也会增加故障时因为从库不够健康（落后太久）而拒绝自动切换的概率。 wal_segsize 参数定义了每个 WAL 日志段文件的大小，默认是 16MB ","date":"2024-07-26","objectID":"/postgres-ha/:4:2","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"架构 目前 Postgres 高可用架构繁多，这里列举两种常见的架构，分别对应自建 Postgres 以及云上托管 Postgres 的典型架构： Pigsty Cloudnative-PG HA ","date":"2024-07-26","objectID":"/postgres-ha/:5:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"Pigsty HA 架构 下图来自于 pigsty: 自上而下： 应用层通过 DNS 解析到 vip-manager 的 VIP，vip-manager 通过 etcd 获取当前主库的 IP 地址，然后将 L2 VIP 绑定到主库所在节点；通过 HAProxy 进行 L5 层端口转发。 Patroni：同步主节点信息给到 etcd。 vip-manager：虚拟 ip 和状态由 etcd 进行同步管理。 HAProxy：根据端口分别进行路由 5433：连接 PGBouncer 连接池，连接 primary 进行 read/write 5434：连接 PGBouncer 连接池，连接 replica 进行 read-only 5436：直连 primary，管理使用 5438：直连 replica，管理使用，连接不处理在线读取流量的专用副本，用于ETL和分析查询。 primary 和 replica 通过 Streaming Replication 进行 WAL 日志的同步，primary 通过 pg_receivexlog 将 WAL 日志发送到 replica，replica 通过 pg_replay 进行 WAL 日志的重放。 Patroni 通过 pgBackRest 进行备份，备份数据可存储在本地，远程 s3 或者 minio 存储中, 可参考文档。 PostgreSQL 使⽤标准流复制搭建物理从库，主库故障时由从库接管。 Patroni 负责管理 PostgreSQL 服务器进程，处理高可用相关事宜。 Etcd 提供分布式配置存储（DCS）能力，并用于故障后的领导者选举 Patroni 依赖 Etcd 达成集群领导者共识，并对外提供健康检查接口。 HAProxy 对外暴露集群服务，并利⽤ Patroni 健康检查接口，自动分发流量至健康节点。 vip-manager 提供一个可选的二层 VIP，从 Etcd 中获取领导者信息，并将 VIP 绑定在集群主库所在节点上。 在主从架构+故障自动切换+同步 streaming replication +pgBackRest 备份的情况下 RTO 在 1min 内且 RPO 为 0，即在不丢失数据的情况下 1min 恢复。 ","date":"2024-07-26","objectID":"/postgres-ha/:5:1","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"Cloudnative-PG HA 架构 根据 Kubernetes 容器编排的特点，Cloudnative-PG HA 架构采用了更加现代化的架构： 多 region 部署 Kubernetes 多可用区（大于等于3）部署 PostgreSQL 节点 Primary-Standby 采用同步或者异步 Streaming Replication PostgreSQL 实例不共享资源,独占节点资源，在不同的 Kubernetes 工作节点，使用本地卷 应用层提供 rw、ro、r 三种服务，分别服务连接主节点、只读工作负载的热备用副本、任意只读工作负载、在发生故障转移时，它会自动更新服务以指向升级的服务，确保来自应用程序的流量无缝重定向。 提供 Pooler 对象，创建 PGBouncer 连接池，用于连接主节点和只读节点 通过 Replica Cluster 跨多个 Kubernetes 集群部署 PostgreSQL 通过将 PostgreSQL 备份数据存储在多个 location、region 并可能使用不同的提供商（灾难恢复）来减少全局恢复点目标 (RPO) 通过利用主 Kubernetes 集群之外的 PostgreSQL 复制来减少全局恢复时间目标 (RTO)（高可用性） 指定的主集群可以随时升级，使副本集群成为能够接受写连接的主集群。 WAL 通过 s3 进行归档 通过 barman 进行备份，可以备份到云对象存储例如 s3 或者使用 Volume Snapshot 进行备份 在上述架构下可为跨区域灾难恢复提供最多大约 5 分钟的 RPO，如果使用同步 Streaming Replication 可以达到 0 RPO, 且具备极低的 RTO。 ","date":"2024-07-26","objectID":"/postgres-ha/:5:2","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"Supabase Backup graph TD; A(Supabase Backup)---\u003eB(Pro); B(Pro)---\u003eE(Database Size 0-40GB); B(Pro)---\u003eF(Database Size 40GB+); B(Pro)---\u003eG(PITR); B(Pro)---\u003eH(Read Replica); E(Database Size 0-40GB)---\u003eI(Logical Backup); F(Database Size 40GB+)---\u003eJ(Physical Backup); G(PITR)---\u003eJ(Physical Backup); H(Read Replica)---\u003eJ(Physical Backup); A(Supabase Backup)---\u003eC(Team); C(Team)---\u003eK(Database Size 0-40GB); C(Team)---\u003eL(Database Size 40GB+); C(Team)---\u003eM(PITR); C(Team)---\u003eN(Read Replica); K(Database Size 0-40GB)---\u003eI(Logical Backup); L(Database Size 40GB+)---\u003eJ(Physical Backup); M(PITR)---\u003eJ(Physical Backup); N(Read Replica)---\u003eJ(Physical Backup); A(Supabase Backup)---\u003eD(Enterprise); D(Enterprise)---\u003eO(Database Size 0-40GB); D(Enterprise)---\u003eP(Database Size 40GB+); D(Enterprise)---\u003eQ(PITR); D(Enterprise)---\u003eR(Read Replica); O(Database Size 0-40GB)---\u003eJ(Physical Backup); P(Database Size 40GB+)---\u003eJ(Physical Backup); Q(PITR)---\u003eJ(Physical Backup); R(Read Replica)---\u003eJ(Physical Backup); graph TD; A(Supabase Backup)--\u003eB(Pro); A(Supabase Backup)--\u003eC(Team); A(Supabase Backup)--\u003eD(Enterprise); B(Pro)--\u003eE(Daily Backup, Retain 7 days); E--\u003eH(pg_dumpall logical backup， when database size \u003e 40GB will use physical backup); C(Team)--\u003eF(Daily Backup, Retain 2 weeks); F--\u003eH(pg_dumpall logical backup， when database size \u003e 40GB will use physical backup); D(Enterprise)--\u003eG(Daily Backup, Retain 1 month); D--\u003eJ(physical backup); 用户可以访问每一天生成的 logical backup 的 sql 文件进行 restore。 graph TD; A(Supabase PITR)--\u003eB(WAL-G, archiving Write Ahead Log files, default 2 min or certain file size threshold and physical backups); B--\u003eC(2 minutes RPO); C--\u003eD(show database restore available from and latest restore available at); graph TD; A(PGVecto.rs Cloud PITR)--\u003eB(barman-cloud-wal-archive archiving Write Ahead Log files, default 5 min or certain file size threshold and barman-cloud-backup for physical backups); B--\u003eC(5 minutes RPO); C--\u003eD(show database restore available from and latest restore available at); D--\u003eE(delete cluster will delete all wal and physical backups); ","date":"2024-07-26","objectID":"/postgres-ha/:5:3","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","Postgres"],"content":"引用 https://pigsty.io/ https://cloudnative-pg.io/ https://www.cnblogs.com/xianghuaqiang/p/14792001.html https://docs.pgbarman.org/release/3.10.1/ https://github.com/cloudnative-pg/cloudnative-pg/discussions/3145 https://supabase.com/blog/postgresql-physical-logical-backups ","date":"2024-07-26","objectID":"/postgres-ha/:6:0","tags":["High Availability","Postgres"],"title":"Postgres 高可用","uri":"/postgres-ha/"},{"categories":["2024","vector search","Postgres"],"content":"加入 Tensorchord 已经一年有余，一直也没有时间静下心来写一些文章。主要是有了彤彤女儿后，事情多了很多。中间也经历过业务从 Serverless 模型推理 Modelz pivot 到向量搜索领域 VectorChord 的过程。Pivot 的经历或许可以在之后的文章中和大家分享，感兴趣的也可以直接联系我。最近半年一直在开发 VectorChord Cloud, 所以在这里边学边总结向量数据库中的门门道道。 ","date":"2024-07-13","objectID":"/vector-search/:0:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"1. 什么是向量 向量在物理，数学，以及计算机科学等领域的含义都有所不同。这里的向量主要指的是计算机科学中的向量，也就是一组有序的数值。在计算机科学中，向量通常用来表示数据，比如在机器学习中，我们通常会将一张图片转换成一个向量，或者将一段文字 tokenizer 之后转换成一个向量，然后再进行训练。在向量数据库中，我们通常会将一张图片，一段文本，或者一段音频通过 embedding 模型转换成一个向量，然后再进行存储和检索。下面是一个简单的例子，我们通过 all-MiniLM-L6-v2 模型将一段文本转换成一个向量。all-MiniLM-L6-v2 将句子和段落映射到 384 维 dense vector，并可用于聚类或语义搜索等任务。 from sentence_transformers import SentenceTransformer # 初始化模型 model = SentenceTransformer('all-MiniLM-L6-v2') # 要嵌入的文本示例 sentences = [ \"Hugging Face is creating a tool that democratizes AI.\", \"I love natural language processing.\", \"Transformers are state-of-the-art models for NLP tasks.\" ] # 生成嵌入 embeddings = model.encode(sentences) # 打印嵌入 for sentence, embedding in zip(sentences, embeddings): print(f\"Sentence: {sentence}\") print(f\"Embedding: {embedding}\\n\") 总结一下,向量其实是真实世界的实体和计算机世界的桥梁, 计算机通过向量来理解和处理真实世界的数据。 ","date":"2024-07-13","objectID":"/vector-search/:1:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"2. 什么是向量数据库 世界本没有向量数据库，只是向量多了，就成了向量数据库，开个玩笑hh。这里我给个简单的定义：能够索引并存储 vector，以实现快速检索和相似性搜索功能的数据库。网络上很多人将向量数据库定义为专注于处理向量数据的数据库，这个定义是不准确的。准确的说向量与向量搜索是一种新的数据类型和查询处理方法，和传统数据库的类似和索引方法并无本质区别。 ","date":"2024-07-13","objectID":"/vector-search/:2:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3. 什么是向量搜索 向量搜索也叫向量检索，是一种 Information Retrieval 的技术，用于在高维向量空间中查找与给定查询向量最相似的向量。为了衡量两个向量之间的相似性，我们通常会使用余弦相似度，欧氏距离，曼哈顿距离等。为了加速向量搜索，我们通常会使用索引结构，比如 KD-Tree，IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。向量搜索在很多领域都有应用，比如在推荐系统中，我们可以使用向量搜索来查找与用户历史行为最相似的商品，然后推荐给用户；在图像检索中，我们可以使用向量搜索来查找与给定图片最相似的图片；在 RAG（Retrieval Augmented Generation）中，我们可以使用向量搜索来查找与给定问题最相似的文本，增强大模型的 Context 从而提高生成答案的质量。 ","date":"2024-07-13","objectID":"/vector-search/:3:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.1 向量搜索应用场景 3.1.1 #### 推荐系统 如 Qdrant 关于 Video Content-based Recommendation 的 On-premise 案例，通过 multilingual universal sentence encoder 来对上传视频时候的脚本进行嵌入。这里不是简单的对视频进行抽帧，更多的信息来自于上传时候的视频标题，描述，自动检测标签以及通过 whisper 语音识别的内容。所以目前遇到的问题是如果视频是无音频，被迫使用标题以及描述进行推荐，这样对于审核团队来说是一个很大的挑战。这里提到了推荐领域的 call start issues, 也就是用户在刚开始使用的时候，推荐系统的推荐质量不高，这个时候用户体验会很差。在非即时更新的协作推荐器以及元数据推荐器的基础上，增加基于内容的推荐器，可以大大优化 call start issues。 3.1.2 #### 图像检索 immich 是一个高性能的开源 self-hosted 图像以及视频管理解决方案。试想当你把你所有的视频和图片都上传到 immich 之后，你很难在很短的时间内找到你想要的图片或者视频。这个时候就需要一个高效的图像检索系统 smart search，通过向量搜索技术，你可以通过文本描述以及额外的过滤器（标签，日期等）来快速精准的找到你想要的图片或者视频。 图片来自于 immich 3.1.3 #### RAG RAG（Retrieval Augmented Generation）主要解决在 LLM 应用中的几个问题： LLM 训练模型的数据不是即时的，换句话说是静态的数据，获取最新数据重新进行训练的成本太大。 LLM 缺乏特定领域的知识，因为 LLM 的训练语料大都是网络上通用的数据集。而在比如金融，医疗，法律等领域，私域中的数据或许是最重要的，缺乏领域内数据会让 LLM 出现幻觉问题。 LLM 的黑匣子问题，我们无法知道 LLM 是如何生成答案的，其答案的来源来自何处。 这里借用 Paul lusztin 和 Aurimas Griciunas 的两张图来解释 RAG 的工作原理： 获取金融新闻的流式即时数据，以及历史数据 将数据进行 chunking 转换成 embedding 模型的输入，然后将 embedding 存储到向量数据库中 用户提问 通过向量搜索找到最相似的新闻 chunks，然后将用户历史的 chat 信息和新闻 chunks 进行 Prompt composition 输入到 LLM 中生成答案。 将答案返回给用户 将新的 chat 信息存储到用户历史数据中 私域中的数据，例如 Notion, Jira,本地 pdf 文件等等进场 chunking 转换成 embedding 模型的输入 将 chunk 输入到 embedding 模型中，然后将 embedding 存储到向量数据库中 Vector Database 构建 Index 用户提问, 输入到 embedding 模型 embedding 输出 query 的 embedding vector 将 5 中的 vector 作为 Query vector 输入到向量数据库中 向量数据库通过 ANNs（Approximate Nearest Neighbors Search）找到最相似的 chunks 将搜索到的 chunks 和 query 构建 Prompt 输入到 LLM 中生成答案 ","date":"2024-07-13","objectID":"/vector-search/:3:1","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.2 相似度指标 余弦相似度是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的夹角来衡量的。余弦相似度的取值范围是[-1, 1]，其中1表示两个向量之间的夹角为0度，表示两个向量完全相同；-1表示两个向量之间的夹角为180度，表示两个向量完全相反；0表示两个向量之间的夹角为90度，表示两个向量之间没有相似性。计算公式如下： 这个公式计算了向量 𝐴 和 𝐵 之间的夹角余弦值。 欧氏距离是一种用于衡量两个向量之间的相似性的方法，它是通过计算两个向量之间的距离来衡量的。欧氏距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下： 这个公式计算了向量 𝐴 和 𝐵 之间的欧氏距离, 有些直接不开根号其只是数值不同，并无本质区别。 负内积（Negative inner product），它是通过计算两个向量之间的内积来衡量的。数值越大则表示两个向量之间的相似性越高。计算公式如下： 曼哈顿距离（taxicab distance），它是通过计算两个向量之间的距离来衡量的。曼哈顿距离的取值范围是[0, ∞]，其中0表示两个向量完全相同， 数值越大则表示两个向量之间的差异越大。计算公式如下： ","date":"2024-07-13","objectID":"/vector-search/:3:2","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.3 向量搜索算法 直觉上，我们可以通过遍历所有的向量来找到与给定查询向量最相似的向量，但是这种方法的时间复杂度是 O(n)，当向量的数量很大时，这种方法是不可行的。为了加速向量搜索，我们通常会使用索引结构，比如 IVF(Inverted File Index)，HNSW(Hierarchical Navigable Small World)等。通过 ANNs (Approximate Nearest Neighbors Search) 算法，我们可以在更低的时间复杂度，比如 O(log(n))，找到与给定查询向量最相似的向量。 3.3.1 #### LSH (Locality Sensitive Hashing) 局部敏感哈希 (LSH) 的工作原理是通过哈希函数处理每个向量，将向量分组到存储桶中，从而最大化哈希冲突，而不是像通常的哈希函数那样最小化冲突。 这里引用 Pinecone 的一张图 LSH 的具体细节如下图所示： Shingling：使用 k-shingling 以及 one-hot encoding 将文本转换成稀疏向量 k-shingling 的意思是以窗口大小为 k 的滑动窗口，在文本中提取 k 个连续的字符 one-shot encoding 的意思是，将 k-shingling 的结果和词汇表进行比较，如果存在则在词汇表表示为1，不存在则为0 后使用 MinHash 创建“签名” 创建 [1…len(voc)+1] 的随机排列 随机排列中从上到下的值作为 index ，如果原始稀疏 vector 的 index-1 位置为1则取随机排列的 index-1 位置数为签名值 重复 n 次得到 n 维度稠密向量 Band and Hash 将 n 维度的签名向量分成 b 组，每组 r 个 对每组进行 hash，得到 b 个 hash 值 如果两个向量的 hash 值相同，则将这两个向量放到同一个桶中 如果在同一个桶中，则认为其为候选对 这里随着 b 的增大返回更多的候选对，这自然会导致更多的误报 这意味着随着纬度的增加，误报的可能性越大，而且维数增大后需要维护更多的 hash 桶，存储的开销也会增大。所以 LSH 更适合低维度的向量搜索，不是目前的主流向量搜索算法。 3.3.2 #### IVF（Inverted File Index） 倒排索引算法是一个简单、易懂而且非常容易实现的算法，而且有着不错的搜索速度，但是搜索的精度较 HNSW 较差些，但是内存消耗相对 HNSW 更少。 构建 IVF 索引的核心分为两个步骤： 通过聚类算法将向量分成 nlist 个簇 将向量分配到对应的簇中 搜索时候，设定需要需要搜索的单元格数量 nprobe 这里参数的影响是： 增大 nlist 会降低构建索引的速度，因为在聚类过程中向量需要跟更多的中心点进行计算；同时会降低搜索时间，因为对应中心点的向量更少了，做 knn 的时候更快。 增大 nprobe 会提高召回率但是会降低搜索速度，因为需要搜索更多的单元格。 3.3.3 #### HNSW (Hierarchical Navigable Small World) HNSW 结合了 NSW 以及 Skip List 的优点，是一种高效的向量搜索算法。HNSW 的核心思想是通过构建一个多层的图，每一层都是一个小世界，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。 NSW 是建立在一个理论的基础上，NSW 上的点到任意点的距离都是有限的，而且是通过很少的几次跳跃就能找到的。 NSW 的构造过程： 随机选择一个点作为插入点 查找与插入点最近的 m 个点 将插入点与 m 个点相连 这里的随机性会让前期的图中长连接线增多，加速搜索，可以理解成“高速公路”，下图中的红色线就是长连接线： NSW 的搜索过程如下，这里借用知乎网友“工牌厂程序猿”的一张图： 初始化三个集合，分别是 visited，candidate，result（定长）；随机选择初始点进入，并加入 visited 以及 candidate 集合, candidate 保存和 query 点的距离 寻找初始点的 n 最近邻点，加入到 visited 集合，注意如果友点在 visited 集合中则废弃，，对 n 个近邻点并行计算和 query 的距离，进行升序排序（由近到远）加入 candidate 集合 寻找 candidate 集合中的 n 个近邻点，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 C 点，只有 D 点没访问过，因为 D 点距离 query 点距离小于 C 到 query 点距离，所以 result 中将 C 换成 D 点，candidate 中将 C 换成 D 点 重复 3 步骤，寻找 D 的 n 哥最近邻，加入 visited 集合，如果已经存在 visited 集合中废弃；这里查询的是 E 点和 G 点，因为 E 点距离 query 点距离小于 result 集合中的最大距离，所以 result 中将 H 换成 E 点，candidate 中将 E 点剔除 重复 3 candidate 集合中距离 query 最小距离的点 H 的距离比 result 集合中距离 query 最大的点 E 的距离还大，则停止查询 Skip List 是一种高效的数据结构，可以在 O(log(n)) 的时间复杂度内找到与给定查询向量最相似的向量。Skip List 的核心思想是通过构建一个多层的链表，每一层都是一个有序的链表，通过在每一层中搜索最近的节点，然后在下一层中搜索最近的节点，最终找到与给定查询向量最相似的向量。 这里需要主要 HNSW 的几个点： 注意这里需要控制 HNSW 每一层的点最大连接数 Max, 在随机（越底层概率越大）插入节点时，如果有邻居节点 N 的连接数大于 Max，则对 N 进行 KNN 搜索重新与新的邻居建立连接。 启发式选边策略：在每一层搜索与插入点最邻近的 M 个节点的时候，它是先召回了 efConstruction 个，然后再选择出 M 个(efConstruction \u003e= M)，选择 M 的过程可以直接选择 Top-M 但是可能会降低整体的连通性，“工牌厂程序猿” 的文章具体列举了这个 case: 这里的 efConstruction 是 4，M 是 2，如果直接选择 Top-M 的话，一定会选择 A 和 B, 这样 ABQ 和 CD 的连通性就降低了，这里在选择 A 后寻找第二个最近邻的时候检测 QA 和 AB 距离，如果 QA \u003e AB 则再寻找下一个最近邻，知道大于 QA 为止,这里找到 C 点时 AC \u003e AQ。 3. High degree vertex 越靠近最上层，这样可以减少搜索路径，提高搜索效率 构建参数： efConstruction: 图构建过程中的一个参数，用来控制在为每个节点建立连接时，考虑的最近邻候选节点的数量。该参数具体影响的是图在构建过程中节点之间连接的质量。较高的 efConstruction 值意味着在为一个节点选择邻居时会考虑更多的候选节点，从而生成更优质的图结构。但是，较高的 efConstruction 值会增加构建图的时间和空间复杂度，而且在搜索时也会增加搜索时间。 m: 每个顶点添加的最大邻居数，分为 m_0=2m 以及 m_max=m, 参看代码. 搜索参数： efSearch: 在搜索时，用来控制搜索的质量。较高的 efSearch 值意味着搜索时会考虑更多的候选节点，从而提高搜索的质量。但是，较高的 efSearch 值会增加搜索时间。 HNSW 由于检索过程中涉及平均单条查询会产生数百个读磁盘操作，需要不停的寻找下一个随机点，放到 SSD 会导致极高的时延，所以是全内存的。 NSG (Navigating Spreading-out Graph) NSG 围绕图的连通性、减少平均出度，缩短搜索路径以及图的大小等方面进行了优化， 提出新的图结构 Monotonic Relative Neighborhood Graph (MRNG)。 具体流程如下： 构建 K-nearest-neighbor-graph (KNNG) 作为构图基准 随机选择一个点作为 Navigation Point，后续所有新插入的节点在选边时都会将Navigation Point加入候选集合 在建图过程中，逐渐会将子图都和 Navigation point 相连接，这样其他的节点只需保持很少的边即可，从而减少了图的大小 每次搜索从 Navigation Point 出发能够指向具体的子图，从而减少无效搜索，获得更好搜索性能。 NSG 选边跟 HNSW 选择最小边策略不同。以点 r 为例，当 r 与 p 建立连接时，以 r 和 p 为圆心，r 和 p 的距离为半径，分别做圆，如果两个圆的交集内没有其他与 p 相连接的点，则 r 与 p 相连。在连接点 s 时，由于以 s 和 p 距离为半径的交集圆内，已有点 r 与 p 相连，所以 s 和 p 不相连。t点因为s点已经排除，所以保留,下图中最终与点 p 相连的点只有r, t 和 q，这样减少了冗余边，减少了平均出度。 3.3.4 #### DiskANN DiskANN 系列有三篇文章，DiskANN，FreshDiskANN，FilterDiskANN， 从本质上是对 HNSW或者说 NSG 算法的优化。 DiskANN 通过引入 SSD 友好","date":"2024-07-13","objectID":"/vector-search/:3:3","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"3.4 向量搜索算法优化 通过减少 Vector 大小，或者通过降维让搜索更快，这里列举了一些常见的向量搜索算法优化方法。 3.4.1 PQ（Product Quantization） 这里借用知乎网友的一张图，没办法，网友的图画的太好了： 构建阶段： 首先将N个原始向量，各自切分为多个子向量。比如256维向量，切分为8个32维子向量 然后在每个子向量空间内进行聚类，可以采用KMeans等聚类算法。假设每个子空间有1024个聚类，对每个聚类中心编码，得到1024个ID 将原始向量编码成最近的聚类中心ID，最后做拼接 检索阶段： 检索向量进行切分 切分的子空间和计算每个聚类中心的距离，做成距离表 利用距离表计算query和候选样本在每个子空间的距离，累加后取 top-k 其中涉及到切分都可以使用并行求解，这里一般不直接使用 PQ 因为依旧需要很多的距离计算，这里一般先进行 IVF 找到最有希望的top-k cluster 然后再进行 PQ。 3.4.2 SQ（Scalar Quantization） SQ 比较简单 编码： scalar = (max-min)/255, floor(value-min/scaler) 如果小于0 则取 0，大于 255 则取 255，这样就将向量压缩到 0-255 之间，这样可以减少向量的大小，但是会损失一些信息。 解码：value = min + (code + 0.5)*(max-min)/255 3.4.3 RabitQ RabitQ 来源于论文 RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search. RabitQ 指出现阶段 PQ 算法的两个问题： 用 kmeans 的质心作为 codebook， 构建时启发式的近似估计，没有理论保证 距离估计，用量化后的向量和 query向量的距离估计原始向量和 query 向量的距离，缺乏近似误差范围 如何解决上述问题： codebook 构建阶段 首先对数据向量进行归一化，以便将它们对齐在单位超球面上D维空间 构建一组 $2^{D}$ 坐标为的双值向量 $−1/\\sqrt{D}$或者 $+1/\\sqrt{D}$（即，该集合由超立方体的顶点组成，这些顶点均匀地分布在单位超球面上） 通过将每个双值向量乘以随机正交矩阵来随机旋转双值向量（即执行一种 Johnson-Lindenstrauss 变换) 对于每个向量，将其与码本最接近的向量作为量化向量。 由于每个量化向量都是旋转的 D 维双值向量，我们将其量化码表示为长度的位串D，其中 0 和 1 表示两个不同的值。 码本构造的基本原理是它具有清晰的几何解释（即码本中的向量是单位超球面上的一组随机旋转向量） 使得可以明确地分析数据向量、它们的量化向量和查询向量之间的几何关系。 距离估计 根据上述几何关系仔细设计了数据向量和查询向量之间距离的估计器，并证明这个估计器是无偏的，而且提供了误差范围。 于此同时在估计距离时，即使使用较短的量化代码，也能以较小的经验误差估计出大约一半的优势 RaBitQ’s distance estimator： 单个 data vector 使用 bitwise 按位操作 批量数据使用 SIMD 加速 使用随机 codebook 避免双值 codebook 在一些特定向量表现不佳，比如 ($1/\\sqrt{D}$… $−1/\\sqrt{D}$) 和 (1, 0, 0, 0) 的量化，我们使用随机正交矩阵去乘这个 codebook, 让 codebook 单位向量有相同的概率旋转到单位超球面上的任意位置 ","date":"2024-07-13","objectID":"/vector-search/:3:4","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4. 常见的向量数据库极其优劣 以下列举了一些常见的向量数据库，以及他们的优劣势, 有些是专用向量数据库，有些是对现有关系型数据库的扩展。 ","date":"2024-07-13","objectID":"/vector-search/:4:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.1 Milvus Milvus 是一款非常优秀的开源向量数据库，支持多种向量搜索算法，包括 HNSW，DiskANN，IVF 等。 除了向量检索的基本功能，还提供 sharding, streaming data ingestion 以及 hybrid search 等功能。 Milvus 采用的是云原生的存算分离，shared-everthing 的架构，且控制面和数据面分离。各个组件都是独立且可横向拓展的，分别为： 接入层(Access Layer)：由一组无状态 proxy 组成。它对外提供用户连接的 endpoint，负责验证客户端请求并合并返回结果。 它使用Nginx、Kubernetes Ingress、NodePort、LVS 等负载均衡组件提供统一的服务地址。 由于 Milvus 采用大规模并行处理 (MPP) 架构，代理会聚合并后处理中间结果，然后将最终结果返回给客户端 协调服务(Coordinator Service)：负责分配任务给执行节点，分别有 root coord、data coord、query coord。 root coord: 处理数据定义语言 (DDL) 和数据控制语言 (DCL) 请求，例如创建或删除集合、分区或索引，以及管理 TSO（时间戳 Oracle）和 time ticker。 data coord: 管理 data 以及 index 节点拓扑，维护元数据，触发flush、compact、索引构建等后台数据操作。 query coord: 管理查询节点 topology ，load balancing 以及 growing segments 到 sealed segments 转换. 执行节点(Worker Node)：执行从协调服务分配的任务以及 proxy DML 命令。 Query Node：检索增量日志数据，并通过订阅 log broker 将其转换为不断增长的段，从对象存储加载历史数据，并在向量和标量数据之间运行混合搜索。 Data Node：通过订阅 log broker 来获取增量日志数据，处理mutation 请求，并将日志数据打包成日志快照并存储在对象存储中 Index Node：构建索引。索引节点不需要常驻内存，可以通过Serverless 框架来实现。 存储层(Storage)：对象存储负责存储数据，存储 Data files 和 Index files。 Meta Storage: 元存储存储元数据的快照，例如 collection schema 和 message consumption checkpoints。存储元数据需要极高的可用性、强一致性和事务支持，因此 Milvus 选择了 etcd 进行元存储。 Milvus 还使用 etcd 进行服务注册和健康检查。 Object Storage: 存储日志的快照文件、标量和向量数据的索引文件以及中间查询结果。 Milvus 使用 MinIO 作为对象存储，可以轻松部署在 AWS S3 和 Azure Blob。但对象存储的访问延迟较高，且按查询次数收费。为了提高性能并降低成本，Milvus 计划在基于内存或 SSD 的缓存池上实现冷热数据分离。 Log Broker: 发布-订阅系统，它负责流数据持久化和事件通知。当工作节点从系统故障中恢复时，它还能确保增量数据的完整性。 Milvus cluster 使用 Pulsar 作为 log broker； Milvus standalone 使用 RocksDB 作为 log broker。此外，log broker 可以很容易地替换为Kafka等流数据存储平台。 Milvus 的云原生架构是其优点，同时也给开发者带来了不小的挑战，例如新概念的学习，以及相关组件 Pulsar 或者 etcd 带来的运维管理上的挑战。 ","date":"2024-07-13","objectID":"/vector-search/:4:1","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.4 Pinecone ","date":"2024-07-13","objectID":"/vector-search/:4:2","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.5 Qdrant ","date":"2024-07-13","objectID":"/vector-search/:4:3","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.6 Pgvector ","date":"2024-07-13","objectID":"/vector-search/:4:4","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.7 Pgvecto.rs ","date":"2024-07-13","objectID":"/vector-search/:4:5","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"4.8 VectorChord ","date":"2024-07-13","objectID":"/vector-search/:4:6","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"5. 优秀的向量搜索库以及向量数据库开源项目 ","date":"2024-07-13","objectID":"/vector-search/:5:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"6. 向量数据库商业化你需要知道什么 ","date":"2024-07-13","objectID":"/vector-search/:6:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"7. 总结 在这里我也是走马观花的介绍了一些向量搜索的基础知识，以及一些常见的向量搜索算法，向量搜索应用场景，向量搜索算法优化，常见的向量数据库极其优劣势，优秀的向量搜索库以及向量数据库开源项目，后面还是希望可以应用到实际的场景中。希望这篇文章能够帮助你更好的了解向量搜索。 ","date":"2024-07-13","objectID":"/vector-search/:7:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2024","vector search","Postgres"],"content":"8. 引用 非常感谢 Pinecone 的文章，让我对向量数据库有了更深的了解。 https://www.pinecone.io/learn/series/faiss/vector-indexes/ https://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/ https://zhuanlan.zhihu.com/p/379372268 https://songlinlife.github.io/2022/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANSG/ https://www.xiemingzhao.com/posts/hnswAlgo.html https://whenever5225.github.io/2020/05/11/hnsw-heuristic/ Search Engine For AI：高维数据检索工业级解决方案 https://zhuanlan.zhihu.com/p/50143204 https://mp.weixin.qq.com/s/AelU5O52Ed0Zx7f9867UNw ","date":"2024-07-13","objectID":"/vector-search/:8:0","tags":["vector search","Postgres"],"title":"向量数据库中的门门道道","uri":"/vector-search/"},{"categories":["2022","kubernetes","documentation"],"content":"最近在搞在内部自研平台上做一些 NUMA 感知调度的工作，涉及到 kubernetes 节点资源拓扑的发现以及调度方面的内容。但是无奈才疏学浅，遇到问题查问题，一知半解的始终抓不到头绪，谨以此篇文章来梳理总结。 ","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:0:0","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"为啥需要感知拓扑 这里 kubernetes 官方说了：目前越来越多的系统利用 CPU 和硬件加速器，比如 GPU,DPU 来支持低延迟的任务以及高吞吐的并行计算任务。 但是好像又说的不清楚，其实本质原因是冯诺依曼架构带来的问题。还是那句老话，没有银弹，冯诺依曼架构将存储器和运算器分开，指令和数据均放在存储器，为现代计算的通用性奠定了基础。但是也埋下了隐患，那就是内存容量指数级提升后，CPU 和内存之前的数据传输成为了瓶颈。目前服务器中的设备基本都是通过 PCIe 总线进行高速连接，而不同的用途的服务器可能其总线布局也不相同，如下图所示（网上找到的，非本人所画）,左图 GPU 驻留在不同的 PCIe 域上，GPU 内存之间的直接 P2P 复制是不可能的，从 GPU 0 的内存复制到 GPU 2 的内存需要首先通过 PCIe 复制到连接到 CPU 0 的内存，然后通过 QPI 链接传输到 CPU 1 并再次通过 PCIe 传输到 GPU 2。可以想象这个过程在延迟和带宽方面增加了大量的开销，而右图可以通过 GPU P2P 连接实现超高速通信。简单总结下，拓扑会影响设备间的通信，通信对业务造成稳定性以及效率，所以需要通过一些技术手段让业务感知拓扑。 PCIe Topo (figure 1) ","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:1:0","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"拓扑类型 目前有哪些拓扑需要感知: GPU Topology Awareness NUMA Topology Awareness ","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:2:0","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"GPU Topology Manager 业界目前有几种实现方案： Volcano GPU Topology Awareness 百度智能云 GPU 拓扑感知调度 Volcano 目前未完全实现，智能云闭源只能通过一些分享的信息，管中窥豹。 Why 为什么需要 GPU 的拓扑感知，首先上个图，这张图来自 NVIDIA 官方，描述了现在主流的 GPU 显卡 V100 在服务器中的拓扑关系。 GPU Topo (figure 2) 每块 V100 GPU有6个 NVLink 通道，8块 GPU 间无法做到全连接，2块 GPU 间最多只能有2条 NVLink 连接。其中 GPU0 和 GPU3，GPU0 和 GPU4 之间有2条NVLink 连接，GPU0 和 GPU1 之间有一条 NVLink 连接，GPU0 和6之间没有 NVLink 连接，故 GPU0 与 GPU6 之间仍然需要通过 PCIe 进行通信。NVlink 连接的单向通信带宽为 25 GB/s，双向通信带宽为 50 GB/s，PCIe 连接的通信带宽为16 GB/s。所以在 GPU 训练过程中如果错误的分配了 GPU， 比如某训练任务 Pod 申请了两张卡 GPU0 与 GPU6，在跨 GPU 通信可能就成为了训练任务的瓶颈。 拓扑信息可以在节点执行命令查看： # nvidia-smi topo -m GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 GPU0 X PIX PHB PHB SYS SYS SYS SYS GPU1 PIX X PHB PHB SYS SYS SYS SYS GPU2 PHB PHB X PIX SYS SYS SYS SYS GPU3 PHB PHB PIX X SYS SYS SYS SYS GPU4 SYS SYS SYS SYS X PIX PHB PHB GPU5 SYS SYS SYS SYS PIX X PHB PHB GPU6 SYS SYS SYS SYS PHB PHB X PIX GPU7 SYS SYS SYS SYS PHB PHB PIX X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge) PIX = Connection traversing a single PCIe switch NV# = Connection traversing a bonded set of # NVLinks How 这里就不一一解析，本身也没完整的实现来看，就以我自己的理解来梳理出一些大致思路。 第一步其实是感知，即通过 daemon 组件来进行 nvidia gpu 、网络拓扑、NVLink、PCIe 的信息。第二步则是调度器，定义策略。策略1：优先将同一个 NUMA node 下 NVLink 数最多的 GPU 调度到一个 Pod 上；策略2：优先将处于同一个 PCI switch 的 GPU 和网卡分配给同一个 Pod。大致整体思路如下： GPU-device-plugin 或者其他 daemon 进程，构造节点的 GPU 拓扑信息 CRD； Pod 定义 topo 策略，比如策略1或者策略2； 新定义调度器会根据 pod 调度的策略 fliter 、priority阶段过滤不满足策略节点，给满足策略节点打高分； 关于节点 gpu device 的发现更新交给 device-plugin 和 kubelet 来做, 参见文章。 目前 GPU 拓扑信息可以通过官方的 nvml (NVIDIA Management Library) 通过接口查询。 ","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:2:1","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"NUMA Topology Awareness Why 谈到 NUMA 拓扑感知，一定要先解释 NUMA 是干啥呢，为啥要感知它呢？ NUMA Topo (figure 3) CPU Cache Latency (figure 4) 上面两幅图给你答案，现代 CPU 多采用 NUMA 架构， NUMA 全称 “Non-Uniform Memory Access” 即非一致性内存访问。为啥搞个非一致性，一致性不好吗？答案肯定是不好，因为如果使用 UMA 即一致性内存访问，随着北桥上的物理核心越来越多，CPU的频率也越来越高，总线带宽扛不住，访问同一块内存的冲突问题也会越来越严重。我们回到 NUMA架构，每个 NUMA node 上会有自己的物理CPU内核，以及每个 NUMA node 上核心之间之间也共享 L3 Cache。同时，内存也分布在每个NUMA node上的。某些开启了超线程的CPU，一个物理CPU内核在操作系统上会呈现两个逻辑的核。 回到业务侧，对于业务侧，如果程序都跑在同一个NUMA node上，可以更好地去共享一些L3 Cache，L3 Cache的访问速度会很快。如果L3 Cache没有命中，可以到内存中读取数据，访存速度会大大降低。 在容器大行其道的今天，由于 CPU 错误分配的问题尤为严重。因为现在节点出现了超卖，节点上有大量的容器在同时运行，如果同一个进行分配了不同的 NUMA 会发生什么问题： CPU 争抢带来频繁的上下文切换时间； 频繁的进程切换导致 CPU 高速缓存失败； 跨 NUMA 访存会带来更严重的性能瓶颈。 总结下：在现代 CPU 架构下，如果不感知 NUMA 拓扑关系，错误的进行 CPU 的分配，会导致性能问题，影响业务的 SLA。 How 上一章节阐述了为什么需要 NUMA 感知调度，那目前怎么感知 NUMA 拓扑呢，有什么现成的方案呢？这边我简单列下在 Kubernetes 生态的项目，各位看官如果有补充，可在评论区评论： Kubernetes Topology Manager Offical Crane NUMA 拓扑感知 Koordinator Fine-grained cpu orchestration Kubernetes Topology Manager 拓扑管理器（Topology Manager） 是一个 kubelet 组件，旨在协调负责这些优化的一组组件。Topology Manager 其实是解决一个历史问题，CPU Manager 和 Device Manager 是独立工作的，互相不感知。 首先来看下 Kubernetes Topology Manager 的实现，这里我也不想造轮子了，可以参看阿里的同学总结的一篇好文。这里做一个总结： 找到不同资源的 topology hints 即拓扑信息， cpu 的选择标准是在满足资源申请的情况下，涉及的 NUMA 节点个数最少前提下涉及到 socket 个数最小的优先选择。 device manager 则在满足资源申请情况下，涉及 NUMA 节点最小优先选择。 不同 topology 类型的 hints 做 merge 操作，也就是 union，选出最优策略 如果选到还好，如果没选出来怎么办？kubernetes 提供了 kubelet 配置策略: best-effort: kubernetes 节点也会接纳这个 Pod，就是效果不达预期。 restricted：节点会拒绝接纳这个 Pod，如果 Pod 遭到节点拒绝，其状态将变为 Terminated。 single-NUMA-node：节点会拒绝接纳这个Pod，如果 Pod 遭到节点拒绝，其状态将变为Terminated。这里比 restricted 还多一个条件是选出来的 NUMA 节点个数需要是1个。 所以我们看到 Kubernetes Topology Manager 还是在以 NUMA 为中心来进行不同资源（NIC, GPU, CPU）来进行 complete fair 最短路径的选择。而且是在 pod 被调度到某个节点上后 kubelet 执行上述的过程，这样会带来几个问题： pod 有很大概率会 Terminated, 生产上不可用。 节点的 topology-manager-policy 配置不方便， kubelet 每次配置参数都需要重启，如果遇到特殊的版本可能会重启所有节点 pod，详见文章 所以我们会想到几个优化的方案： 让一个类似 kubelet 的 daemon 进程，可以发现 topo 关系，并向外暴露； 可以让拓扑感知放在 kube-scheduler 给 pod 分配 node 的时候就感知，而且可以指导调度； 提供声明式的灵活的 topology manager policy。 下面介绍的几个拓扑感知方案其实就是基于上面的 idea 应运而生的。 Crane NUMA 拓扑感知 首先看下 Crane NUMA 感知调度的架构图。 Crane NUMA Topology Aware (figure 5) 大致流程如下： Crane-Agent 从节点采集资源拓扑，包括NUMA、Socket、设备等信息，汇总到NodeResourceTopology这个自定义资源对象中。 Crane-Scheduler在调度时会参考节点的NodeResourceTopology对象获取到节点详细的资源拓扑结构，在调度到节点的同时还会为Pod分配拓扑资源，并将结果写到Pod的annotations中。 Crane-Agent在节点上Watch到Pod被调度后，从Pod的annotations中获取到拓扑分配结果，并按照用户给定的CPU绑定策略进行CPUSet的细粒度分配。 这里其实就已经解决上述 Kubernetes Topology Manager 的缺陷。不过我们发现策略怎么配置呢，这里提供两种策略配置方案 业务侧，在 pod 上打不同的标签来指定策略： none：该策略不进行特别的CPUSet分配，Pod会使用节点CPU共享池。 exclusive：该策略对应kubelet的static策略，Pod会独占CPU核心，其他任何Pod都无法使用。 NUMA：该策略会指定NUMA Node，Pod会使用该NUMA Node上的CPU共享池。 immovable：该策略会将Pod固定在某些CPU核心上，但这些核心属于共享池，其他Pod仍可使用。 节点侧 目前默认是 cpu manager policy 是 static 即允许为节点上具有某些资源特征的 Pod 赋予增强的 CPU 亲和性和独占性； topology manager policy 是 SingleNUMANodePodLevel。 如果节点无 topology.crane.io/topology-awareness 标签，则 topology manager policy 为 none 这里有个比较特别的功能，默认 kubelet 的 static 的 cpu manager 策略，只对 pod qos 为 guranteed 且资源申请为整数的 pod 生效，且分配指定的 cpu 其他进程无法占用。但是配合 crane-agent 和 Crane NUMA 感知调度后, 可以实现 pod 和 绑定核心的 pod 共享资源，可以在利用绑核更少的上下文切换和更高的缓存亲和性的优点的前提下，还能让其他 workload 部署共用，提升资源利用率。而且放松了 pod 的要求，只需要任意container的CPU limit大于或等于1且等于CPU request即可为该container设置绑核。实验下： $ cat nginx apiVersion: v1 kind: Pod metadata: name: nginx annotations: topology.crane.io/topology-awareness: 'true' topology.crane.io/cpu-policy: 'immovable' spec: containers: - image: nginx imagePullPolicy: Always name: nginx resources: requests: cpu: 2 memory: 1Gi limits: cpu: 2 memory: 1Gi $ k exec -it nginx /bin/bash $ taskset -cp 1 # 查看绑核 pid 1's current affinity list: 0,1 # 查看 burstable pod 的 cpuset 信息 $ cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod2260198d_db73_41f0_8ae3_387e09d3b9ec.slice/cri-containerd-6a5dfa37f9ce9102e1f781160d1fecb11b17dc835e5d72b9d7f573b515af86b3.scope/cpuset.cpus 0-9 # change to exclusive annotations: topology.crane.io/topology-awareness: 'true' topology.crane.io/cpu-policy: 'exclusive' $","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:2:2","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":["2022","kubernetes","documentation"],"content":"引用 这里站在巨人的肩膀上，再次感谢。 https://kubernetes.io/zh-cn/ https://github.com/volcano-sh/volcano/ https://mp.weixin.qq.com/s/uje27_MHBh8fMzWATusVwQ https://www.infoq.cn/article/tdfgiikxh9bcgknywl6s https://github.com/NVIDIA/go-nvml https://gocrane.io/zh-cn/docs/ https://koordinator.sh/docs/user-manuals https://www.likakuli.com/posts/kubernetes-kubelet-restart/ https://zhuanlan.zhihu.com/p/121588317 ","date":"2022-12-29","objectID":"/kubernetes-topo-aware-all-you-need-know/:3:0","tags":["kubernetes","topo aware"],"title":"关于 Kubernetes 中，拓扑感知你需要知道的一切","uri":"/kubernetes-topo-aware-all-you-need-know/"},{"categories":null,"content":"关于远东","date":"2022-12-29","objectID":"/about/","tags":null,"title":"关于远东","uri":"/about/"},{"categories":null,"content":"截止到 2024 年，有着超过 6 年的 AI Infra 的经验： ","date":"2022-12-29","objectID":"/about/:0:0","tags":null,"title":"关于远东","uri":"/about/"},{"categories":null,"content":"2018-2021.2（含实习）Unisound 在 AI 算法公司 Unisound 负责 Atlas 超算平台的研发和运维，支持 NLP 以及 CV 模型训练。主要工作如下： 开发大规模智能调度系统以优化多租户资源分配 优化高性能分布式文件系统 Lustre 的性能 构建多层缓存的云原生架构以加速 AI 模型训练 在 Unisound 从事 8 Bit 训练及推理优化工作，落地模型在 NPU 以及 NVIDIA Edge Device 的优化工作。 ","date":"2022-12-29","objectID":"/about/:0:1","tags":null,"title":"关于远东","uri":"/about/"},{"categories":null,"content":"2021.2-2023.5 Tencent Cloud 构建公有云大规模 AI 平台： 通过 EKS （Elastic Kubernetes Service） 构建高性能、可伸缩的弹性离线训练平台。 结合公有云对象存储以及加速器 GooseFS, 构建云上高性能缓存调度系统 构建 FinOps 基础设施帮助公共云中的客户更轻松地管理优化云成本，提升云资源利用率： 通过调度时以及重调度优化，通过高低优任务识别，智能弹性扩缩容。 结合腾讯如意内核调度器优化以及可观测性，在保证服务质量的同时进行成本优化 在内部云中大规模推出降低成本计划，通过资源的合理分配，提升资源利用率 ","date":"2022-12-29","objectID":"/about/:0:2","tags":null,"title":"关于远东","uri":"/about/"},{"categories":null,"content":"2023.5-至今 Tensorchord 负责在 GCP 上构建 Serverless Inference 平台 ModelZ， 提供极致的冷启动优化模型服务推理服务： 通过构建缓存模型服务、镜像预热等手段优化模型服务的冷启动时间 引入 JuiceFS 构建高性能缓存调度系统，提升模型服务的性能 负责整个 Cloud Team, 构建向量数据库 VectorChord 的云服务以及客户支持 VectorChord Cloud： 在 AWS 上构建基于 Postgres 的向量数据库，实现控制面、数据面分离，BYOC(Bring Your Own Cloud)、BYOD(Bring Your Own Data) 等功能 引入云原生架构，实现 Postgres 存算分离、高可用、Backup、PITR(Point-In-Time Recovery)、In-Place Upgrade 等功能 技能栈：Kubernetes, GCP, AWS, Kubeflow, FinOps, RAG, Vector Database, Storage Accelerate, Tensorflow , Pytorch, Cloud Native, MLOps, AI Infra, etc. 🌱 目前专注在 MLOps 以及 FinOps 领域,贡献了一些开源项目： fluid Fluid, elastic data abstraction and acceleration for BigData/AI applications in cloud. (Project under CNCF) crane Crane is a FinOps Platform for Cloud Resource Analytics and Economics in Kubernetes clusters. The goal is not only to help users to manage cloud cost easier but also ensure the quality of applications. crane-scheduler Crane scheduler is a Kubernetes scheduler which can schedule pod based on actual node load. creator Creator is the brain of crane project, contains crane core algorithm module and evaluation module. openmodelz One-click machine learning deployment (LLM, text-to-image and so on) at scale on any cluster (GCP, AWS, Lambda labs, your home lab, or even a single machine). clusternet [CNCF Sandbox Project] Managing your Kubernetes clusters (including public, private, edge, etc.) as easily as visiting the Internet vectorchord Scalable, fast, and disk-friendly vector search in Postgres, the successor of pgvecto.rs. 📫 您如果想联系我，可以直接发送邮件，邮箱地址 xieydd@gmail.com,或者您可以加我的微信 echo -n 'eGlleWRkX2hhaGEK' | base64 -d. ","date":"2022-12-29","objectID":"/about/:0:3","tags":null,"title":"关于远东","uri":"/about/"}]