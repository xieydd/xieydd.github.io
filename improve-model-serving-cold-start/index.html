<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>优化模型推理的冷启动 | Xieydd&#39;s Blog</title>
  <meta name="description" content="AI Infra Engineer&#39;s Blog">
  
  
  <meta property="og:title" content="优化模型推理的冷启动">
  <meta property="og:description" content="AI Infra Engineer&#39;s Blog">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://blog.xieydd.top/improve-model-serving-cold-start/">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="优化模型推理的冷启动">
  <meta name="twitter:description" content="AI Infra Engineer&#39;s Blog">
  
  
  <link rel="icon" href="/favicon-32x32.png">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
  <link rel="stylesheet" href="/css/style.css">
  
  
  <meta name="view-transition" content="same-origin">
  
  
  <style>
    @font-face {
      font-family: 'Monaco';
      src: local('Monaco'), local('Menlo'), local('Consolas');
      font-display: swap;
    }
  </style>
  
  
</head>
<body class="antialiased">
  <div class="container">
    <header class="top-nav">
      <a href="https://blog.xieydd.top/" class="site-name">xieydd</a>
      <nav class="nav-links">
        
        <a href="https://blog.xieydd.top/posts/">文章</a>
        <a href="https://blog.xieydd.top/about/">关于</a>
        
        <div class="nav-icons">
          <button id="theme-toggle" class="theme-toggle" title="Toggle dark mode" aria-label="Toggle dark mode">
            <svg class="icon-moon" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
            </svg>
            <svg class="icon-sun" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
              <circle cx="12" cy="12" r="5"></circle>
              <line x1="12" y1="1" x2="12" y2="3"></line>
              <line x1="12" y1="21" x2="12" y2="23"></line>
              <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
              <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
              <line x1="1" y1="12" x2="3" y2="12"></line>
              <line x1="21" y1="12" x2="23" y2="12"></line>
              <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
              <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
            </svg>
          </button>
          <div class="language-switcher">
  
  
    
  
    
      
      
      
        
          <a href="https://blog.xieydd.top/en/improve-model-serving-cold-start/" class="lang-link">EN</a>
          
        
      
      
      
    
  
</div>
        </div>
      </nav>
    </header>
    <div class="nav-divider"></div>
    <main class="content swup-transition-main" id="swup">
      
<div class="reading-progress" id="reading-progress"></div>
<article class="post">
  <header class="post-header">
    <h1 class="post-title" style="view-transition-name: index.zh-cn">优化模型推理的冷启动</h1>
    <div class="post-meta">
      <span>发布于 8 Jan, 2025</span>
      <span>• 3 min read</span>
    </div>
  </header>

  
  <nav class="toc">
    <h2>目录</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#问题">问题</a></li>
    <li><a href="#deep-dive">Deep Dive</a>
      <ul>
        <li><a href="#why-image-is-so-large">Why image is so large?</a></li>
      </ul>
    </li>
    <li><a href="#我们尝试过的解决方案">我们尝试过的解决方案</a>
      <ul>
        <li><a href="#1-预热">1. 预热</a></li>
        <li><a href="#2-cache-模型">2. Cache 模型</a></li>
        <li><a href="#3-gcp-image-streaming">3. GCP Image Streaming</a></li>
        <li><a href="#4-更换镜像格式">4. 更换镜像格式</a></li>
        <li><a href="#5-使用-juicefs-构建模型缓存集群">5. 使用 JuiceFS 构建模型缓存集群</a></li>
      </ul>
    </li>
    <li><a href="#后续可能的优化">后续可能的优化</a></li>
    <li><a href="#引用">引用</a></li>
  </ul>
</nav>
  </nav>
  

  <div class="post-content">
    <p>之前在做 Serverless 模型推理 <a href="https://modelz.ai/">Modelz</a>，虽然现在已经 pivot 了，但是还是想分享一下如何优化模型推理的冷启动问题。由于我们的服务是基于容器调度，所以这里也涉及到了容器的冷启动问题。</p>
<h1 id="优化模型推理的冷启动">优化模型推理的冷启动</h1>
<h2 id="问题">问题</h2>
<p>首先我们看下 Serverless 模型推理，从用户请求到模型推理的过程：</p>
<details>
  <summary>Click me</summary>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">sequenceDiagram
    participant User
    participant Cloudflare 
    participant Ingress 
    participant AutoScaler 
    participant Node
    participant containerd
    User-&gt;&gt;Cloudflare: Model Call
    Cloudflare-&gt;&gt;Ingress: Request
    Ingress-&gt;&gt;AutoScaler: Request
    AutoScaler-&gt;&gt;Node: Scale Up
    Node-&gt;&gt;containerd: Container
    Note right of containerd: 1. Pull Image &lt;br&gt;2. Start Container&lt;br&gt;3. Download model
</code></pre></td></tr></table>
</div>
</div></details>
<div align="center">
  <img src="image-1.png" alt="image 1" />
</div>
<p>整个流程的链路很长，但是真正耗时的地方在最后 Containerd 拉取镜像和启动容器的过程。我们将这个部分进一步细化,这里的每个阶段的时间大致来自于引用1：</p>
<details>
  <summary>Click me</summary>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre class="chroma"><code class="language-fallback" data-lang="fallback">flowchart TD
    subgraph Pod Create
    3A[Pull Image 3.5GB 140s] --&gt; 3B[Download Model]
    end
    subgraph GPU Node Provision
    2A[VM Create 40s] --&gt; 2B[Node Initialize 45s]
    2B --&gt; 2C[GPU Driver Install 25s]
    end
    subgraph AutoScaler
    1A[HPA reaction 10s] --&gt; 1B[Auto Provisioning reaction 30s] --&gt; 1C[Node auto-scaling 35s]
    end
</code></pre></td></tr></table>
</div>
</div></details>
<div align="center">
  <img src="image-2.png" alt="image 2" />
</div>
<p>如果是 30G（在 AI 推理场景并不稀有） 的镜像，那么拉取时间将超过 15min, 这个时间对于用户来说是不可接受的。</p>
<p>而模型下载取决于模型的大小以及模型是否已经存在于 Pod 中，这个时间也是不可控的，但是后文我们也会针对性的提出优化方案。</p>
<h2 id="deep-dive">Deep Dive</h2>
<h3 id="why-image-is-so-large">Why image is so large?</h3>
<div align="center">
  <img src="ai-image.png" alt="ai docker image" />
</div>
<div align="center">
  <img src="nvidia-dl-stack.png" alt="nvidia docker image stack" />
</div>
<p>由上面两张图可以看到</p>
<ol>
<li>除了 NVIDIA Kernel Driver 以及 CUDA Lib 放在 Host 上，AI 应用程序以及框架所依赖的库都放在镜像中。</li>
<li>NVIDIA 的策略导致你无法大幅缩减你的镜像，你不知道哪些库会被使用，所以你只能把所有的库都放在镜像中。</li>
</ol>
<h2 id="我们尝试过的解决方案">我们尝试过的解决方案</h2>
<h3 id="1-预热">1. 预热</h3>
<p>首先我们会使用 <a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster-proportional-autoscaler</a> 根据既定规则，比如总节点为 8 的时候该类型 GPU 资源扩容到 2 个节点即使没有请求，也有预留 bubble. 然后根据 image 使用频率，使用 <a href="https://github.com/senthilrch/kube-fledged">kube-fledged</a> 在这些节点上创建 ImageCache，这样在真正请求的时候，镜像已经在节点上了。</p>
<h3 id="2-cache-模型">2. Cache 模型</h3>
<p>我们开发了一个 HuggingFace 的模型缓存服务，这个服务会在模型被调用的时候，通过比对模型的 hash 值，如果模型已经存在缓存服务中，那么直接返回缓存的模型，否则下载模型到缓存服务中。</p>
<h3 id="3-gcp-image-streaming">3. GCP Image Streaming</h3>
<p>利用 <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-container-image-streaming-in-gke">GCP Image Streaming</a> 将自己管理的镜像或者用户自定义的镜像转换到 GCP 的 Artifact Registry 中，在节点拉取镜像时，通过网络 mount container layers 到节点上，让 containerd 误以为镜像已经在节点上。但是这个方案有几个缺点：</p>
<ol>
<li>需要 GCP 的支持, vendor lock-in</li>
<li>用户镜像需要 proxy 转换到 GCP，这个过程会有一定的延迟</li>
<li>虽然 pod running 但是不是真正的完全可运行，可能会导致运行时缓慢</li>
</ol>
<h3 id="4-更换镜像格式">4. 更换镜像格式</h3>
<p>将 OCI 镜像格式转换成 <a href="https://github.com/dragonflyoss/nydus">nydus</a> 格式，并结合 lazy pulling 技术 <a href="https://github.com/dragonflyoss/nydus/blob/master/docs/nydus-zran.md">zran</a>, 测试下来有着数倍的提升。就是需要修改 containerd 配置，支持 nydus。 配合 <a href="https://github.com/dragonflyoss/dragonfly">Dragonfly</a> 的 P2P 技术，可以进一步提升镜像拉取速度。</p>
<h3 id="5-使用-juicefs-构建模型缓存集群">5. 使用 JuiceFS 构建模型缓存集群</h3>
<p>通过构建独立缓存池，将模型缓存到 JuiceFS 中。通过 JuiceFS CSI 将缓存目录挂载到容器中，如果模型已经存在 JuiceFS 中，那么直接使用，不存在则下载并直接缓存到 JuiceFS 中。这套架构主要是利用 JuiceFS Posix 以及使用对象存储的优势，无需关注缓存大小。这里需要对 JuiceFS 的参数进行调优，比如 prefetch block, buffer size 等。</p>
<h2 id="后续可能的优化">后续可能的优化</h2>
<ul>
<li>
<p>使用 GCP 的 <a href="data-container-image-preloading">image preloading</a> 功能，通过secondary boot disks preload 镜像到 node 上。</p>
</li>
<li>
<p>In-class registry cache <a href="https://github.com/spegel-org/spegel">spegel</a>。</p>
</li>
<li>
<p>Parallel Downloading in Kubelet <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/-kubelet-parallel-image-pull-limit/README.md">KEP 3673</a>。</p>
</li>
<li>
<p><a href="https://github.com/containerd/containerd/issues/8881">Parallel Container Layer Unpacking</a>, 这里在引用1中提到 containerd 需要实现 high IO queue depth 才能充分利用 EBS 的 throughput。</p>
</li>
<li>
<p><a href="https://x.com/yetone/status/1858034646585368891">yetone 的方案</a>:</p>
<blockquote>
<p>parse 了 Dockerfile，然后获得了 base image 和一系列 args、env 和 commands，并将其顺序合并起来 hash 了一下作为 s3 object key，然后在我们的 image builder job 的 pod 的 container 里起了 dind，然后在里面用 base image 起了新的 container 然后在里面执行上一步 parse 出来的 commands，执行完毕后把这个 container 的 rootfs 打成 tar 包并用 zstd 压缩然后上传到 s3</p>
</blockquote>
<blockquote>
<p>OCI image builder 和 containerd remote snapshotter，在 builder 侧自己构建镜像把所有 layer 只分成两个 layer ：环境（极大）和代码（极小），然后用 pzstd 和 s5cmd 流式压缩和流式上传到 s3，然后在 snapshotter 侧用 s5cmd 和 pzstd 流式下载和流式解压，直接打满了 GKE 的 disk IO，把 image 的拉取速度提升到了以前的 4 倍左右</p>
</blockquote>
</li>
<li>
<p>Modal lazying container loading</p>
<blockquote>
<p><img src="image.png" alt="alt text"></p>
</blockquote>
</li>
<li>
<p>Do some research on <a href="https://www.usenix.org/system/files/osdi24-fu.pdf">ServerlessLLM OSDI24</a></p>
</li>
</ul>
<h2 id="引用">引用</h2>
<ol>
<li><a href="https://www.youtube.com/watch?v=e6Oo2aoZPnA">https://www.youtube.com/watch?v=e6Oo2aoZPnA</a></li>
<li><a href="https://www.youtube.com/watch?v=SlkEW4C2kd4">https://www.youtube.com/watch?v=SlkEW4C2kd4</a></li>
</ol>

  </div>

  <footer class="post-footer">
    <div class="post-footer-divider"></div>

    
    <div class="post-tags">
      
      <a href="https://blog.xieydd.top/tags/serving" class="tag">#serving</a>
      
      <a href="https://blog.xieydd.top/tags/cold-start" class="tag">#cold start</a>
      
      <a href="https://blog.xieydd.top/tags/inference" class="tag">#inference</a>
      
    </div>
    

    <div class="share-section">
      <span class="share-label">分享到:</span>
      <div class="share-buttons">
        <a href="https://x.com/intent/post?url=https%3a%2f%2fblog.xieydd.top%2fimprove-model-serving-cold-start%2f" target="_blank" rel="noopener noreferrer" class="share-btn" title="Share on X">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M4 4l11.733 16h4.267l-11.733 -16z" />
            <path d="M4 20l6.768 -6.768m2.46 -2.46l6.772 -6.772" />
          </svg>
        </a>
        <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3a%2f%2fblog.xieydd.top%2fimprove-model-serving-cold-start%2f" target="_blank" rel="noopener noreferrer" class="share-btn" title="Share on LinkedIn">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M8 11v5" />
            <path d="M8 8v.01" />
            <path d="M12 16v-5" />
            <path d="M16 16v-3a2 2 0 1 0 -4 0" />
            <path d="M3 7a4 4 0 0 1 4 -4h10a4 4 0 0 1 4 4v10a4 4 0 0 1 -4 4h-10a4 4 0 0 1 -4 -4z" />
          </svg>
        </a>
        <a href="mailto:?subject=%e4%bc%98%e5%8c%96%e6%a8%a1%e5%9e%8b%e6%8e%a8%e7%90%86%e7%9a%84%e5%86%b7%e5%90%af%e5%8a%a8&body=https%3a%2f%2fblog.xieydd.top%2fimprove-model-serving-cold-start%2f" class="share-btn" title="Share via Email">
          <svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M3 7a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v10a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2v-10z" />
            <path d="M3 7l9 6l9 -6" />
          </svg>
        </a>
      </div>
    </div>

    <a href="#" class="back-to-top">返回顶部</a>

    <div class="post-footer-divider"></div>

    <div class="post-nav">
      
      <a href="https://blog.xieydd.top/s3/" class="prev-post">
        <span class="nav-label">上一篇</span>
        <span class="nav-title">S3</span>
      </a>
      
      
      <a href="https://blog.xieydd.top/llm-infra/" class="next-post">
        <span class="nav-label">下一篇</span>
        <span class="nav-title">LLM Infra 你需要知道的一切</span>
      </a>
      
    </div>
  </footer>
</article>

    </main>
  </div>
  <script src="https://unpkg.com/swup@4"></script>
  <script>
    (function() {
      function initThemeToggle() {
        var toggle = document.getElementById('theme-toggle');
        var html = document.documentElement;
        var stored = localStorage.getItem('theme');
        if (stored === 'dark' || (!stored && window.matchMedia('(prefers-color-scheme: dark)').matches)) {
          html.classList.add('dark');
        }
        if (toggle) {
          toggle.addEventListener('click', function() {
            html.classList.toggle('dark');
            localStorage.setItem('theme', html.classList.contains('dark') ? 'dark' : 'light');
          });
        }
      }

      function initReadingProgress() {
        var progress = document.getElementById('reading-progress');
        if (!progress) return;
        function updateProgress() {
          var scrollTop = window.scrollY || document.documentElement.scrollTop;
          var docHeight = document.documentElement.scrollHeight - window.innerHeight;
          var percent = docHeight > 0 ? (scrollTop / docHeight) * 100 : 0;
          progress.style.width = percent + '%';
        }
        window.removeEventListener('scroll', updateProgress);
        window.addEventListener('scroll', updateProgress);
        updateProgress();
      }

      function initBackToTop() {
        var backToTop = document.querySelector('.back-to-top');
        if (backToTop) {
          backToTop.addEventListener('click', function(e) {
            e.preventDefault();
            window.scrollTo({ top: 0, behavior: 'smooth' });
          });
        }
      }

      initThemeToggle();
      initReadingProgress();
      initBackToTop();

      var swup = new Swup({
        animationSelector: '[class*="swup-transition-"]',
        containers: ['#swup', '.top-nav'],
        cache: true
      });

      swup.hooks.on('content:replace', function() {
        initThemeToggle();
        initReadingProgress();
        initBackToTop();
        window.scrollTo(0, 0);
      });
    })();
  </script>
</body>
</html>
